WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.879
我们已经学习了绝对值巧和平方技巧 

00:00:02.879 --> 00:00:04.620
以及怎样在线性回归问题中使用这些技巧

00:00:04.620 --> 00:00:08.504
但我们还想对这些技巧有个直觉理解

00:00:08.505 --> 00:00:10.800
这些技巧真是太神奇了

00:00:10.800 --> 00:00:14.595
我们想探索一下它们的本质 开始吧

00:00:14.595 --> 00:00:18.269
假设我们想开发一个算法

00:00:18.269 --> 00:00:21.757
来为一组点找到一条最佳拟合的线

00:00:21.757 --> 00:00:23.509
算法的工作步骤如下

00:00:23.510 --> 00:00:25.250
首先 随机画条直线

00:00:25.250 --> 00:00:27.035
计算它的误差

00:00:27.035 --> 00:00:32.195
这个误差表示点到直线的距离

00:00:32.195 --> 00:00:35.960
本图中 距离的度量方法是点到直线的垂直距离之和  (或纵坐标绝对值误差)

00:00:35.960 --> 00:00:39.417
你也可以使用其他度量方法  (例如 水平距离)

00:00:39.417 --> 00:00:43.100
现在我们移动一下这条线 看误差会不会减小

00:00:43.100 --> 00:00:45.500
沿这个方向移动线 

00:00:45.500 --> 00:00:49.174
可以看到误差在增加 所以不是这个方向

00:00:49.174 --> 00:00:53.881
再沿另一个方向移动线  现在误差减少了 

00:00:53.881 --> 00:00:56.359
所以我们选这个方向

00:00:56.359 --> 00:01:00.019
重复这个过程

00:01:00.020 --> 00:01:03.680
一点点减少误差 直到找到误差最小的那条线

00:01:03.679 --> 00:01:04.924
我们将用梯度下降算法

00:01:04.924 --> 00:01:07.144
来最小化误差

00:01:07.144 --> 00:01:09.229
现在开始介绍梯度下降

00:01:09.230 --> 00:01:11.840
梯度下降的工作方式就像是下山

00:01:11.840 --> 00:01:16.400
这座山叫雷尼尔山 误差就象这座山

00:01:16.400 --> 00:01:18.859
我们想下山

00:01:18.859 --> 00:01:22.234
要下山 我们需要减少高度

00:01:22.234 --> 00:01:25.894
在右侧 我们要用一条直线来拟合这些数据点

00:01:25.894 --> 00:01:30.844
我们通过一点点减少误差来让这条直线贴近这些点

00:01:30.844 --> 00:01:35.923
因此下山等价于线性拟合问题

00:01:35.923 --> 00:01:37.100
如果我们想下山 

00:01:37.099 --> 00:01:39.229
我们就要先找下山的路

00:01:39.230 --> 00:01:43.228
要选择一个下山最快的方向

00:01:43.227 --> 00:01:44.674
假设选这个方向

00:01:44.674 --> 00:01:46.924
我们沿这个方向下降一点

00:01:46.924 --> 00:01:51.149
这等价于让这条线靠近这些点一点

00:01:51.150 --> 00:01:53.300
现在我们的高度降了一点

00:01:53.299 --> 00:01:56.734
同样这条线也更靠近这些点了

00:01:56.734 --> 00:01:59.329
不断重复这个过程  寻找下山最快的方向

00:01:59.329 --> 00:02:02.112
假如说我们到达这儿

00:02:02.112 --> 00:02:04.909
现在我们到达山底

00:02:04.909 --> 00:02:08.585
在右图 我们也找到了那条最佳拟合线

00:02:08.585 --> 00:02:12.500
这就是梯度下降

00:02:12.500 --> 00:02:15.090
现在我们再用数学语言来描叙一下梯度下降

00:02:15.090 --> 00:02:17.560
这是一个二维图像 

00:02:17.560 --> 00:02:20.640
也可以扩展到更高维的情况

00:02:20.639 --> 00:02:24.024
X轴表示权重  Y轴表示误差

00:02:24.025 --> 00:02:27.115
这个是误差函数

00:02:27.115 --> 00:02:30.550
开始时我们站在这里  

00:02:30.550 --> 00:02:34.435
这里下山的方法就是计算误差函数关于每个权重的梯度

00:02:34.435 --> 00:02:38.409
梯度方向总是指向误差函数增大最多的方向

00:02:38.409 --> 00:02:41.079
而逆梯度方向则指向

00:02:41.080 --> 00:02:44.140
误差函数减少最多的方向

00:02:44.139 --> 00:02:48.864
因此我们沿逆梯度方向走一步

00:02:48.865 --> 00:02:52.330
这一步表示用原权重 wi减去 

00:02:52.330 --> 00:02:56.500
误差相对于权重 wi的梯度

00:02:56.500 --> 00:02:59.080
实际上还要用学习速率 乘以误差的梯度

00:02:59.080 --> 00:03:01.855
因为我们控制步长大小

00:03:01.854 --> 00:03:05.844
这意味着误差函数不断减小  我们快接近最小值了

00:03:05.844 --> 00:03:08.439
重复上述过程

00:03:08.439 --> 00:03:11.514
就可以得到误差最小或很好的值

00:03:11.514 --> 00:03:13.539
一旦我们到这个点

00:03:13.539 --> 00:03:16.179
我们就找到了线性回归问题的一个很好的解决方案

00:03:16.180 --> 00:03:18.719
这就是梯度下降算法

