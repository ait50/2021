WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.690
接下来要学习的这个概念适用于回归和分类

00:00:04.690 --> 00:00:07.589
在这个视频里 我会用一个分类场景来解释这个新概念

00:00:07.790 --> 00:00:12.169
但是你会发现 所有参数值也适用于回归算法

00:00:12.169 --> 00:00:14.230
这个概念叫做正则化

00:00:14.230 --> 00:00:17.914
这是改善我们模型 确保不会过度拟合的有效技巧

00:00:18.114 --> 00:00:20.853
那么我们来看一下这些数据 这是一些数据

00:00:20.855 --> 00:00:22.550
已经备份了

00:00:22.550 --> 00:00:25.039
我们看一下分类数据的两个模型

00:00:25.039 --> 00:00:29.609
第一个是一条直线 第二个是高次多项式

00:00:29.609 --> 00:00:31.399
那么问题是 哪个更好呢？

00:00:31.399 --> 00:00:33.868
两种方法是不是各有利弊？

00:00:33.868 --> 00:00:36.259
左侧这个存在各种错误

00:00:36.259 --> 00:00:38.168
可以看到 有一个红点和

00:00:38.170 --> 00:00:41.335
蓝点都在错误位置 但是这种方法更加简单

00:00:41.335 --> 00:00:44.965
右侧这个没有任何错误 但实际上有点更加复杂

00:00:44.965 --> 00:00:47.140
那么如果我们想要左侧这个

00:00:47.140 --> 00:00:49.780
因为右侧这个过度拟合 并且不太好泛化

00:00:49.780 --> 00:00:52.579
问题在于当我们训练模型时

00:00:52.579 --> 00:00:55.920
右侧这个更可能出现 原因如下

00:00:55.920 --> 00:00:57.255
当我们训练模型时

00:00:57.255 --> 00:00:59.780
模型得到误差 并进行最小化

00:00:59.780 --> 00:01:02.890
左侧这个误差较小

00:01:02.890 --> 00:01:06.295
虽然只有两个点错了 但也是错了

00:01:06.295 --> 00:01:08.989
右侧这个的误差更小

00:01:08.989 --> 00:01:11.448
因为没有分类错误

00:01:11.450 --> 00:01:13.549
当我们训练模型来最小化误差时

00:01:13.549 --> 00:01:16.670
模型会像右边这个一样建立边界 而不会遵循左边这个

00:01:16.670 --> 00:01:21.060
那么我们如何得到左侧的呢？答案是这样的

00:01:21.060 --> 00:01:24.200
来看看等式 比如左边的直线等式是

00:01:24.200 --> 00:01:27.965
3X1 + 4X2 + 5 = 0

00:01:27.965 --> 00:01:31.159
多元方程式可能更加复杂 包括高次项 例如

00:01:31.159 --> 00:01:35.834
X1 的平方或 X1 乘以 X2 或者 X1 的三次方等

00:01:35.834 --> 00:01:37.778
如果我们观察左边的多项式

00:01:37.780 --> 00:01:40.049
这个要比右侧的更加简单

00:01:40.049 --> 00:01:42.935
系数更少 只有 3 4 5

00:01:42.935 --> 00:01:46.289
而右边的要多很多

00:01:46.489 --> 00:01:50.509
因此 如果我们能够通过关于这些数字的某个函数来表达这种误差

00:01:50.510 --> 00:01:52.805
那将非常有用 因为在某种程度上

00:01:52.805 --> 00:01:56.170
模型的复杂性将体现在这个函数中

00:01:56.170 --> 00:01:59.629
所以一个复杂模型相较于简单模型将得到更大的误差 

00:01:59.629 --> 00:02:03.238
我们来做一下 随后我会向你展示细节 但总体想法是

00:02:03.438 --> 00:02:07.348
我们使用这里的 3 和 4 注意我们这里忽略了常数项

00:02:07.349 --> 00:02:08.688
这是有原因的

00:02:08.688 --> 00:02:11.698
如果我们用 3 和 4 并加到误差中

00:02:11.699 --> 00:02:13.644
们得到稍大一些的误差

00:02:13.645 --> 00:02:16.999
但是如果我们拿 2 2 4 -3 6 4  并全部加到误差中

00:02:17.199 --> 00:02:18.429
会得到很大的误差

00:02:18.628 --> 00:02:21.318
现在我们看到左侧模型更好一些

00:02:21.318 --> 00:02:23.958
因为它的总体误差更小 所以 我们要做的是

00:02:23.960 --> 00:02:26.779
在计算误差时考虑模型的复杂性

00:02:26.979 --> 00:02:29.519
从这个角度来说

00:02:29.718 --> 00:02:32.723
简单的模型比复杂的模型更胜一筹

00:02:32.723 --> 00:02:36.988
简单的模型也可以容易泛化 而这也符合我们的目标

00:02:36.990 --> 00:02:39.610
现在 我来讲解如何在计算误差时

00:02:39.610 --> 00:02:42.404
考虑模型的复杂性

00:02:42.604 --> 00:02:48.489
简单来说 就是拿这些高亮的系数 并加到误差里

00:02:48.490 --> 00:02:52.875
这种方法叫做 L1 正则化 非常简单 我给你看一下

00:02:52.875 --> 00:02:54.810
L1 正则化就是

00:02:54.810 --> 00:02:58.930
拿这些系数 并且把它们的绝对值加起来 加到误差里

00:02:58.930 --> 00:03:01.399
比如在这个例子里   2 的绝对值是 2

00:03:01.598 --> 00:03:04.038
-2 的绝对值也是 2 等等

00:03:04.038 --> 00:03:07.703
所有系数绝对值加起来是 21

00:03:07.705 --> 00:03:09.058
在直线例子里

00:03:09.258 --> 00:03:13.348
我们是拿了 3 和 4 的绝对值相加 也就是 7

00:03:13.348 --> 00:03:15.363
7 比 21 要小很多

00:03:15.365 --> 00:03:18.719
所以这就是为什么复杂的模型得出较大的误差

00:03:18.919 --> 00:03:23.769
这就是 L1 正则化 它与绝对值相关

00:03:23.770 --> 00:03:25.810
L2 正则化是类似的 我们这里要做的

00:03:25.810 --> 00:03:28.343
不是得到绝对值

00:03:28.343 --> 00:03:30.829
而是把系数的平方加起来

00:03:30.830 --> 00:03:32.219
对于复杂模型例子

00:03:32.419 --> 00:03:34.449
我们把 2 的平方加 -2 的平方加上 -4 的平方等等

00:03:34.449 --> 00:03:36.935
最后得到 85

00:03:36.935 --> 00:03:41.020
在直线的这个例子中 我们将 3 的平方加 4 的平方等于 25

00:03:41.020 --> 00:03:43.145
这个值远小于 85

00:03:43.145 --> 00:03:46.993
我们再次看到复杂模型将比简单模型受到更多“惩罚”

00:03:47.193 --> 00:03:48.573
但是现在问题是

00:03:48.574 --> 00:03:53.019
如果我们惩罚复杂模型太少或者太多怎么办？

00:03:53.020 --> 00:03:57.280
也许有些模型 比如将火箭发送到月球或医学模型

00:03:57.280 --> 00:04:01.098
几乎没有容错的余地 那么我们可以接受一定的复杂性

00:04:01.098 --> 00:04:03.848
而另一些其他模型 如视频推荐

00:04:03.848 --> 00:04:07.209
或社交网络上的潜在好友推荐则有更多容错空间

00:04:07.210 --> 00:04:11.000
需要模型简单从而更快地处理海量数据

00:04:11.000 --> 00:04:12.590
因此我们可以接受有一定误差

00:04:12.590 --> 00:04:14.715
似乎对于每一个案例

00:04:14.715 --> 00:04:18.720
我们必须调整要在多大程度上“惩罚”每个模型的复杂度

00:04:18.720 --> 00:04:22.514
这个问题可以通过参数 λ 来解决

00:04:22.713 --> 00:04:24.108
我们将 λ

00:04:24.110 --> 00:04:27.845
乘以误差中的复杂度部分 如下所示

00:04:27.845 --> 00:04:29.870
让我们再看一下这两个模型

00:04:29.870 --> 00:04:31.910
记住误差中的黄色部分来自

00:04:31.910 --> 00:04:36.115
被错误分类的点 绿色部分来自模型的复杂度

00:04:36.115 --> 00:04:38.795
即多项式中的系数

00:04:38.795 --> 00:04:40.720
比方说 我们有一个很小的 λ

00:04:40.720 --> 00:04:45.845
当我们将绿色部分的误差乘以 λ 后得到的结果也很小

00:04:45.845 --> 00:04:48.550
因此 右边的模型依然更好

00:04:48.550 --> 00:04:51.910
因为误差中关于模型复杂度的部分很小并且不会有很大波动

00:04:51.910 --> 00:04:54.610
但是如果我们有一个很大的 λ

00:04:54.610 --> 00:04:57.694
那么误差中关于模型复杂度的部分将乘以一个很大的值 而被放大很多

00:04:57.894 --> 00:05:02.319
这将更大程度地惩罚了复杂模型 因此简单模型获胜

00:05:02.319 --> 00:05:04.149
总而言之

00:05:04.149 --> 00:05:05.949
如果我们的 λ 值很大

00:05:05.949 --> 00:05:08.963
那么将对模型复杂度有很大程度的惩罚 我们选择更简单的模型

00:05:08.963 --> 00:05:10.718
而如果我们有一个小的 λ 值

00:05:10.720 --> 00:05:12.870
那么我们对模型复杂度的惩罚将很小

00:05:12.870 --> 00:05:15.689
因此我们可以选择更复杂的模型

00:05:15.689 --> 00:05:19.949
现在的问题是我们应该使用哪个呢？ L1 还是 L2？

00:05:19.949 --> 00:05:23.259
这里是每个 L1 和 L2 优点的备忘单

00:05:23.259 --> 00:05:26.259
所以 L1 正则化实际上有着较低效的计算

00:05:26.259 --> 00:05:29.230
虽然它没有平方 看起来好像更简单

00:05:29.230 --> 00:05:31.660
但实际上那些绝对值很难求导

00:05:31.660 --> 00:05:35.439
而 L2 正则化的平方具有很漂亮的导数

00:05:35.439 --> 00:05:37.834
因此这些更容易进行计算处理

00:05:37.834 --> 00:05:40.629
L1 正则化比 L2 更快只会发生在

00:05:40.629 --> 00:05:43.555
数据稀疏时

00:05:43.555 --> 00:05:45.949
假如你有一千列数据 

00:05:45.949 --> 00:05:48.334
但是只有十列是相关的 其余大多都是零

00:05:48.334 --> 00:05:49.759
那么此时 L1 更快

00:05:49.759 --> 00:05:52.420
L2 对于非稀疏的输出更好

00:05:52.420 --> 00:05:55.290
此时数据在各列之间更均匀地分布

00:05:55.290 --> 00:05:57.079
L1 有一个巨大的好处就是

00:05:57.278 --> 00:05:58.754
它给了我们特征选择

00:05:58.754 --> 00:06:00.033
再比如说

00:06:00.035 --> 00:06:01.639
我们还是有一千列数据 但是确实只有

00:06:01.838 --> 00:06:04.139
十列是有用的而剩下的都是噪音

00:06:04.139 --> 00:06:08.168
L1将检测到这一点 并将那些列设为零

00:06:08.170 --> 00:06:12.514
而 L2 却不会这样做 它只是把所有的列拿来近似地处理它们

00:06:12.713 --> 00:06:17.713
就是这样 这就是正则化 

