WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.690
The following concept is one that works both for regression and classification.

00:00:04.690 --> 00:00:07.790
So in this video, we'll explain it using a classification problem.

00:00:07.790 --> 00:00:12.169
But as you will see, all the arguments here work with regression algorithms as well.

00:00:12.169 --> 00:00:14.230
The concept is called regularization,

00:00:14.230 --> 00:00:18.115
it's a very useful technique to improve our models and make sure they don't over fit.

00:00:18.114 --> 00:00:20.854
So let's look at some data, here's the data,

00:00:20.855 --> 00:00:22.550
and let's make two copies of it,

00:00:22.550 --> 00:00:25.039
and let's look at two models that classify this data.

00:00:25.039 --> 00:00:29.609
The first one is a line and the second one is a higher degree polynomial curve.

00:00:29.609 --> 00:00:31.399
So, the question is which one is better?

00:00:31.399 --> 00:00:33.869
Well, they both have their pros and cons, right?

00:00:33.869 --> 00:00:36.259
The one on the left makes a couple of mistakes.

00:00:36.259 --> 00:00:38.169
As you can see, there is a red point and

00:00:38.170 --> 00:00:41.335
a blue point in the wrong sides but it is much simpler.

00:00:41.335 --> 00:00:44.965
The one on the right makes zero mistakes but it's actually a bit more complicated.

00:00:44.965 --> 00:00:47.140
So let's say we want the one on the left because the one on

00:00:47.140 --> 00:00:49.780
the right over fits and it just doesn't generalize well.

00:00:49.780 --> 00:00:52.579
So the problem is that when we train the model,

00:00:52.579 --> 00:00:55.920
the one on the right will appear more likely and the reason is the following.

00:00:55.920 --> 00:00:57.255
When we're training the model,

00:00:57.255 --> 00:00:59.780
the model takes an error and minimizes it.

00:00:59.780 --> 00:01:02.890
So, the model on the left has a small error since it own

00:01:02.890 --> 00:01:06.295
misclassifies two points but it's an error nonetheless.

00:01:06.295 --> 00:01:08.989
The model in the right has a really small error since it

00:01:08.989 --> 00:01:11.449
does not misclassifies any of the points.

00:01:11.450 --> 00:01:13.549
So, if we train a model to minimize error,

00:01:13.549 --> 00:01:16.670
it will build a boundary like the one on the right, not the one on the left.

00:01:16.670 --> 00:01:21.060
So the question is, how do we pick the one in the left? Well, here's an idea.

00:01:21.060 --> 00:01:24.200
Let's look at the equation and let's say the equation of the line on the left is

00:01:24.200 --> 00:01:27.965
something like 3x_1 plus 4x_2 plus five equals zero.

00:01:27.965 --> 00:01:31.159
The equation of the polynomial is something more complex with

00:01:31.159 --> 00:01:35.834
high degree terms like x_1 squared x_1 x_2 x_2 cube, et cetera.

00:01:35.834 --> 00:01:37.779
If we look at the equation in the left,

00:01:37.780 --> 00:01:40.049
it's much simpler than the equation on the right.

00:01:40.049 --> 00:01:42.935
In particular, there are less coefficients, only three,

00:01:42.935 --> 00:01:46.490
four, five whereas the right one has many more.

00:01:46.489 --> 00:01:50.509
So, if we find a way to in commend the error by some function of these numbers,

00:01:50.510 --> 00:01:52.805
that would be very helpful because in some way

00:01:52.805 --> 00:01:56.170
the complexity of the model will be added into the error.

00:01:56.170 --> 00:01:59.629
So a complex model will have a larger error and then a simple model.

00:01:59.629 --> 00:02:03.439
So, let's do that and I'll show you the details later but the idea is that

00:02:03.439 --> 00:02:07.349
we take this three and four and notice that we're forgetting about the constant term,

00:02:07.349 --> 00:02:08.689
and there's a reason for that.

00:02:08.689 --> 00:02:11.699
But if we take this three and four and say add them to the error,

00:02:11.699 --> 00:02:13.644
we get a slightly bigger error.

00:02:13.645 --> 00:02:17.200
But what if we take all these coefficients and add them to the error here,

00:02:17.199 --> 00:02:18.629
now we get a huge error.

00:02:18.629 --> 00:02:21.319
Now, we can see that the modeling the left is better

00:02:21.319 --> 00:02:23.959
because it has a smaller combined error, so again,

00:02:23.960 --> 00:02:26.980
what we did is we took the complexity in the model into account,

00:02:26.979 --> 00:02:29.719
when we calculated the error and in that way,

00:02:29.719 --> 00:02:32.724
a simpler model has an edge over the complicated model.

00:02:32.724 --> 00:02:36.989
Simpler models have a tendency to generalize better so, that's what we want.

00:02:36.990 --> 00:02:39.610
So now, let me be more detailed on how to take

00:02:39.610 --> 00:02:42.605
the complexity of a model and turn into part of the error.

00:02:42.604 --> 00:02:48.489
In summary, will take this highlighted coefficients and somehow add them to the error,

00:02:48.490 --> 00:02:52.875
this method is called L1 regularization and it's very simple, here's how it works.

00:02:52.875 --> 00:02:54.810
What L1 regularization does,

00:02:54.810 --> 00:02:58.930
it takes the coefficient and just adds the absolute values of them to the error.

00:02:58.930 --> 00:03:01.599
So in this case, we're adding absolute value

00:03:01.599 --> 00:03:04.039
of two which is two plus absolute value of minus two,

00:03:04.039 --> 00:03:07.704
which is two again, et cetera and we see that they add to 21.

00:03:07.705 --> 00:03:09.260
In the linear case,

00:03:09.259 --> 00:03:13.349
we see that we're adding is the absolute value of three and four which is seven,

00:03:13.349 --> 00:03:15.364
so a seven is much less than 21,

00:03:15.365 --> 00:03:18.920
we can see that the complicated model gives us a much higher error.

00:03:18.919 --> 00:03:23.769
That's L1 regularization and the one is attached to the absolute value.

00:03:23.770 --> 00:03:25.810
L2 regularization is very similar and what we do

00:03:25.810 --> 00:03:28.344
here is instead of adding the absolute values,

00:03:28.344 --> 00:03:30.830
we add the squares of the coefficients.

00:03:30.830 --> 00:03:32.420
So for the complicated case,

00:03:32.419 --> 00:03:34.449
we get two squared plus minus two squared,

00:03:34.449 --> 00:03:36.935
et cetera which gives us 85.

00:03:36.935 --> 00:03:41.020
For the linear case, we get three squared plus four squared which is 25,

00:03:41.020 --> 00:03:43.145
which is much smaller than 85.

00:03:43.145 --> 00:03:47.195
So again, we see that the complex model gets punished a lot more than the simple model.

00:03:47.194 --> 00:03:48.574
But now the question is,

00:03:48.574 --> 00:03:53.019
what if we punish the complicated model too little or what if we punish it too much?

00:03:53.020 --> 00:03:57.280
Maybe some models, like a model to send a rocket to the moon or a medical model,

00:03:57.280 --> 00:04:01.099
have very little room for error and we're okay with some complexity,

00:04:01.099 --> 00:04:03.849
or maybe other models like a video recommendation model,

00:04:03.849 --> 00:04:07.210
or model recommending potential friends on a social network have more room

00:04:07.210 --> 00:04:11.000
for experimenting and need to be simpler and faster to run a big data.

00:04:11.000 --> 00:04:12.590
So we're okay with having some error.

00:04:12.590 --> 00:04:14.715
So it seems that for every case,

00:04:14.715 --> 00:04:18.720
we have to tune how much we want to punish complexity in each model.

00:04:18.720 --> 00:04:22.715
This can be fixed with a parameter and this parameter is called lambda.

00:04:22.714 --> 00:04:24.109
What we do with lambda,

00:04:24.110 --> 00:04:27.845
is we multiply the complexity part of the error as follows.

00:04:27.845 --> 00:04:29.870
Let's look at the two models again and let's

00:04:29.870 --> 00:04:31.910
remember that the yellow part of the error comes from

00:04:31.910 --> 00:04:36.115
the misclassified points and the green part comes from the complexity of the model,

00:04:36.115 --> 00:04:38.795
namely the coefficients in the polynomial.

00:04:38.795 --> 00:04:40.720
Let's say, we have a small lambda,

00:04:40.720 --> 00:04:45.845
so we take the green error and multiplied by small lambda which gives us something small.

00:04:45.845 --> 00:04:48.550
Therefore, the right model still wins because

00:04:48.550 --> 00:04:51.910
the complexity part of the error is small and it won't swing the balance.

00:04:51.910 --> 00:04:54.610
But if we have a large value for lambda,

00:04:54.610 --> 00:04:57.895
then we're multiplying the complexity part of the error by a lot.

00:04:57.894 --> 00:05:02.319
Which punishes the complex model more and then the simple model wins.

00:05:02.319 --> 00:05:04.149
So in summary, this is what happens,

00:05:04.149 --> 00:05:05.949
if we have a large lambda then we're punishing

00:05:05.949 --> 00:05:08.964
complexity by a large amount and we're picking a simpler model.

00:05:08.964 --> 00:05:10.719
Whereas if we have a small lambda,

00:05:10.720 --> 00:05:12.870
then we're punishing complexity by a small amount,

00:05:12.870 --> 00:05:15.689
so we're okay with having more complex models.

00:05:15.689 --> 00:05:19.949
Now the question is, which one to use L1 or L2 regularization?

00:05:19.949 --> 00:05:23.259
So here's a cheat sheet with some benefits for each one.

00:05:23.259 --> 00:05:26.259
L1 regularization is actually computationally inefficient

00:05:26.259 --> 00:05:29.230
even though it seems simpler because it has no squares,

00:05:29.230 --> 00:05:31.660
but actually those absolute values are hard to differentiate.

00:05:31.660 --> 00:05:35.439
Whereas, an L2 regularization squares have very nice derivatives.

00:05:35.439 --> 00:05:37.834
So, these are easy to deal with computation.

00:05:37.834 --> 00:05:40.629
The only times where L1 regularization is faster than

00:05:40.629 --> 00:05:43.555
L2 regularization is when the data is sparse.

00:05:43.555 --> 00:05:45.949
So let's say if you have a thousand columns of data but

00:05:45.949 --> 00:05:48.334
only 10 are relevant and the rest are mostly zeros,

00:05:48.334 --> 00:05:49.759
then L1 is faster,

00:05:49.759 --> 00:05:52.420
L2 is better for non-sparse outputs which

00:05:52.420 --> 00:05:55.290
is when the data is more equally distributed among the columns.

00:05:55.290 --> 00:05:57.280
L1 has one huge benefit which is that,

00:05:57.279 --> 00:05:58.754
it gives us feature selection.

00:05:58.754 --> 00:06:00.034
So let's say, we have again,

00:06:00.035 --> 00:06:01.840
data in a thousand columns but really only

00:06:01.839 --> 00:06:04.139
10 of them matters and the rest are mostly noise.

00:06:04.139 --> 00:06:08.169
So, L1 will detect this and will make the relevant columns into zeroes.

00:06:08.170 --> 00:06:12.715
L2 on the other hand won't do this and it just take the columns and treat them similarly.

00:06:12.714 --> 00:06:14.919
So that's it, that's regularization.

