WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.310
In the last video, we'll learned how to decrease

00:00:02.310 --> 00:00:05.990
an error function by walking along the negative of its gradient.

00:00:05.990 --> 00:00:09.970
Now in this video, we're going to learn formulas for these error functions.

00:00:09.970 --> 00:00:14.095
The two most common error functions for linear regression are the mean absolute error,

00:00:14.095 --> 00:00:15.885
and the mean squared error.

00:00:15.884 --> 00:00:18.015
First, we'll learn the mean absolute error.

00:00:18.015 --> 00:00:20.179
So, here's a point and a line,

00:00:20.179 --> 00:00:22.710
the point has coordinates x, y,

00:00:22.710 --> 00:00:25.370
and the line is called Y-hat since it's the prediction.

00:00:25.370 --> 00:00:27.810
So, our prediction for this point is going to be the point

00:00:27.809 --> 00:00:30.149
in the line with the same x coordinate as our point,

00:00:30.149 --> 00:00:32.479
that is the point x, y hat.

00:00:32.479 --> 00:00:37.784
This means the vertical distance from the point to the line is y minus y hat,

00:00:37.784 --> 00:00:40.399
and that's what we'll be calling the error.

00:00:40.399 --> 00:00:42.619
Notice that this is not the actual distance to

00:00:42.619 --> 00:00:44.989
the line since this would be a perpendicular segment,

00:00:44.990 --> 00:00:47.060
but it's the vertical distance from the point to

00:00:47.060 --> 00:00:50.770
the line which is the distance between the point and its prediction.

00:00:50.770 --> 00:00:53.210
Now, our total error is going to be the sum of

00:00:53.210 --> 00:00:56.015
all these distances for all the points in our dataset.

00:00:56.015 --> 00:00:58.185
And sometimes we'll use this as our error,

00:00:58.185 --> 00:01:01.219
but in this case we'll use the average or the mean absolute error,

00:01:01.219 --> 00:01:04.319
which is the sum of all the errors divided by m,

00:01:04.319 --> 00:01:06.349
the number of points in our dataset.

00:01:06.349 --> 00:01:09.329
Using the sum or the average won't change our algorithms,

00:01:09.329 --> 00:01:11.530
since that would only scale our error by a constant,

00:01:11.530 --> 00:01:13.989
namely m. Notice something here which is

00:01:13.989 --> 00:01:16.899
that we have an absolute value around the y minus y hat,

00:01:16.900 --> 00:01:19.060
the reason is that if the point is on top of the line,

00:01:19.060 --> 00:01:20.680
the distance is y minus y hat,

00:01:20.680 --> 00:01:24.175
but if it's under the line then it's y hat minus y.

00:01:24.174 --> 00:01:26.579
We want the error to always be positive,

00:01:26.579 --> 00:01:29.899
otherwise negative errors will cancel with positive errors.

00:01:29.900 --> 00:01:33.685
Therefore, we take the absolute value of y minus y hat.

00:01:33.685 --> 00:01:38.890
So, our mean absolute error is the average of all absolute errors or in other words,

00:01:38.890 --> 00:01:42.960
the sum of all these absolute values divided by the number of points,

00:01:42.959 --> 00:01:47.449
which is m. We're going to plot these error over here in a graph.

00:01:47.450 --> 00:01:49.700
Obviously as we mentioned before,

00:01:49.700 --> 00:01:54.254
the graph has more dimensions but this is a two-dimensional simplification of that graph.

00:01:54.254 --> 00:01:56.959
As we descend in this graph using gradient descent,

00:01:56.959 --> 00:01:59.359
we get a better and better line until we find

00:01:59.359 --> 00:02:03.120
the best possible fit with the smallest possible mean absolute error.

