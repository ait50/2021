WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.399
So in the previous example,

00:00:01.399 --> 00:00:03.850
we had a one column input and one column output.

00:00:03.850 --> 00:00:07.084
The input was the size of the house and the output was the price.

00:00:07.084 --> 00:00:09.515
So we had a two-dimensional problem.

00:00:09.515 --> 00:00:12.519
Our prediction for the price would be a line and

00:00:12.519 --> 00:00:16.179
the equation would just be a constant times size plus another constant.

00:00:16.179 --> 00:00:18.079
What if we had more columns in the input,

00:00:18.079 --> 00:00:20.079
for example size and school quality?

00:00:20.079 --> 00:00:22.539
Well, now we have a three dimensional graph because

00:00:22.539 --> 00:00:25.304
we have two dimensions for the input and one for the output.

00:00:25.304 --> 00:00:27.429
So now our points don't live in the plane,

00:00:27.429 --> 00:00:30.804
but they look like points flying in 3-dimensional space.

00:00:30.804 --> 00:00:33.979
What we do here is we'll feed a plane through them instead of fitting a line,

00:00:33.979 --> 00:00:38.969
and our equation won't be a constant times one variable plus another constant.

00:00:38.969 --> 00:00:42.420
It's going to be a constant times school quality plus

00:00:42.420 --> 00:00:46.140
another constant times size plus a third constant.

00:00:46.140 --> 00:00:48.795
That's what happens when we're in three dimensions.

00:00:48.795 --> 00:00:51.179
So what happens if we're in n dimensions?

00:00:51.179 --> 00:00:56.524
So in this case we have n minus one columns in the input and one in the output.

00:00:56.524 --> 00:00:58.289
So, for example the inputs are size,

00:00:58.289 --> 00:01:00.829
school quality, number of rooms, et cetera.

00:01:00.829 --> 00:01:05.304
Well, now we have the same thing except our data lives in n-dimensional space.

00:01:05.305 --> 00:01:09.320
So for our input, we have n minus one variables namely; x_1,

00:01:09.319 --> 00:01:13.214
x_2 up to x_n minus one and for the output of the prediction,

00:01:13.215 --> 00:01:15.909
we only have one variable y hat.

00:01:15.909 --> 00:01:21.774
Our prediction would be an n minus one dimensional hyperplane living in n dimensions.

00:01:21.775 --> 00:01:26.090
Since it's hard to picture n-dimensions just think of a linear equation in n variables,

00:01:26.090 --> 00:01:32.070
such as y hat equals w1x1 plus w2x2 plus all the way to w_n

00:01:32.069 --> 00:01:38.269
minus one x_n minus one plus w_n and that's how we do predictions for higher dimensions.

00:01:38.269 --> 00:01:41.060
In order to find the weights w_1 up to w_n

00:01:41.060 --> 00:01:44.725
the algorithm is exactly the same thing for two variables.

00:01:44.724 --> 00:01:47.539
We can either do the absolute or square root tricks,

00:01:47.540 --> 00:01:50.625
or we can calculate the mean absolute or square errors,

00:01:50.625 --> 00:01:53.890
and minimize using gradient descent.

