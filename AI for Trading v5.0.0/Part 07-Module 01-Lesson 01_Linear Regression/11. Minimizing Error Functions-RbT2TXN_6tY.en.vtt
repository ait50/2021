WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.309
So far, we've learned two algorithms that will fit a line through a set of points.

00:00:04.309 --> 00:00:08.240
One is using any of the tricks namely the absolute and the square trick,

00:00:08.240 --> 00:00:11.160
and the other one is minimizing any of the error functions

00:00:11.160 --> 00:00:14.609
namely the mean absolute error and the mean squared error.

00:00:14.609 --> 00:00:17.559
The interesting thing is that these two are actually the exact same thing.

00:00:17.559 --> 00:00:20.579
What I'm saying is that when we minimize the mean absolute error,

00:00:20.579 --> 00:00:23.129
we're using a gradient descent step and that

00:00:23.129 --> 00:00:26.535
gradient descent step is the exact same thing as the absolute trick.

00:00:26.535 --> 00:00:28.800
Likewise, when we minimize the squared error,

00:00:28.800 --> 00:00:32.399
the gradient descent step is the exact same thing as the square trick.

00:00:32.399 --> 00:00:35.899
So let's see why, let's start with the mean squared error.

00:00:35.899 --> 00:00:37.640
Here's a point with coordinates (x,

00:00:37.640 --> 00:00:43.034
y) and here's our line with equation y-hat equals w_1 times x plus w_2.

00:00:43.034 --> 00:00:47.204
So y-hat is our prediction and the line predicts the y coordinate of this point

00:00:47.204 --> 00:00:49.469
that prediction gives us a point on the line that

00:00:49.469 --> 00:00:52.155
matches the x coordinate of the point (x,y).

00:00:52.155 --> 00:00:55.045
So it's the point (x,y-hat).

00:00:55.045 --> 00:00:59.920
Now, the error for this point is 1.5 times y minus y-hat squared,

00:00:59.920 --> 00:01:04.500
and the mean squared error for this set of points is the average of all these errors.

00:01:04.500 --> 00:01:06.340
But since average is a linear function,

00:01:06.340 --> 00:01:09.170
then whatever we do here applies to the entire error.

00:01:09.170 --> 00:01:13.040
Now, we know that the gradient descent step uses these two derivatives,

00:01:13.040 --> 00:01:15.275
namely the derivative with respect to the slope

00:01:15.275 --> 00:01:19.430
w_1 and the derivative with respect to the y-intercept w_2.

00:01:19.430 --> 00:01:21.190
If we calculate the derivatives,

00:01:21.189 --> 00:01:24.000
and you can see the calculation in detail in the instructor notes,

00:01:24.000 --> 00:01:29.480
we get negative times y minus y-hat times x for the one respect to the slope

00:01:29.480 --> 00:01:35.975
and negative times y minus y-hat for the one with respect to the y-intercept w_2.

00:01:35.974 --> 00:01:40.084
And notice that the length of this red segment is precisely (y minus

00:01:40.084 --> 00:01:44.924
y-hat) and the length of this green segment is precisely x.

00:01:44.924 --> 00:01:46.420
And if you remember correctly,

00:01:46.420 --> 00:01:49.799
the square trick told us that we have to upgrade the slope by

00:01:49.799 --> 00:01:53.989
adding y minus y-hat times x times the learning rate alpha,

00:01:53.989 --> 00:01:59.339
and upgrade the y-intercept by adding y minus y-hat times the learning rate alpha.

00:01:59.340 --> 00:02:03.284
But that is precisely what this gradient descent step is doing.

00:02:03.284 --> 00:02:06.929
If you like, feel free to pause the video or actually write it down

00:02:06.930 --> 00:02:11.344
in a little piece of paper to verify that is the exact same calculation.

00:02:11.344 --> 00:02:14.639
So this is why the gradient descent step utilize when we

00:02:14.639 --> 00:02:18.199
minimize the mean squared error is the same as the square trick.

00:02:18.199 --> 00:02:21.399
We can do the same thing with the absolute trick.

00:02:21.400 --> 00:02:25.680
The procedure is very similar except we have to be careful about the sign.

00:02:25.680 --> 00:02:28.765
This is our error, the absolute value of

00:02:28.764 --> 00:02:32.619
y minus y-hat and the derivatives of the error with respect

00:02:32.620 --> 00:02:35.650
to w_1 and w_2 are plus or minus x and

00:02:35.650 --> 00:02:39.504
plus or minus one based on the point is on top or underneath the line.

00:02:39.504 --> 00:02:41.409
Since the distance is x,

00:02:41.409 --> 00:02:43.509
then you can also check that this is precisely what

00:02:43.509 --> 00:02:47.344
the gradient descent step does when we minimize the mean absolute error.

00:02:47.344 --> 00:02:50.770
So that's it, that's why minimizing these errors with gradient descent is

00:02:50.770 --> 00:02:54.570
the exact same thing that using the absolute and the square tricks.

