WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.685
我在此单元格中将特征和编码标签

00:00:02.685 --> 00:00:06.455
拆分成了训练集、测试集和验证集

00:00:06.455 --> 00:00:11.130
首先根据 split_frac 拆分特征和标签数据

00:00:11.130 --> 00:00:13.050
将 80% 的数据用于训练

00:00:13.050 --> 00:00:16.230
我根据 0.8 这个值

00:00:16.230 --> 00:00:18.645
判断在哪个索引处拆分特征和标签数据

00:00:18.645 --> 00:00:22.235
我可以直接把这个变量放在这里

00:00:22.235 --> 00:00:24.415
先拆分特征

00:00:24.415 --> 00:00:27.610
保留直到 80% 拆分索引处的所有特征

00:00:27.610 --> 00:00:32.735
这样就会获得训练特征 train_x 然后获取剩余数据

00:00:32.735 --> 00:00:36.655
即拆分索引之后的数据 并构成 remaining_x

00:00:36.655 --> 00:00:40.670
然后对标签数据执行完全一样的操作

00:00:40.670 --> 00:00:44.030
在 80% 索引处拆分标签

00:00:44.030 --> 00:00:46.670
获得训练标签及剩余数据

00:00:46.670 --> 00:00:49.930
接着对剩余数据执行相似的操作

00:00:49.930 --> 00:00:53.045
需要将数据一分为二

00:00:53.045 --> 00:00:54.790
所以拆分比例为 0.5

00:00:54.790 --> 00:00:59.150
remaining_x 的每一半将构成验证集和测试集特征

00:00:59.150 --> 00:01:04.240
remaining_y 的每一半将构成验证集和测试集标签

00:01:04.240 --> 00:01:07.010
搞定最后一步是检查代码

00:01:07.010 --> 00:01:09.820
并输出特征数据的形状

00:01:09.820 --> 00:01:13.760
可以看出训练集中有数量最多的影评

00:01:13.760 --> 00:01:18.760
序列长度为 200 验证集和测试集的大小一样

00:01:18.760 --> 00:01:20.360
你可以输出标签数据的形状

00:01:20.360 --> 00:01:24.015
并且会在这里看到相同的行数

00:01:24.015 --> 00:01:26.125
这是 80% 的数据

00:01:26.125 --> 00:01:27.710
这是 10% 这也是10%

00:01:27.710 --> 00:01:30.740
创建训练集、测试集和验证集之后

00:01:30.740 --> 00:01:34.615
我们需要批处理数据 这样就能一次训练一批数据

00:01:34.615 --> 00:01:36.515
之前我们一直都是使用生成器函数

00:01:36.515 --> 00:01:40.190
现在还可以使用生成器函数

00:01:40.190 --> 00:01:42.980
但是我想要演示一种在拆分输入特征和标签时

00:01:42.980 --> 00:01:45.985
批处理数据的很方便方式

00:01:45.985 --> 00:01:50.315
我们可以创建数据加载器

00:01:50.315 --> 00:01:54.020
首先使用 PyTorch 的 TensorDataset

00:01:54.020 --> 00:01:58.530
将数据封装为已知格式 我们看看 TensorDataset 的文档

00:01:58.530 --> 00:02:03.015
该数据集会接受任意数量的张量

00:02:03.015 --> 00:02:04.585
并且第一个维度相同 即相同的行数

00:02:04.585 --> 00:02:08.780
也就是我们的输入特征和标签张量

00:02:08.780 --> 00:02:13.360
它会创建一个能被 PyTorch 的 DataLoader 类处理的数据

00:02:13.360 --> 00:02:16.770
创建数据并将数据封装到 TensorDataset 里后

00:02:16.770 --> 00:02:19.595
我们可以将其传递给 DataLoader

00:02:19.595 --> 00:02:22.070
DataLoader 的参数包括一些数据和批次大小

00:02:22.070 --> 00:02:27.305
返回一个对数据进行批处理的数据加载器

00:02:27.305 --> 00:02:29.330
这是一种创建生成器函数

00:02:29.330 --> 00:02:32.450
并将数据分成多个批次的很好方式

00:02:32.450 --> 00:02:35.120
DataLoader 类将在后台为我们处理很多工作

00:02:35.120 --> 00:02:38.725
代码看起来是这样的

00:02:38.725 --> 00:02:41.370
首先创建 TensorDataset

00:02:41.370 --> 00:02:42.845
要创建训练数据

00:02:42.845 --> 00:02:46.960
在这里传入在上面创建的 train_x 和 train_y 的张量版本

00:02:46.960 --> 00:02:49.555
torch.from.numpy 会接受 NumPy 数组

00:02:49.555 --> 00:02:52.570
并将它们转换为张量

00:02:52.570 --> 00:02:55.250
对训练、验证和测试数据执行相同的操作

00:02:55.250 --> 00:02:57.770
如果你在上面为这些数据集设定了不同的名称

00:02:57.770 --> 00:02:59.520
则需要在这里使用这些名称

00:02:59.520 --> 00:03:02.815
也可以反过来执行这些操作

00:03:02.815 --> 00:03:05.615
先为所有数据创建一个张量数据集

00:03:05.615 --> 00:03:08.955
然后将数据拆分为不同的数据集两种方法都可行

00:03:08.955 --> 00:03:11.540
然后将刚刚创建的每个张量数据集

00:03:11.540 --> 00:03:14.510
传入 PyTorch 的 DataLoader 中

00:03:14.510 --> 00:03:17.810
将 batch_size 设为 50

00:03:17.810 --> 00:03:20.905
我们不用使用各种循环和 yield 命令

00:03:20.905 --> 00:03:24.110
就能定义训练、验证和测试数据加载器

00:03:24.110 --> 00:03:27.710
并在训练循环里使用这些加载器将数据批处理为指定大小

00:03:27.710 --> 00:03:32.420
在这里生成了三个不同的迭代器

00:03:32.420 --> 00:03:34.310
我们看看这个数据集加载器中的样本数据

00:03:34.310 --> 00:03:37.545
在这里传入 train_loader 并获得一个迭代器

00:03:37.545 --> 00:03:40.670
然后通过调用 next() 获得一批数据

00:03:40.670 --> 00:03:45.075
返回一些样本输入特征和样本标签

00:03:45.075 --> 00:03:49.445
然后输出输入数据的大小 结果显示批次大小为 50

00:03:49.445 --> 00:03:54.030
序列长度为 200 并且输出标签大小为 50

00:03:54.030 --> 00:03:55.700
输入批次数据中的每个影评对应一个标签

00:03:55.700 --> 00:04:00.220
还有一些标记和编码标签

00:04:00.220 --> 00:04:02.120
看起来没问题

00:04:02.120 --> 00:04:06.200
接下来我们将继续定义模型并用这些数据训练模型

