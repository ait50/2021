{
  "data": {
    "lesson": {
      "id": 783465,
      "key": "54d166b1-bed4-48bf-b0d9-0c34d688fab5",
      "title": "Random Forests",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Learn about random forest models and how to use them to combine alpha factors.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/54d166b1-bed4-48bf-b0d9-0c34d688fab5/783465/1552687949788/Random+Forests+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/54d166b1-bed4-48bf-b0d9-0c34d688fab5/783465/1552687947426/Random+Forests+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 793534,
          "key": "67a7784b-2d89-46d7-9bd0-5a871df53a62",
          "title": "Intro",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "67a7784b-2d89-46d7-9bd0-5a871df53a62",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 815503,
              "key": "8572dec1-24ec-4182-8007-4830b9dfb832",
              "title": "L4 01  HS  Intro V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "9c5d6MvguA0",
                "china_cdn_id": "9c5d6MvguA0.mp4"
              }
            }
          ]
        },
        {
          "id": 793535,
          "key": "2b8bc6f9-3d34-419b-a874-eefecca1712f",
          "title": "Review Decision Trees",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "2b8bc6f9-3d34-419b-a874-eefecca1712f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 830904,
              "key": "e713f6b8-c4e1-4dd7-b2d1-a6a0b01d332e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2019/February/5c6daa42_decision-trees-review-quiz/decision-trees-review-quiz.jpg",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e713f6b8-c4e1-4dd7-b2d1-a6a0b01d332e",
              "caption": "",
              "alt": "",
              "width": 960,
              "height": 540,
              "instructor_notes": null
            },
            {
              "id": 830900,
              "key": "7bfb8f48-74ed-4cd3-b231-57cae1feed52",
              "title": "Review Decision Trees",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "7bfb8f48-74ed-4cd3-b231-57cae1feed52",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Before we move on, let’s quickly review what you learned in the lesson on decision trees. Let’s say you have the dataset above, and you are using the features “gender” and “occupation” to predict “app preference”. What is the information gain if you split the data by occupation?",
                "answers": [
                  {
                    "id": "a1550689219057",
                    "text": "0.534",
                    "is_correct": false
                  },
                  {
                    "id": "a1550689267908",
                    "text": "0.712",
                    "is_correct": false
                  },
                  {
                    "id": "a1550689268629",
                    "text": "0.605",
                    "is_correct": false
                  },
                  {
                    "id": "a1550689269253",
                    "text": "0.618",
                    "is_correct": true
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 793536,
          "key": "0fa34d22-1ce4-44e5-bf43-6cb8f3175f1b",
          "title": "Ensemble Methods",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "0fa34d22-1ce4-44e5-bf43-6cb8f3175f1b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 793540,
              "key": "052f7530-4460-48a8-b742-ed450b91a378",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Ensemble Methods\n\nGreat, so now that you’ve learned about decision trees, let’s learn about some new methods for making them more powerful. The idea is actually quite simple: we’re going to combine several weaker models (in this case, individual decision trees) together to make a more powerful model. The constituent models are called **weak learners**, while the combined model is called the **strong learner**. Combining many models together to yield a more powerful model is called **ensembling**.\n\nA key part of ensembling, though, is that the constituent models are not the same. In fact, ensembles tend to yield better results when the constituent models are very _different_. So how do we use the same dataset to grow many _different_ trees? Well, there are actually many ways. Let’s discuss a few of the most commonly used ones. ",
              "instructor_notes": ""
            },
            {
              "id": 813287,
              "key": "c376aa8f-276f-4ea2-be17-999a2845430c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2019/January/5c410d94_row-column-example-dataset/row-column-example-dataset.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c376aa8f-276f-4ea2-be17-999a2845430c",
              "caption": "Sample dataset",
              "alt": "Sample dataset",
              "width": 1374,
              "height": 700,
              "instructor_notes": null
            },
            {
              "id": 813288,
              "key": "1995a7af-aade-4d37-9506-b20b10c8e73c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "* For every tree, create a new dataset by drawing a random subset of **rows** from the original dataset. Train the tree on this new dataset.\n* For every tree, create a new dataset by drawing a random subset of **rows** from the original dataset _with replacement_. Train the tree on this new dataset.\n* For every tree, create a new dataset by drawing a random subset of **columns** from the original dataset. Train the tree on this new dataset.\n\nThese are examples of \"perturbations\"—ways to \"shake up\" the constituent trees in order to ensure that they are different from each other. These are all examples of ways to introduce “perturbations” _randomly_ and _independently_. In contrast, there's another class of methods where perturbations (on a given training set) are chosen deterministically and serially, with the nth perturbation depending strongly on all of the previously generated rules. So that we have more time to talk about the random and independent methods, we’re not going to talk more about the deterministic and serial methods for now. \n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 793507,
          "key": "e1a32abc-6ae4-41e8-8acd-24eadaf5504d",
          "title": "Perturbations on Columns",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e1a32abc-6ae4-41e8-8acd-24eadaf5504d",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 793528,
              "key": "4cd43dc4-e1ce-4284-8513-325eeb7c0824",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Perturbations on Columns\nAs we previously discussed, one way to generate different trees is to subsample the set of _columns_ the trees use to generate splits. When random subsets of the dataset are drawn as random subsets of features, then the method is known as the **Random Subspace Method**.",
              "instructor_notes": ""
            },
            {
              "id": 793509,
              "key": "6ce4ecfc-c4bd-41c0-8202-de8bf4edcd6f",
              "title": "MLND SL DT 13 Random Forests MAIN V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "4xhjf6s_Pr0",
                "china_cdn_id": "4xhjf6s_Pr0.mp4"
              }
            },
            {
              "id": 793529,
              "key": "50f83862-8e98-41df-9bfd-79f8303d20c2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Importance of Random Column Selection\nSometimes one feature will dominate in finance. If you don’t apply some type of random feature selection, then your trees will not be that different (i.e., will be correlated) and that reduces the benefit of ensembling.\n\nWhat features are typically dominant? Well, we'll talk about this more later when we talk about feature engineering, but when we use random forests for alpha combination, some of our features are alpha factors. Classical, price-driven factors, like mean reversion or momentum factors, often dominate. You may also see that features that define industry sectors or market \"regimes\" (periods defined, for example, by high or low market volatility or other market-wide trends) are towards the root of the tree.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 793506,
          "key": "e88cc77a-a6c4-4b00-b5f4-e7f9e1709d8f",
          "title": "Perturbations on Rows",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e88cc77a-a6c4-4b00-b5f4-e7f9e1709d8f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 793527,
              "key": "1b1d5409-b516-4717-92c7-49c0072746ca",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Perturbations on Rows\nAnother way to generate different trees is to grow each tree on a random subset of the original dataset’s _rows_. Subsets can be generated with or without replacement. When it's done with replacement, it's called **bagging**, and when it’s done without replacement, it’s called **pasting**. _Bagging_ is short for **bootstrap aggregating**.",
              "instructor_notes": ""
            },
            {
              "id": 793508,
              "key": "0efb5fa5-f349-4469-8a76-6ad0221ad4c3",
              "title": "MLND SL EM 02 Bagging V1 MAIN V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "9L_B0Jcio3c",
                "china_cdn_id": "9L_B0Jcio3c.mp4"
              }
            }
          ]
        },
        {
          "id": 793533,
          "key": "3bec96ce-8cc5-4829-ac6c-d8501c9b9f75",
          "title": "Forests of Randomized Trees",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "3bec96ce-8cc5-4829-ac6c-d8501c9b9f75",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 793541,
              "key": "9e0ba8d1-0a30-4c43-ac8b-06e5a2efda97",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Random Forests\nRandom Forests are ensemble prediction algorithms that use _both_ random column and random row selection. Each tree in the ensemble is created as follows:\n\n* If the number of rows in the training dataset is N, generate the dataset for each constituent tree by choosing N rows at random — but with replacement — from the original data.\n\n* If there are M columns in the training dataset, pick a number m<<M. At each node, select m columns at random out of the M and use the best split of possible splits on these m columns to split the node. The value of m is held constant during the forest growing. m is known as the `max_features` parameter, and the default value is sqrt(M).\n\n* Grow each tree to the largest extent possible.\n\nFor a regression tree model, use the average value of the ensemble of trees’ predictions. For a classification model, use the mode of the ensemble of trees’ predictions.\n\nIf you’d like to learn more, check out [this](https://link.springer.com/content/pdf/10.1023%2FA%3A1010933404324.pdf) paper.",
              "instructor_notes": ""
            },
            {
              "id": 831958,
              "key": "3250c4a9-9c46-430e-bbb3-b5f8ae406d00",
              "title": "L4 011  HS  Random Forests V5",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "TSpYXdBYo1s",
                "china_cdn_id": "TSpYXdBYo1s.mp4"
              }
            }
          ]
        },
        {
          "id": 783466,
          "key": "aab3a540-e2ae-4c95-bed0-d562059c439f",
          "title": "Random Forests Exercise",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "aab3a540-e2ae-4c95-bed0-d562059c439f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 783468,
              "key": "28f08406-5e6f-4c9a-a21b-a27ed6e1ac57",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "r403173c783466xJUPYTERjzce99bu",
              "pool_id": "jupyter",
              "view_id": "jupyter-5g7fm",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowGrade": false,
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/spam_rf.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 793537,
          "key": "44e0275f-5c8d-421b-83ec-1b8143dc8119",
          "title": "The Out-of-Bag Estimate",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "44e0275f-5c8d-421b-83ec-1b8143dc8119",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 831959,
              "key": "64f26d2f-5e52-404c-a6c1-c1bb5ff757a3",
              "title": "L4 15  HS  Outofbag Score V4",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "CcdXrGYaOhE",
                "china_cdn_id": "CcdXrGYaOhE.mp4"
              }
            }
          ]
        },
        {
          "id": 811258,
          "key": "af673a18-689d-46cc-b459-6bf933dbce37",
          "title": "Random Forest Hyperparameters",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "af673a18-689d-46cc-b459-6bf933dbce37",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 811259,
              "key": "a25547c9-c83c-45ad-9afb-5ad59a84112e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## [Random Forest Hyperparameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba)\n\nYou may have noticed that the values of several hyperparameters were set to default values when we instantiated the Random Forest model in the last exercise. Let's discuss a few of these hyperparameters and learn how they influence the model.\n\nWe've seen a few of the Random Forest hyperparameters before because they are also hyperparameters of the individual decision trees in the forest.\n\n### `min_samples_leaf`\nAs before, this is the minimum number of observations allowed at a leaf. Setting this hyperparameter keeps the algorithm from further splitting nodes with very few observations.\n\n### `min_samples_split`\nAs before, this is the minimum number of observations required to be at node before it can be split. Setting this hyperparameter keeps the algorithm from further splitting nodes with very few observations.\n\nHowever, as stated earlier, this hyperparameter does not actually prevent very small leaf nodes from being created. If a node has at least `min_samples_split` observations, then it can be split, and this split can result in a leaf with fewer than `min_samples_split` observations.\n\n### `max_features`\nThis sets the maximum number of features to evaluate when randomly sampling features at each split and deciding which feature to use to create the next split. The default value is the square root of the total number of features in the dataset.\n\nIn fact this is also a hyperparameter of the single decision tree classes, so it's possible to randomly choose subsets of features to evaluate at each split even when growing a single decision tree.\n\n### `n_estimators`\nThis is not a hyperparameter of individual decision trees because it's only applicable when growing forests—this is the number of trees to grow in the forest. \n\n### `oob_score`\nThis is a boolean hyperparameter that you set to `True` if you want the out-of-bag score to be calculated as an estimate of out-of-sample accuracy.\n\n### `bootstrap`\nThis is a boolean hyperparameter that sets whether or not bootstrap samples are used to grow the trees. If `False`, the entire original dataset is used to grow each tree.\n\n### `n_jobs`\nThis parameter allows you to use parallel threads to perform some parts of the algorithm's computations. Set `n_jobs = -1` to use all available CPUs. Most often, parallelism happens in fitting, but sometimes, as for random forests, it happens during prediction.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 813289,
          "key": "9a6e883c-3eb5-4791-a4a5-12dcec5701af",
          "title": "Choosing Hyperparameter Values",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "9a6e883c-3eb5-4791-a4a5-12dcec5701af",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 814163,
              "key": "58656c16-a7e6-4d69-ba97-c6df1cb3a4c2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Let's say we are trying to choose the `min_samples_leaf` hyperparameter, and want to avoid overfitting. How many training samples would we choose to be the minimum per leaf? In non-financial and non-time series machine learning, setting this hyperparameter is fairly straightforward: you use grid search cross-validation to find the value that maximizes the model’s performance on validation data. When you have time-series data, you typically don’t use cross-validation because usually you just want a single validation dataset that is as close in time as possible to the present. If you have a problem with high signal-to-noise, then you can try a bit of parameter tuning on the single validation set. In finance, though, you have time series data _and_ you have low signal-to-noise. Therefore, you have one validation set and if you were to try a bunch of parameter values on this validation set, you would almost surely be overfitting. As such, you need to set the parameter with some judgement and minimal trials. Later, we'll discuss a bit more about how we make this choice in the project.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 831935,
          "key": "853a50b9-c307-40e8-8339-031724642ce9",
          "title": "Random Forests for Alpha Combination",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "853a50b9-c307-40e8-8339-031724642ce9",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 831936,
              "key": "632a78b7-9f04-4659-86f8-eac36b40db6c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Random Forests for Alpha Combination\n\nSo we've seen how random forests are used for certain problems, like predicting a consumer's app preference from personal data. How do we use random forests for alpha combination?",
              "instructor_notes": ""
            },
            {
              "id": 831938,
              "key": "e618d96b-a226-4288-9164-5e9c35d10f54",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2019/February/5c75985d_rf-for-alpha-combination/rf-for-alpha-combination.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e618d96b-a226-4288-9164-5e9c35d10f54",
              "caption": "An example alpha combination data subset.",
              "alt": "An example alpha combination data subset.",
              "width": 1014,
              "height": 636,
              "instructor_notes": null
            },
            {
              "id": 831939,
              "key": "cce0d769-a15a-4abe-80d1-6dfe9824ff96",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "For this type of problem, we have data that look like the above. Each row is indexed by both date and asset. We typically have several alpha factors, and we then calculate \"features\", which provide the random forest model additional information. For example, we may calculate date features, which the algorithm could use to learn that certain factors are particularly predictive during certain periods. ",
              "instructor_notes": ""
            },
            {
              "id": 831945,
              "key": "c824dac4-fd59-4a9b-8ec2-828e47a9587f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2019/February/5c759a61_example-finance-tree/example-finance-tree.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c824dac4-fd59-4a9b-8ec2-828e47a9587f",
              "caption": "An example alpha combination tree.",
              "alt": "An example alpha combination tree.",
              "width": 1569,
              "height": 965,
              "instructor_notes": null
            },
            {
              "id": 831946,
              "key": "7221a8ed-9fdf-4398-abfe-5d70afe7fc84",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "What are we trying to predict? We're trying to predict asset returns—but not their decimal values! We rank them relative to each other into only two buckets, such that we essentially predict winners and losers on the day. The next lesson is all about feature engineering, so let's move on to learn more about features and labels in more detail!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 793539,
          "key": "003bb5be-eb22-4797-92aa-3e023924c364",
          "title": "Outro",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "003bb5be-eb22-4797-92aa-3e023924c364",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 831966,
              "key": "25fb8e54-96c8-43db-8080-4245edab8bad",
              "title": "L4 17  HS  Outro V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "oH7B6EyLE0k",
                "china_cdn_id": "oH7B6EyLE0k.mp4"
              }
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}