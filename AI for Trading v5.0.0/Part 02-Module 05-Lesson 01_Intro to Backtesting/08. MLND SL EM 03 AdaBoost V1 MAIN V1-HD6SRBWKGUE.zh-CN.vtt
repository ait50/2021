WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.649
Boosting （提升算法）

00:00:01.649 --> 00:00:02.934
会稍复杂一点

00:00:02.935 --> 00:00:06.269
Boosting 算法有很多种

00:00:06.269 --> 00:00:11.269
其中最流行的是Freund 和 Schapire 在1996年发明的ADABOOST 算法

00:00:11.269 --> 00:00:12.843
尽管稍后

00:00:12.845 --> 00:00:15.784
我们会讲到一些数学细节 但这里是要点

00:00:15.984 --> 00:00:18.820
如果仅从字面看,

00:00:18.820 --> 00:00:21.565
会有点不同 但我保证

00:00:21.565 --> 00:00:25.070
除了一些微不足道的事情 例如用常量乘以所有的剩余

00:00:25.070 --> 00:00:26.635
但这不会改变结果

00:00:26.635 --> 00:00:29.603
我将要讲述的是一个完全相同的ADABOOST算法

00:00:29.803 --> 00:00:34.004
想法如下 我们先拟合第一个学习器 来最大化准确度

00:00:34.005 --> 00:00:36.765
或者最小化错误数量

00:00:36.765 --> 00:00:40.700
有一些好的 但我们最多只能犯三个错误

00:00:40.700 --> 00:00:42.945
就是这个模型 让我们来拟合它

00:00:42.945 --> 00:00:44.859
我们先保存这个模型

00:00:44.859 --> 00:00:49.019
现在 我们的第二个弱学习器要修正第一个学习器的错误

00:00:49.020 --> 00:00:53.895
我们把第一个模型（弱学习器）误分类的点放大

00:00:53.895 --> 00:00:58.359
换句话说 就是如果第二个模型漏掉这些误分类点 就加大惩罚

00:00:58.359 --> 00:01:01.350
所以 下一个弱学习器需要更多地关注这些误分类的点

00:01:01.350 --> 00:01:03.439
这是第二个弱学习器

00:01:03.439 --> 00:01:05.685
它正确分类了这些点

00:01:05.685 --> 00:01:07.685
我们保存这个模型

00:01:07.685 --> 00:01:11.500
现在 我们再次惩罚这个错误分类的点

00:01:11.500 --> 00:01:13.280
把这些点放大

00:01:13.280 --> 00:01:16.075
这是第三个弱学习器

00:01:16.075 --> 00:01:20.915
它会更努力的尝试对大点进行正确的分类我们保存这个模型

00:01:20.915 --> 00:01:22.390
我们可以继续重复这个过程

00:01:22.390 --> 00:01:24.140
但假设三个就够了

00:01:24.140 --> 00:01:26.674
现在 我们要组合这些弱学习器模型

00:01:26.674 --> 00:01:29.170
以后  我会更具体的讲一下怎样组合它们

00:01:29.170 --> 00:01:32.408
现在 先让我们假设让它们象以前一样举行投票

00:01:32.608 --> 00:01:34.809
所以 这个就是我们拟合数据后

00:01:34.810 --> 00:01:36.290
得到的模型

00:01:36.290 --> 00:01:38.319
它拟合的非常好

00:01:38.519 --> 00:01:41.375
在这节课 我没有讲太多细节

00:01:41.375 --> 00:01:46.375
我们将在接下来的几个视频中讲更具体的细节

