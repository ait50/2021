WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:06.179
Word2Vec 可能是词嵌入在实践中运用最广泛的例子

00:00:06.179 --> 00:00:08.324
顾名思义 Word2Vec

00:00:08.324 --> 00:00:11.129
是将词转换成向量

00:00:11.130 --> 00:00:15.894
但是这个名字并没有体现出如何进行转换

00:00:15.894 --> 00:00:18.519
Word2Vec 的核心概念是

00:00:18.519 --> 00:00:21.509
一个模型能根据相邻词

00:00:21.510 --> 00:00:24.210
预测特定词 反之亦然

00:00:24.210 --> 00:00:27.000
预测特定词的相邻词

00:00:27.000 --> 00:00:30.804
从而能捕捉词的上下文意义

00:00:30.804 --> 00:00:31.890
实际上 这是

00:00:31.890 --> 00:00:34.380
Word2Vec 模型的两种功能

00:00:34.380 --> 00:00:38.969
一种是给出相邻词 称为连续词袋

00:00:38.969 --> 00:00:43.195
另一种是给出中间词 称为 Skip-gram

00:00:43.195 --> 00:00:44.939
在 Skip-gram 模型中

00:00:44.939 --> 00:00:47.114
可以从句子中挑选任何词

00:00:47.115 --> 00:00:51.840
将其转换成独热编码向量 将其输入神经网络或其他概率模型

00:00:51.840 --> 00:00:54.450
这个模型用于预测周围词 也就是上下文

00:00:57.880 --> 00:00:59.845
使用适当的损失函数

00:00:59.844 --> 00:01:03.299
优化模型的权重或参数并重复

00:01:03.299 --> 00:01:07.384
直到模型学会了尽可能地预测语境词

00:01:07.385 --> 00:01:12.120
现在将中间表示法作为神经网络中的隐藏层

00:01:12.120 --> 00:01:17.505
这一层特定词的输出变成对应的词向量

00:01:17.504 --> 00:01:22.039
连续词袋模型采用的策略相似

00:01:22.040 --> 00:01:25.020
由于每个词的意义分布在向量中

00:01:25.019 --> 00:01:29.004
这种词表示法非常稳健

00:01:29.004 --> 00:01:31.274
词向量的大小由你决定

00:01:31.275 --> 00:01:34.740
取决于你想如何平衡性能和复杂度

00:01:34.739 --> 00:01:37.824
无论训练时使用多少词 向量的大小都保持不变

00:01:37.825 --> 00:01:39.689
而词袋不同

00:01:39.689 --> 00:01:42.599
其大小会随着唯一词数量的增加而增加

00:01:42.599 --> 00:01:46.289
一旦预训练了一大组词向量

00:01:46.290 --> 00:01:50.445
就可以有效率地使用这些词向量 无需多次转换

00:01:50.444 --> 00:01:53.019
只需将其存储在查找表中即可

00:01:53.019 --> 00:01:56.789
最后便可直接用于深度学习架构

00:01:56.790 --> 00:02:01.645
例如 可以将其用作循环神经网络的输入向量

00:02:01.644 --> 00:02:06.524
还可使用循环神经网络训练出更好的词嵌入模型

00:02:06.525 --> 00:02:10.379
还有其它优化方式 可进一步降低模型和

00:02:10.379 --> 00:02:15.509
训练复杂度 例如 使用 Hierarchical Softmax 表示输出词

00:02:15.509 --> 00:02:19.000
使用稀疏交叉熵计算损失等

