WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.830
Where the embeddings are fast becoming the de facto choice for representing words,

00:00:04.830 --> 00:00:07.890
especially for use and deep neural networks.

00:00:07.889 --> 00:00:11.099
But why do these techniques work so well?

00:00:11.099 --> 00:00:15.419
Doesn't it seem almost magical that you can actually do arithmetic with words,

00:00:15.419 --> 00:00:19.734
like woman minus man plus king equals queen?

00:00:19.734 --> 00:00:22.574
The answer might lie in the distributional hypothesis,

00:00:22.574 --> 00:00:28.574
which states that words that occur in the same contexts tend to have similar meanings.

00:00:28.574 --> 00:00:30.719
For example, consider this sentence.

00:00:30.719 --> 00:00:33.774
Would you like to have a cup of blank?

00:00:33.774 --> 00:00:38.609
Okay. How about, I like my blank black.

00:00:38.609 --> 00:00:44.335
One more, I need my morning blank before I can do anything.

00:00:44.335 --> 00:00:45.789
What are you thinking?

00:00:45.789 --> 00:00:49.890
Tea? Coffee? What give you the hint?

00:00:49.890 --> 00:00:55.380
Cup? Black? Morning? But it could be either of the two, right?

00:00:55.380 --> 00:00:56.580
And that's the point.

00:00:56.579 --> 00:01:00.719
In these contexts, tea and coffee are actually similar.

00:01:00.719 --> 00:01:05.500
Therefore, when a large collection of sentences is used to learn in embedding,

00:01:05.500 --> 00:01:11.484
words with common context words tend to get pulled closer and closer together.

00:01:11.484 --> 00:01:15.989
Of course, there could also be contexts in which tea and coffee are dissimilar.

00:01:15.989 --> 00:01:19.689
For example, blank grounds are great for composting.

00:01:19.689 --> 00:01:23.250
Or, I prefer loose leaf blank.

00:01:23.250 --> 00:01:26.182
Here we are clearly talking about coffee grounds,

00:01:26.183 --> 00:01:28.045
and loose leaf tea.

00:01:28.045 --> 00:01:33.200
How do we capture these similarities and differences in the same embedding?

00:01:33.200 --> 00:01:35.415
By adding another dimension.

00:01:35.415 --> 00:01:37.230
Let's see how.

00:01:37.230 --> 00:01:40.020
Words can be close along one dimension.

00:01:40.019 --> 00:01:42.563
Here, tea and coffee are both beverages,

00:01:42.563 --> 00:01:45.295
but separated along some other dimension.

00:01:45.295 --> 00:01:49.840
Maybe this dimension captures all the variability among beverages.

00:01:49.840 --> 00:01:50.881
In a human language,

00:01:50.881 --> 00:01:54.495
there are many more dimensions along which word meanings can vary.

00:01:54.495 --> 00:01:57.582
And the more dimensions you can capture in your word vector,

00:01:57.582 --> 00:02:00.679
the more expressive that representation will be.

00:02:00.680 --> 00:02:03.245
But how many dimensions do you really need?

00:02:03.245 --> 00:02:06.320
Consider a typical neural network architecture

00:02:06.319 --> 00:02:10.055
designed for an NLP task, say word prediction.

00:02:10.055 --> 00:02:12.590
It's common to use a word embedding layer that

00:02:12.590 --> 00:02:15.604
produces a vector with a few hundred dimensions,

00:02:15.604 --> 00:02:20.004
but that's significantly small compared to using one heart encodings directly,

00:02:20.004 --> 00:02:22.777
which are as large as the size of the vocabulary,

00:02:22.777 --> 00:02:25.829
sometimes in tens of thousands of words.

00:02:25.830 --> 00:02:30.260
Also, if you learn the embedding as part of the model training process,

00:02:30.259 --> 00:02:32.599
you can obtain a representation that captures

00:02:32.599 --> 00:02:35.974
the dimensions that are most relevant for your task.

00:02:35.974 --> 00:02:37.579
This often adds complexity.

00:02:37.580 --> 00:02:39.350
So unless you're building a model for

00:02:39.349 --> 00:02:43.729
a very narrow application like one that deals with medical terminology,

00:02:43.729 --> 00:02:46.798
you can use a pre-trained embedding as a look-up.

00:02:46.798 --> 00:02:49.455
For example, work to veck or glove.

00:02:49.455 --> 00:02:53.300
Then you only need to train the layer specific to your task.

00:02:53.300 --> 00:02:57.050
Compare this with the network architecture for a computer vision task, say,

00:02:57.050 --> 00:03:01.085
image classification, the raw input here is also very high dimensional.

00:03:01.085 --> 00:03:07.750
For example, even 128 by 128 Image contains over 16 thousand pixels.

00:03:07.750 --> 00:03:10.460
We typically use convolutional layers to exploit

00:03:10.460 --> 00:03:15.145
the spatial relationships and image data and reduce this dimensionality.

00:03:15.145 --> 00:03:19.415
Early stages and visual processing are often transferable across tasks,

00:03:19.414 --> 00:03:23.257
so it is common to use some pre-trained layers from an existing network,

00:03:23.258 --> 00:03:28.406
like Alex nad or BTG 16 and only learn the later layers.

00:03:28.406 --> 00:03:31.280
Come to think of it, using an embedding look up for NLP

00:03:31.280 --> 00:03:34.905
is not on like using pre-treated layers for computer vision.

00:03:34.905 --> 00:03:37.400
Both are great examples of transfer learning.

