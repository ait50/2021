WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.774
Word2vec is just one type of forward embedding.

00:00:03.774 --> 00:00:08.849
Recently, several other related approaches have been proposed that are really promising.

00:00:08.849 --> 00:00:14.429
GloVe or global vectors for word representation is one such approach that tries to

00:00:14.429 --> 00:00:17.054
directly optimize the vector representation of

00:00:17.054 --> 00:00:20.300
each word just using co- occurrence statistics,

00:00:20.300 --> 00:00:24.269
unlike word2vec which sets up an ancillary prediction task.

00:00:24.269 --> 00:00:30.359
First, the probably that word j appears in the context of word i is computed,

00:00:30.359 --> 00:00:36.090
pj given i for all word pairs ij in a given corpus.

00:00:36.090 --> 00:00:39.295
What do we mean by j appears in context of i?

00:00:39.295 --> 00:00:43.525
Simply that word j is present in the vicinity of word i,

00:00:43.524 --> 00:00:44.835
either right next to it,

00:00:44.835 --> 00:00:46.890
or a few words away.

00:00:46.890 --> 00:00:50.984
We count all such occurrences of i and j in our text collection,

00:00:50.984 --> 00:00:53.939
and then normalize account to get a probability.

00:00:53.939 --> 00:00:59.309
Then, a random vector is initialized for each word, actually two vectors.

00:00:59.310 --> 00:01:02.640
One for the word when it is acting as a context,

00:01:02.640 --> 00:01:07.284
and one when it is acting as the target. So far, so good.

00:01:07.284 --> 00:01:09.200
Now, for any pair of words, ij,

00:01:09.200 --> 00:01:13.500
we want the dot product of their word vectors,

00:01:13.500 --> 00:01:18.045
w_i times w_j, to be equal to their co-occurrence probability.

00:01:18.045 --> 00:01:21.540
Using this as our goal and a suitable last function,

00:01:21.540 --> 00:01:24.845
we can iteratively optimize these word vectors.

00:01:24.844 --> 00:01:27.390
The result should be a set of vectors that capture

00:01:27.390 --> 00:01:31.489
the similarities and differences between individual words.

00:01:31.489 --> 00:01:33.780
If you look at it from another point of view,

00:01:33.780 --> 00:01:35.579
we are essentially factorizing

00:01:35.579 --> 00:01:39.689
the co-occurrence probability matrix into two smaller matrices.

00:01:39.689 --> 00:01:42.454
This is the basic idea behind GloVe.

00:01:42.454 --> 00:01:43.984
All that sounds good,

00:01:43.984 --> 00:01:46.900
but why co-occurrence probabilities?

00:01:46.900 --> 00:01:48.580
Consider two context words,

00:01:48.579 --> 00:01:50.694
say ice and steam,

00:01:50.694 --> 00:01:54.139
and two target words, solid and water.

00:01:54.140 --> 00:01:59.370
You would come across solid more often in the context of ice than steam, right?

00:01:59.370 --> 00:02:03.840
But water could occur in either context with roughly equal probability.

00:02:03.840 --> 00:02:06.020
At least, that's what we would expect.

00:02:06.019 --> 00:02:10.724
Surprise. That's exactly what co-occurrence probabilities reflect.

00:02:10.724 --> 00:02:12.280
Given a large corpus,

00:02:12.280 --> 00:02:15.145
you'll find that the ratio of P solid given

00:02:15.145 --> 00:02:19.150
ice to P solid given steam is much greater than one,

00:02:19.150 --> 00:02:25.270
while the ratio of P water given ice and P water given steam is close to one.

00:02:25.270 --> 00:02:27.985
Thus, we see that co-occurrence probabilities

00:02:27.985 --> 00:02:31.570
already exhibit some of the properties we want to capture.

00:02:31.569 --> 00:02:34.000
In fact, one refinement over using

00:02:34.000 --> 00:02:38.919
raw probability values is to optimize for the ratio of probabilities.

00:02:38.919 --> 00:02:41.514
Now, there are a lot of subtleties here,

00:02:41.514 --> 00:02:46.869
not the least of which is the fact that the co-occurence probability matrix is huge.

00:02:46.870 --> 00:02:48.129
At the same time,

00:02:48.129 --> 00:02:51.204
co-occurrence probability values are typically very low,

00:02:51.205 --> 00:02:54.840
so it makes sense to work with the log of these values.

00:02:54.840 --> 00:02:57.724
I encourage you to read the original paper that introduced

00:02:57.724 --> 00:03:01.000
GloVe to get a better understanding of this technique.

