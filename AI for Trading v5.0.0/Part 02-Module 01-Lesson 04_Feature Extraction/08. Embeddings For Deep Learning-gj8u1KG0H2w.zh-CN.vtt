WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.830
词嵌入快速成为人们实际选择的词表示法

00:00:04.830 --> 00:00:07.890
主要用于深度神经网络

00:00:07.889 --> 00:00:11.099
但是 为什么这些技术的效果这么好？

00:00:11.099 --> 00:00:15.419
你可以对词进行算术运算 

00:00:15.419 --> 00:00:19.734
例如 woman - man + king = queen , 这难道不神奇吗？

00:00:19.734 --> 00:00:22.574
关键在于分布假设

00:00:22.574 --> 00:00:28.574
根据分布假设 同一上下文中出现的词有相似含义

00:00:28.574 --> 00:00:30.719
例如这个句子

00:00:30.719 --> 00:00:33.774
“你想来杯&lt;u&gt;     &lt;/u&gt;吗？”

00:00:33.774 --> 00:00:38.609
那么 这个句子呢 “我想要杯黑&lt;u&gt;     ”&lt;/u&gt;

00:00:38.609 --> 00:00:44.335
还有一个句子 “早上我想先喝&lt;u&gt;      &lt;/u&gt; 再做其它事情”

00:00:44.335 --> 00:00:45.789
你认为是什么？

00:00:45.789 --> 00:00:49.890
“茶”？ “咖啡”？ 是哪个词给了你提示？

00:00:49.890 --> 00:00:55.380
“杯”？ “黑”？ “早上”？ 两个都有可能 对吧？

00:00:55.380 --> 00:00:56.580
这就是关键所在

00:00:56.579 --> 00:01:00.719
在这些上下文中 “茶”和“咖啡”实际是相似的

00:01:00.719 --> 00:01:05.500
因此 使用大量句子训练嵌入模型时

00:01:05.500 --> 00:01:11.484
拥有相同上下文的词汇被拉得越来越近

00:01:11.484 --> 00:01:15.989
当然 在一些上下文中 “茶”和“咖啡”是有差异的

00:01:15.989 --> 00:01:19.689
例如 “&lt;u&gt;     &lt;/u&gt;渣非常适合做堆肥”

00:01:19.689 --> 00:01:23.250
或者 “我喜欢喝散叶&lt;u&gt;     &lt;/u&gt;”

00:01:23.250 --> 00:01:26.182
很明显 我们在这里说的是“咖啡渣”

00:01:26.183 --> 00:01:28.045
和“散叶茶”

00:01:28.045 --> 00:01:33.200
如何在同一次嵌入中捕捉这些相似性和差异性？

00:01:33.200 --> 00:01:35.415
方法就是增加另一个维度

00:01:35.415 --> 00:01:37.230
我们看一下如何增加维度

00:01:37.230 --> 00:01:40.020
在一个维度上 词可能互相接近

00:01:40.019 --> 00:01:42.563
这里 “茶”和“咖啡”都是饮料

00:01:42.563 --> 00:01:45.295
但在另一个维度上 可能互相远离

00:01:45.295 --> 00:01:49.840
可能这个维度包含了各种饮料之间的所有差异性

00:01:49.840 --> 00:01:50.881
在自然语言中

00:01:50.881 --> 00:01:54.495
还有很多维度 在这些维度上 词的含义各不相同

00:01:54.495 --> 00:01:57.582
词向量中捕捉的维度越多

00:01:57.582 --> 00:02:00.679
表示法的表达越充分

00:02:00.680 --> 00:02:03.245
但是 到底需要多少个维度呢？

00:02:03.245 --> 00:02:06.320
想象用于 NLP 任务 例如词预测的

00:02:06.319 --> 00:02:10.055
一个典型神经网络架构

00:02:10.055 --> 00:02:12.590
我们常见的是使用一个词嵌入层

00:02:12.590 --> 00:02:15.604
生成具有几百个维度的向量

00:02:15.604 --> 00:02:20.004
但与直接使用独热编码相比 这个数量小得多

00:02:20.004 --> 00:02:22.777
独热编码有时可以达到

00:02:22.777 --> 00:02:25.829
几万个词的词汇表的大小

00:02:25.830 --> 00:02:30.260
另外 如果你在模型训练过程中训练词嵌入

00:02:30.259 --> 00:02:32.599
你会学习到一种表示法

00:02:32.599 --> 00:02:35.974
其捕捉了与你任务的相关性最强的维度

00:02:35.974 --> 00:02:37.579
这通常会增加复杂度

00:02:37.580 --> 00:02:39.350
因此 除非你构建的是

00:02:39.349 --> 00:02:43.729
适用范围非常小的模型 例如 处理医疗术语的模型

00:02:43.729 --> 00:02:46.798
可以使用预训练的词嵌入模型进行查找

00:02:46.798 --> 00:02:49.455
例如 Word2vec 或 GloVe

00:02:49.455 --> 00:02:53.300
然后你只需要训练与任务相关的特定层

00:02:53.300 --> 00:02:57.050
将其与用于计算机视觉 例如 图像分类的任务的网络架构相比较

00:02:57.050 --> 00:03:01.085
这里的原始输入也是高维度的

00:03:01.085 --> 00:03:07.750
例如 即使一张 128*128 的图像 也包含 16000 多个像素

00:03:07.750 --> 00:03:10.460
我们一般使用卷积层

00:03:10.460 --> 00:03:15.145
探索图像数据中的空间关系 减少维数

00:03:15.145 --> 00:03:19.415
早期的视觉处理技术可以应用在不同的任务中

00:03:19.414 --> 00:03:23.257
因此通常使用现有网络中的某些预训练层

00:03:23.258 --> 00:03:28.406
例如 Alex nad 或 BTG 16 只学习后面的层

00:03:28.406 --> 00:03:31.280
想想看 使用词嵌入查找进行 NLP

00:03:31.280 --> 00:03:34.905
与计算机视觉使用预处理层没有什么不同

00:03:34.905 --> 00:03:37.400
两者都是很好的迁移学习的例子

