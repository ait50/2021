WEBVTT
Kind: captions
Language: pt-BR

00:00:00.066 --> 00:00:02.302
Embeddings de palavras estão
se tornando

00:00:02.335 --> 00:00:04.771
a escolha padrão
para representar palavras,

00:00:04.804 --> 00:00:07.841
especialmente para uso
em redes neurais profundas.

00:00:07.874 --> 00:00:11.044
Mas por que estas técnicas
funcionam tão bem?

00:00:11.077 --> 00:00:15.382
Não é quase mágico você poder fazer
aritmética com palavras,

00:00:15.415 --> 00:00:19.719
como mulher-homem+rei
=rainha?

00:00:19.753 --> 00:00:22.655
A resposta pode estar
na hipótese distributiva,

00:00:22.689 --> 00:00:26.459
que afirma que palavras que ocorrem
nos mesmos contextos

00:00:26.493 --> 00:00:28.561
tendem a ter
significados parecidos.

00:00:28.595 --> 00:00:30.697
Por exemplo,
considere esta frase:

00:00:30.730 --> 00:00:33.767
"Você gostaria de tomar
uma xícara de______?"

00:00:33.800 --> 00:00:38.605
Ok. E: "Eu gosto
do meu ______ preto."

00:00:38.638 --> 00:00:39.906
Mais uma:

00:00:39.939 --> 00:00:44.344
"Preciso do meu ______ matinal
antes de fazer qualquer coisa."

00:00:44.377 --> 00:00:45.779
O que você acha?

00:00:45.812 --> 00:00:49.883
Chá? Café?
O que lhe deu a dica?

00:00:49.916 --> 00:00:52.752
Xícara? Preto? Matinal?

00:00:52.786 --> 00:00:55.388
Mas podia ser qualquer um
dos dois, certo?

00:00:55.422 --> 00:00:56.656
E esta é a questão.

00:00:56.690 --> 00:01:00.727
Nestes contextos,
chá e café são similares.

00:01:00.760 --> 00:01:03.863
Assim, quando uma grande coleção
de frases é usada

00:01:03.897 --> 00:01:05.532
para aprender um embedding,

00:01:05.565 --> 00:01:07.867
palavras com palavras
de contexto comuns

00:01:07.901 --> 00:01:11.504
tendem a ficar
cada vez mais próximas.

00:01:11.538 --> 00:01:16.009
É claro, pode haver contextos
em que chá e café não são similares.

00:01:16.042 --> 00:01:17.143
Por exemplo:

00:01:17.177 --> 00:01:19.713
"Grãos de____
são ótimos para compostagem!"

00:01:19.746 --> 00:01:23.283
Ou: "Prefiro____
com folhas soltas."

00:01:23.316 --> 00:01:26.219
Aqui estamos claramente falando
de grãos de café

00:01:26.252 --> 00:01:28.088
e chá com folhas soltas.

00:01:28.121 --> 00:01:33.259
Como capturamos similaridades
e diferenças no mesmo embedding?

00:01:33.293 --> 00:01:35.462
Adicionando outra dimensão.

00:01:35.495 --> 00:01:37.263
Vamos ver como.

00:01:37.297 --> 00:01:40.066
Palavras podem estar próximas
numa dimensão -

00:01:40.100 --> 00:01:42.602
aqui, chá e café
são bebidas -,

00:01:42.635 --> 00:01:45.338
mas separadas
em outra dimensão.

00:01:45.372 --> 00:01:49.909
Talvez esta dimensão capture
toda a variabilidade entre bebidas.

00:01:49.943 --> 00:01:52.545
Numa linguagem humana,
há muitas outras dimensões

00:01:52.579 --> 00:01:54.647
onde significados
de palavras variam.

00:01:54.681 --> 00:01:57.650
Quanto mais palavras você capturar
no vetor de palavras,

00:01:57.684 --> 00:02:00.754
mais expressiva
aquela representação será.

00:02:00.787 --> 00:02:03.323
Mas de quantas dimensões
você precisa?

00:02:03.356 --> 00:02:06.426
Considere uma arquitetura
de rede neural típica

00:02:06.459 --> 00:02:10.130
projetada para uma tarefa PNL,
como previsão de palavras.

00:02:10.163 --> 00:02:12.599
É comum usar uma camada
de embedding de palavras

00:02:12.632 --> 00:02:15.702
que produz um vetor
com algumas centenas de dimensões,

00:02:15.735 --> 00:02:17.671
mas isto é
significativamente pequeno

00:02:17.704 --> 00:02:20.173
comparado a usar diretamente
códigos one-hot,

00:02:20.206 --> 00:02:22.876
que são tão grandes
quanto o tamanho do vocabulário,

00:02:22.909 --> 00:02:25.912
às vezes em dezenas
de milhares de palavras.

00:02:25.945 --> 00:02:28.081
Também,
se você aprender o embedding

00:02:28.114 --> 00:02:30.350
como parte do processo
do treino do modelo,

00:02:30.383 --> 00:02:32.018
você pode obter
a representação

00:02:32.052 --> 00:02:36.089
que captura as dimensões
mais relevantes para a sua tarefa.

00:02:36.122 --> 00:02:37.691
Isto adiciona complexidade.

00:02:37.724 --> 00:02:40.994
Ao menos que construa um modelo
para uma aplicação bem limitada,

00:02:41.027 --> 00:02:43.830
como um modelo que lida
com terminologia médica,

00:02:43.863 --> 00:02:46.900
você pode usar embeddings
pré-treinados como pesquisa.

00:02:46.933 --> 00:02:49.569
Por exemplo,
Word2Vec ou GloVe.

00:02:49.602 --> 00:02:53.440
Então você só precisa treinar
a camada específica para a tarefa.

00:02:53.473 --> 00:02:55.308
Compare com a arquitetura
de rede

00:02:55.342 --> 00:02:58.644
da tarefa de visão computacional,
como uma classificação de imagem.

00:02:58.677 --> 00:03:01.214
O input bruto aqui também
é bem dimensional.

00:03:01.247 --> 00:03:04.751
Por exemplo,
até imagens de 128 por 128

00:03:04.784 --> 00:03:07.887
contêm mais de 16 mil pixels.

00:03:07.921 --> 00:03:10.056
Tipicamente usamos
camadas convolucionais

00:03:10.090 --> 00:03:13.093
para explorar as relações espaciais
em dados de imagem

00:03:13.126 --> 00:03:15.295
e reduzir
esta dimensionalidade.

00:03:15.328 --> 00:03:17.397
Estágios iniciais
do processamento visual

00:03:17.430 --> 00:03:19.566
são transferíveis
entre tarefas,

00:03:19.599 --> 00:03:23.403
então é comum usar camadas
pré-treinadas de uma rede existente,

00:03:23.436 --> 00:03:28.575
como AlexNet ou VGG-16,
e só aprender as últimas camadas.

00:03:28.608 --> 00:03:32.178
Usar uma pesquisa de embedding
para uma PNL não é diferente

00:03:32.212 --> 00:03:35.115
de usar camadas pré-treinadas
para visão computacional.

00:03:35.148 --> 00:03:38.151
Ambas são ótimos exemplos
de transferência de aprendizado.

