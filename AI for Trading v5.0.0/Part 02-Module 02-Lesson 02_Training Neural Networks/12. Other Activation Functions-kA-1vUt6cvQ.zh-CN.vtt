WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.419
调整这个最好的方法是改变激活函数

00:00:04.419 --> 00:00:07.615
这是另外一种方法叫做双曲正切

00:00:07.615 --> 00:00:10.224
在这个公式的下面

00:00:10.224 --> 00:00:15.795
e 的 x 次方减去 e 的 -x 次方除以 e 的 x 次方加上 e 的 -x 次方

00:00:15.795 --> 00:00:18.004
这个公式与 sigmoid 函数相似

00:00:18.004 --> 00:00:20.914
不过由于我们的范围在 -1 到 1 之间

00:00:20.914 --> 00:00:23.089
导数更大

00:00:23.089 --> 00:00:24.554
这个较小的区别实际上会

00:00:24.554 --> 00:00:27.589
在神经网络中产生巨大进步

00:00:27.589 --> 00:00:33.115
另外一个非常流行的激活函数是修正线性单元或 ReLU

00:00:33.115 --> 00:00:35.995
这是个很简单的函数

00:00:35.994 --> 00:00:38.824
它只表示 如果为正

00:00:38.825 --> 00:00:40.670
我会返回相同值

00:00:40.670 --> 00:00:44.395
如果为负 我会返回 0

00:00:44.395 --> 00:00:48.575
另一种观察方法是最大值在 x 和 0 之间

00:00:48.575 --> 00:00:52.270
除了 sigmoid 这个函数是最常使用的

00:00:52.270 --> 00:00:55.585
可以在不牺牲精确度的前提下 极大提高训练

00:00:55.585 --> 00:00:59.850
因为如果值为正 导数等于 1

00:00:59.850 --> 00:01:02.609
有趣的是 这个函数很少会打破线性

00:01:02.609 --> 00:01:06.495
得出复杂的非线性解

00:01:06.495 --> 00:01:08.469
现在利用更好的激活函数

00:01:08.469 --> 00:01:12.605
我们乘以导数 获得任何权重的导数时

00:01:12.605 --> 00:01:14.260
乘积会成为更大的数字

00:01:14.260 --> 00:01:18.715
这个数字可以让导数更小

00:01:18.715 --> 00:01:21.340
可以使我们进行梯度下降

00:01:21.340 --> 00:01:25.510
我们通过绘制函数 展现 ReLU 单元

00:01:25.510 --> 00:01:30.730
这是个包含一批 ReLU 激活单元的多层感知器例子

00:01:30.730 --> 00:01:33.405
注意到最后一个单元是 sigmoid

00:01:33.405 --> 00:01:38.385
因为我们最后的输出需要是 0 到 1 之间的概率

00:01:38.385 --> 00:01:40.969
不过我们最后一个单元是关于 ReLU 的内容

00:01:40.969 --> 00:01:45.280
实际上我们最后讲解回归模型 预测值

00:01:45.280 --> 00:01:49.000
这将会在纳米学程的循环神经网络课程中非常有用

