WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.129
我们从刚刚停下的地方继续

00:00:02.129 --> 00:00:04.794
我们有一个复杂的网络架构

00:00:04.796 --> 00:00:08.929
比我们所需的要复杂 但是我们需要采用这一架构

00:00:08.929 --> 00:00:11.509
我们看看训练流程

00:00:11.509 --> 00:00:15.929
我们在第一个 epoch 中以随机权重开始 获得这样的模型

00:00:15.929 --> 00:00:17.885
有很多错误

00:00:17.885 --> 00:00:22.524
假设训练 20 个 epoch 后 我们获得了很好的模型

00:00:22.524 --> 00:00:26.085
但是假设继续 100 个 epoch

00:00:26.085 --> 00:00:28.196
我们将获得数据拟合效果更好的模型

00:00:28.196 --> 00:00:30.589
但是我们看到有点过拟合了

00:00:30.588 --> 00:00:32.219
假设我们再训练 600 个 epoch

00:00:32.219 --> 00:00:36.325
模型变得严重过拟合

00:00:36.325 --> 00:00:42.399
可以看到蓝色区域差不多就是一堆围绕蓝点的圆圈

00:00:42.399 --> 00:00:44.390
这样对训练数据拟合的很好

00:00:44.390 --> 00:00:46.590
但是无法泛化

00:00:46.590 --> 00:00:49.170
假设蓝色区域有个新的蓝点

00:00:49.170 --> 00:00:54.140
这个点很有可能会被分类为红色 除非它离某个蓝点非常近

00:00:54.140 --> 00:01:00.445
我们尝试添加一些测试集来评估这些模型 例如添加这些点

00:01:00.445 --> 00:01:03.060
我们将训练集和测试集相对于

00:01:03.060 --> 00:01:06.209
每个 epoch 的误差绘制成图表

00:01:06.209 --> 00:01:07.865
对于第一个 epoch

00:01:07.864 --> 00:01:09.868
因为模型完全是随机的

00:01:09.870 --> 00:01:14.840
那么对训练集和测试集的分类都很糟糕

00:01:14.840 --> 00:01:18.284
所以训练误差和测试误差都很大

00:01:18.284 --> 00:01:19.844
我们将其画到这里

00:01:19.843 --> 00:01:21.493
对于第 20 个 epoch

00:01:21.492 --> 00:01:25.152
模型好多了 能够很好地拟合训练数据

00:01:25.152 --> 00:01:27.608
测试集也能很好地拟合

00:01:27.608 --> 00:01:32.669
所以两个误差都相对比较小 画在这里

00:01:32.670 --> 00:01:34.417
对于第 100 个 epoch

00:01:34.417 --> 00:01:36.358
我们看到我们开始过拟合了

00:01:36.358 --> 00:01:41.478
模型能很好地拟合数据 但是开始在测试数据中出错了

00:01:41.480 --> 00:01:44.316
我们看到训练误差不断下降

00:01:44.316 --> 00:01:46.838
但是测试误差开始升高了

00:01:46.837 --> 00:01:48.393
将它们画到这里

00:01:48.394 --> 00:01:51.795
对于第 600 个 epoch 我们严重过拟合了

00:01:51.795 --> 00:01:56.189
可以看到训练误差非常小 因为数据可以非常完美地拟合训练集

00:01:56.188 --> 00:02:00.858
但是模型在测试集中犯了大量错误

00:02:00.858 --> 00:02:03.112
所以测试误差很大

00:02:03.114 --> 00:02:04.939
将它们画到这里

00:02:04.938 --> 00:02:09.937
现在画出连接训练误差和测试误差的曲线

00:02:09.937 --> 00:02:11.352
在此图中

00:02:11.354 --> 00:02:15.850
很清楚何时停止欠拟合并开始过拟合

00:02:15.849 --> 00:02:20.609
训练曲线始终会下降 因为在我们训练模型时

00:02:20.610 --> 00:02:23.569
我们不断越来越好地拟合数据

00:02:23.568 --> 00:02:29.193
当我们欠拟合时 测试误差很大 因为模型不够精确

00:02:29.193 --> 00:02:33.038
随着模型开始很好地泛化 它不断降低

00:02:33.038 --> 00:02:37.158
直到抵达最低点（黄金点）

00:02:37.157 --> 00:02:40.029
最终 当我们超过该点后

00:02:40.030 --> 00:02:43.205
模型开始过拟合

00:02:43.205 --> 00:02:47.530
因为它已无法泛化并开始记住训练数据

00:02:47.530 --> 00:02:50.783
这个图表叫做模型复杂度图表

00:02:50.782 --> 00:02:53.437
在 y 轴上是误差测量结果

00:02:53.437 --> 00:02:59.203
在 x 轴上是模型复杂度测量结果

00:02:59.205 --> 00:03:01.960
这里是 epoch 数量

00:03:01.960 --> 00:03:03.550
可以看出

00:03:03.550 --> 00:03:08.064
在左侧是很高的测试和训练误差 所以我们欠拟合

00:03:08.063 --> 00:03:14.842
在右侧有很高的测试误差和很低的训练误差 所以我们过拟合

00:03:14.842 --> 00:03:16.477
在中间

00:03:16.479 --> 00:03:19.335
是令人满意的黄金点

00:03:19.335 --> 00:03:22.740
这个就决定了我们要使用的 epoch 次数

00:03:22.740 --> 00:03:25.314
总结下

00:03:25.312 --> 00:03:31.643
我们要做的是降低梯度 直到测试误差停止降低并开始增大

00:03:31.645 --> 00:03:34.044
这时候 我们就停止

00:03:34.044 --> 00:03:40.090
这个算法叫做早期停止法 广泛用于训练神经网络

