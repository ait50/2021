WEBVTT
Kind: captions
Language: pt-BR

00:00:00.367 --> 00:00:04.367
Vejamos o que o algoritmo
gradiente descendente faz.

00:00:04.401 --> 00:00:07.167
Nós estamos no topo
do Monte Errorest

00:00:07.201 --> 00:00:09.234
e precisamos descer.

00:00:09.268 --> 00:00:12.367
Para isso, nós damos
um monte de passos

00:00:13.467 --> 00:00:16.634
seguindo o negativo
do gradiente da altura,

00:00:16.668 --> 00:00:18.400
que é a função de erro.

00:00:19.200 --> 00:00:21.267
Cada passo é
chamado de epoch.

00:00:21.301 --> 00:00:23.601
Quando nos referimos
à quantidade de passos,

00:00:23.635 --> 00:00:25.767
nos referimos
à quantidade de epochs.

00:00:25.801 --> 00:00:27.901
Vejamos o que ocorre
em cada epoch.

00:00:27.935 --> 00:00:31.667
Nós pegamos a entrada,
todos os nossos dados,

00:00:31.701 --> 00:00:34.868
e passamos pela rede neural.

00:00:34.902 --> 00:00:36.667
Depois nós encontramos
as previsões,

00:00:36.701 --> 00:00:38.334
calculamos o erro,

00:00:38.368 --> 00:00:41.767
ou seja, a distância entre eles
e os rótulos,

00:00:41.801 --> 00:00:44.300
e fazemos a propagação
de retorno do erro

00:00:44.334 --> 00:00:47.801
para atualizar os pesos
na rede neural.

00:00:47.835 --> 00:00:51.667
Isso nos dará um limite melhor
para previsão de dados.

00:00:51.701 --> 00:00:53.834
Isso é feito
com todos os dados.

00:00:53.868 --> 00:00:55.634
Se tivermos
muitos pontos de dados,

00:00:55.668 --> 00:00:57.167
que é o que
geralmente acontece,

00:00:57.201 --> 00:00:59.567
teremos cálculos matriciais
gigantescos

00:00:59.601 --> 00:01:01.701
que usam muita memória.

00:01:01.735 --> 00:01:04.000
Tudo isso
para um único passo.

00:01:04.034 --> 00:01:07.434
Se houver muitos passos,
imagine o tempo

00:01:07.468 --> 00:01:09.133
e a potência de cálculo
necessários.

00:01:09.167 --> 00:01:12.067
Há algo que podemos fazer
para acelerar isto?

00:01:12.101 --> 00:01:15.868
Nós precisamos ligar
todos os dados

00:01:15.902 --> 00:01:17.734
sempre que dermos um passo?

00:01:17.768 --> 00:01:19.734
Se os dados estiverem
bem distribuídos,

00:01:19.768 --> 00:01:21.601
um pequeno subconjunto

00:01:21.635 --> 00:01:24.801
nos dará uma ideia
sobre o que seria o gradiente.

00:01:24.835 --> 00:01:27.267
Talvez não seja a melhor
estimativa do gradiente,

00:01:27.301 --> 00:01:29.634
mas ele é rápido,
e como estamos iterando,

00:01:29.668 --> 00:01:31.434
isso pode ser uma boa ideia.

00:01:31.468 --> 00:01:35.434
É aqui que o gradiente
descendente estocástico age.

00:01:35.468 --> 00:01:40.267
A ideia por trás disso é pegar
pequenos subconjuntos de dados,

00:01:40.301 --> 00:01:42.434
rodá-los pela rede neural,

00:01:42.468 --> 00:01:46.200
calcular o gradiente da função
de erro dos pontos

00:01:46.234 --> 00:01:48.934
e dar um passo
naquela direção.

00:01:48.968 --> 00:01:51.567
Nós ainda queremos usar
todos os dados,

00:01:51.601 --> 00:01:53.601
então nós fazemos
o seguinte:

00:01:53.635 --> 00:01:56.434
nós dividimos os dados
em vários lotes.

00:01:56.468 --> 00:01:59.834
Neste exemplo nós temos
24 pontos.

00:01:59.868 --> 00:02:03.334
Nós os dividimos
em quatro lotes de 6 pontos.

00:02:03.368 --> 00:02:06.033
Nós pegamos os pontos
do primeiro lote

00:02:06.067 --> 00:02:08.434
e os rodamos da rede neural,

00:02:08.468 --> 00:02:11.200
calculamos o erro
e o gradiente

00:02:11.234 --> 00:02:13.801
e a propagação de retorno
para atualizar os pesos.

00:02:13.835 --> 00:02:15.434
Isso nos dará novos pesos,

00:02:15.468 --> 00:02:17.334
que definirão
regiões de limites,

00:02:17.368 --> 00:02:19.133
que vemos à esquerda.

00:02:19.167 --> 00:02:21.934
Pegamos os pontos
do segundo lote

00:02:21.968 --> 00:02:24.300
e fazemos o mesmo.

00:02:24.334 --> 00:02:26.934
Isso nos dará
pesos melhores

00:02:26.968 --> 00:02:29.234
e uma região limite melhor.

00:02:29.268 --> 00:02:32.334
Fazemos o mesmo
com o terceiro lote.

00:02:32.368 --> 00:02:36.200
Por fim, fazemos
com o quarto.

00:02:36.234 --> 00:02:37.701
Pronto.

00:02:37.735 --> 00:02:40.868
Nós pegamos os dados,
demos quatro passos,

00:02:40.902 --> 00:02:43.801
e, quando fizemos
o gradiente descendente normal,

00:02:43.835 --> 00:02:46.634
só demos um passo
com todos os dados.

00:02:46.668 --> 00:02:51.033
Os quatro passos são
menos precisos,

00:02:51.067 --> 00:02:52.601
mas, na prática,

00:02:52.635 --> 00:02:56.267
é melhor dar vários
pequenos passos não precisos

00:02:56.301 --> 00:02:58.300
do que dar um bom.

00:02:58.334 --> 00:02:59.934
Posteriormente, no curso,

00:02:59.968 --> 00:03:02.901
você aplicará o gradiente
descendente estocástico

00:03:02.935 --> 00:03:04.634
e verá os benefícios dele.

