WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.852
首先观察两个方程式得到相同的线段

00:00:04.852 --> 00:00:07.500
线段的方程式是 X1+X2=0

00:00:07.500 --> 00:00:10.664
原因在于第二个解法

00:00:10.664 --> 00:00:15.839
实际上只是第一个解法的纯量倍数 那么我们来看一下

00:00:15.839 --> 00:00:18.929
回想一下 预测是一个 sigmoid 线性函数

00:00:18.929 --> 00:00:20.579
所以在第一个例子中 对于点 (1,1)

00:00:20.579 --> 00:00:22.788
可以得到 σ (1+1)

00:00:22.789 --> 00:00:27.600
即 σ 2 等于 0.88

00:00:27.600 --> 00:00:29.370
因为这个点是蓝色的 所以结果不算差

00:00:29.370 --> 00:00:31.268
所以标签为 1

00:00:31.268 --> 00:00:32.460
对于点 (-1,-1)

00:00:32.460 --> 00:00:35.880
预测是 σ (-1-1)

00:00:35.880 --> 00:00:40.304
即 σ -2 等于 0.12

00:00:40.304 --> 00:00:45.359
因为这个点标签为 0 是红色的 所以结果不是最佳

00:00:45.359 --> 00:00:48.009
我们来看一下第二个模型

00:00:48.009 --> 00:00:51.335
点 (1,1) 得到预测是

00:00:51.335 --> 00:00:55.469
σ (10*1+10*1) 也就是 σ 20

00:00:55.469 --> 00:01:01.920
结果是 0.9999999979

00:01:01.920 --> 00:01:04.109
非常接近 1

00:01:04.109 --> 00:01:06.015
所以这是个很好的预测

00:01:06.015 --> 00:01:08.400
对于点 (-1,-1)

00:01:08.400 --> 00:01:13.350
σ (10*-1+10*-1)

00:01:13.349 --> 00:01:16.837
也就是 σ (-20)

00:01:16.837 --> 00:01:23.409
等于 0.0000000021

00:01:23.409 --> 00:01:27.256
非常接近 0 所以这是个很好的预测

00:01:27.256 --> 00:01:30.143
所以这个测试的答案是第二个模型

00:01:30.143 --> 00:01:32.150
第二个模型非常准确

00:01:32.150 --> 00:01:33.765
也意味着更好 对不对？

00:01:33.765 --> 00:01:35.909
最后一部分后 你觉得有点勉强

00:01:35.909 --> 00:01:38.909
因为这可能有点过度拟合

00:01:38.909 --> 00:01:40.954
你的预感是正确的

00:01:40.954 --> 00:01:43.814
这个问题过度拟合 不过程度比较轻微

00:01:43.814 --> 00:01:46.259
这是整个过程 以及第一个模型更好的原因

00:01:46.260 --> 00:01:49.206
即使得出更大的误差

00:01:49.206 --> 00:01:54.150
我们把 sigmoid 运用到较小值中 如 X1+X2

00:01:54.150 --> 00:01:59.484
我们得到左侧的函数 可以有梯度下降更好的坡度

00:01:59.484 --> 00:02:07.594
我们把线性函数乘以 10 后得到 σ (10X1+10X2)

00:02:07.594 --> 00:02:11.919
由于它们更接近 0 和 1 所以预测更好一些

00:02:11.919 --> 00:02:17.737
但是函数更加陡峭 这里也更难进行较大幅度的下降

00:02:17.737 --> 00:02:19.960
因为导数非常接近 0

00:02:19.960 --> 00:02:24.960
到达曲线中部时 导数非常大

00:02:24.960 --> 00:02:27.939
所以 为了合理使用梯度下降法

00:02:27.939 --> 00:02:33.585
相比右侧的模型来说 我们更想用左侧的模型

00:02:33.585 --> 00:02:35.200
从概念上来说

00:02:35.199 --> 00:02:37.259
右侧的模型非常稳定

00:02:37.259 --> 00:02:41.454
很难运用梯度下降法

00:02:41.455 --> 00:02:42.550
同时正如我们想象的

00:02:42.550 --> 00:02:45.910
这些点在右侧模型中分类错误

00:02:45.909 --> 00:02:51.250
会产生更大的误差 很难调优模型或纠正模型

00:02:51.250 --> 00:02:53.379
这可以引用著名哲学家和数学家

00:02:53.379 --> 00:02:58.264
伯特兰·罗素 (Bertrand Russell) 的名言进行总结

00:02:58.264 --> 00:03:00.819
人工智能的整个问题

00:03:00.819 --> 00:03:04.194
在于错误模型对自身非常确定

00:03:04.194 --> 00:03:07.634
而好的模型充满疑问

00:03:07.634 --> 00:03:08.764
现在问题是

00:03:08.764 --> 00:03:12.422
我们如何避免这种过度拟合发生呢？

00:03:12.423 --> 00:03:16.775
由于错误模型提供了更小误差 所以这并不简单

00:03:16.775 --> 00:03:20.865
我们要做的是对误差函数稍作调整

00:03:20.865 --> 00:03:24.250
大体来说 我们想要惩罚高系数

00:03:24.250 --> 00:03:25.969
所以我们这里要做的是

00:03:25.969 --> 00:03:32.164
利用原来的误差函数 权重大时 添加较大的一项

00:03:32.164 --> 00:03:34.569
两种方法可以做到这一点

00:03:34.569 --> 00:03:40.935
第一种方法是加上权重乘以常量 λ 绝对值的总和

00:03:40.935 --> 00:03:46.490
另一种方法是加上权重平方总和 再乘以相同的常数

00:03:46.490 --> 00:03:52.082
可以看到 如果权重大 这两个函数都大

00:03:52.082 --> 00:03:56.430
参数 λ 告诉我们惩罚系数的多少

00:03:56.430 --> 00:03:57.467
如果 λ 很大

00:03:57.467 --> 00:03:59.085
惩罚会很严重

00:03:59.085 --> 00:04:02.875
如果 λ 很小 那么无需对其进行严重的惩罚

00:04:02.875 --> 00:04:05.775
最后如果我们想要得到绝对值

00:04:05.775 --> 00:04:09.138
可以使用 L1 正则化

00:04:09.138 --> 00:04:10.944
如果想要得到平方

00:04:10.944 --> 00:04:13.969
可以使用 L2 正则化

00:04:13.969 --> 00:04:15.294
两者都比较通用

00:04:15.294 --> 00:04:18.069
这取决于我们的目标

00:04:18.069 --> 00:04:20.724
我们可以两者选其一

00:04:20.725 --> 00:04:27.185
对于使用 L1 和 L2 正则化 这里有一些通用规则

00:04:27.185 --> 00:04:32.634
使用 L1 正则化时 我们希望得到稀疏向量

00:04:32.634 --> 00:04:36.449
它表示较小权重趋向于 0

00:04:36.449 --> 00:04:40.449
所以如果我们想降低权重值 最终得到较小的数

00:04:40.449 --> 00:04:41.944
我们可以使用 L1

00:04:41.944 --> 00:04:44.550
这也利于特征选择

00:04:44.550 --> 00:04:47.550
有时候我们会遇到几百个特征问题

00:04:47.550 --> 00:04:52.379
并且L1 正则化可以帮我们选择哪一些更重要

00:04:52.379 --> 00:04:55.129
然后将其余的变为 0

00:04:55.129 --> 00:04:56.939
而 L2 正则化

00:04:56.939 --> 00:04:59.519
不支持稀疏向量

00:04:59.519 --> 00:05:02.894
因为它确保所有权重一致较小

00:05:02.894 --> 00:05:06.329
这个一般可以训练模型 得出更好结果

00:05:06.329 --> 00:05:09.854
所以这会是我们最常用的 现在我们来思考一下

00:05:09.855 --> 00:05:13.710
为什么 L1 正则化得出稀疏权重的向量

00:05:13.709 --> 00:05:18.495
而 L1 正则化得出较小同质权重的向量？

00:05:18.495 --> 00:05:20.545
原因是这样的

00:05:20.545 --> 00:05:23.030
如果我们利用向量 (1,0)

00:05:23.029 --> 00:05:26.384
权重绝对值的总和是 1

00:05:26.384 --> 00:05:30.267
权重平方的总和也是 1

00:05:30.267 --> 00:05:35.187
不过如果得到向量 (0.5, 0.5)

00:05:35.187 --> 00:05:39.298
权重绝对值的总和仍然是 1

00:05:39.298 --> 00:05:46.024
平方总和是 0.25+0.25 等于 0.5

00:05:46.024 --> 00:05:51.245
那么 L2 正则化更倾向于向量 (0.5, 0.5)

00:05:51.245 --> 00:05:53.599
而不是向量 (1,0)

00:05:53.600 --> 00:05:57.020
因为前者可以得出更小的平方总和

00:05:57.019 --> 00:06:00.000
反过来说得到更小的函数

