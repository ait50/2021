WEBVTT
Kind: captions
Language: pt-BR

00:00:00.087 --> 00:00:02.780
A primeira observação é que
as duas equações

00:00:02.813 --> 00:00:04.659
nos dão a mesma linha,

00:00:04.692 --> 00:00:08.682
a linha com a equação
x1 + x2 = 0.

00:00:08.715 --> 00:00:09.954
E o motivo para isto

00:00:09.987 --> 00:00:14.898
é que a solução 2 é apenas
um múltiplo escalar da solução 1.

00:00:14.931 --> 00:00:16.052
Vejamos.

00:00:16.085 --> 00:00:18.805
Lembre-se de que a predição
é um sigmoide da função linear.

00:00:18.838 --> 00:00:23.421
Então, no primeiro caso, para 0.11,
seria sigmoide de 1 + 1,

00:00:23.454 --> 00:00:27.443
que é sigmoide de 2,
que é 0.88.

00:00:27.476 --> 00:00:29.179
Não é ruim,
já que o ponto é azul,

00:00:29.212 --> 00:00:31.111
ou seja, tem rótulo 1.

00:00:31.144 --> 00:00:32.661
Para o ponto (-1, -1),

00:00:32.694 --> 00:00:36.007
a predição
é sigmoide de -1 + (- 1),

00:00:36.040 --> 00:00:38.720
que é sigmoide de -2,

00:00:38.753 --> 00:00:40.963
que é 0.12.

00:00:40.996 --> 00:00:42.150
Também não é ruim,

00:00:42.183 --> 00:00:43.818
já que o rótulo do ponto
tem um rótulo zero,

00:00:43.851 --> 00:00:45.399
já que é vermelho.

00:00:45.432 --> 00:00:47.805
Agora vamos ver o que acontece
com o segundo modelo.

00:00:47.838 --> 00:00:50.330
O ponto (1, 1)
tem uma predição

00:00:50.363 --> 00:00:53.423
sigmoide de 10 vezes 1,
mais 10 vezes 1,

00:00:53.456 --> 00:00:55.781
que é sigmoide de 20.

00:00:55.814 --> 00:01:01.790
Isto é 0.9999999979,

00:01:01.823 --> 00:01:04.297
que é bem próximo de 1,

00:01:04.330 --> 00:01:06.003
portanto é uma ótima predição.

00:01:06.036 --> 00:01:09.254
E o ponto (-1, -1)
tem uma predição

00:01:09.287 --> 00:01:13.974
sigmoide de 10 vezes -1,
mais 10 vezes -1,

00:01:14.007 --> 00:01:16.678
que é sigmoide de -20,

00:01:16.711 --> 00:01:23.128
e isso é 0.0000000021.

00:01:23.161 --> 00:01:27.197
Isso é bem perto de zero,
portanto é uma ótima predição.

00:01:27.230 --> 00:01:29.957
A resposta do teste
é o segundo modelo.

00:01:29.990 --> 00:01:32.012
O segundo modelo
é super preciso.

00:01:32.045 --> 00:01:33.910
Ou seja, é melhor, certo?

00:01:33.943 --> 00:01:36.410
Depois da última aula,
você deve estar um pouco relutante,

00:01:36.443 --> 00:01:39.127
já que isso aponta um pouco
para um sobreajuste.

00:01:39.160 --> 00:01:40.812
E seu palpite está correto.

00:01:40.845 --> 00:01:43.727
O problema se sobreajusta,
mas de uma forma sutil.

00:01:43.760 --> 00:01:45.005
É isto o que está acontecendo,

00:01:45.038 --> 00:01:47.225
e este é o motivo
para o segundo modelo ser melhor,

00:01:47.258 --> 00:01:48.988
mesmo gerando um grande erro.

00:01:49.021 --> 00:01:51.639
Quando aplicamos o sigmoide
a pequenos valores,

00:01:51.672 --> 00:01:54.472
como x1 + x2.

00:01:54.505 --> 00:01:56.279
temos a função da esquerda,

00:01:56.312 --> 00:01:59.826
que tem uma boa inclinação
no gradiente descendente.

00:01:59.859 --> 00:02:03.522
Quando multiplicamos
a função linear por 10

00:02:03.555 --> 00:02:07.978
e pegamos o sigmoide
de 10x1 + 10x2,

00:02:08.011 --> 00:02:12.256
nossas predições ficam bem melhores,
já que ficam mais próximas de 0 e 1.

00:02:12.289 --> 00:02:14.925
Mas a função fica
bem mais íngreme,

00:02:14.958 --> 00:02:17.707
e fica muito mais difícil de fazer
um gradiente descendente aqui,

00:02:17.740 --> 00:02:20.984
já que os derivativos
estão mais próximos de 0

00:02:21.017 --> 00:02:24.569
e muito grandes quando chegamos
ao meio da curva.

00:02:24.602 --> 00:02:25.734
Portanto,

00:02:25.767 --> 00:02:27.994
para fazer o gradiente descendente
corretamente,

00:02:28.027 --> 00:02:30.626
queremos um modelo
como o da esquerda

00:02:30.659 --> 00:02:33.487
mais do que um modelo
como o da direita.

00:02:33.520 --> 00:02:35.376
De maneira conceitual,

00:02:35.409 --> 00:02:38.077
o modelo da direita é
certeiro demais

00:02:38.110 --> 00:02:41.326
e dá pouco espaço para aplicar
o gradiente descendente.

00:02:41.359 --> 00:02:42.640
Também, como podemos imaginar,

00:02:42.673 --> 00:02:45.718
os pontos classificados
incorretamente no modelo da direita

00:02:45.751 --> 00:02:47.705
gerarão erros maiores,

00:02:47.738 --> 00:02:51.044
e será difícil afinar o modelo
para corrigi-los.

00:02:51.077 --> 00:02:53.042
Isso pode ser resumido
na citação

00:02:53.075 --> 00:02:58.149
do famoso filósofo e matemático
"BertrAIND" Russel.

00:02:58.182 --> 00:03:00.870
"O problema
da inteligência artificial

00:03:00.903 --> 00:03:03.955
é que os modelos ruins
são tão certos sobre si mesmos

00:03:03.988 --> 00:03:07.165
e os modelos bons
estão cheios de dúvidas."

00:03:07.198 --> 00:03:08.641
A questão é,

00:03:08.674 --> 00:03:12.543
como prevenir
este tipo de sobreajuste?

00:03:12.576 --> 00:03:14.152
Não parece fácil,

00:03:14.185 --> 00:03:16.870
já que modelos ruins
geram erros menores.

00:03:16.903 --> 00:03:20.698
O que devemos fazer é modificar
um pouco a função de erro.

00:03:20.731 --> 00:03:23.957
Basicamente,
queremos punir coeficientes altos.

00:03:23.990 --> 00:03:29.757
Pegamos a função de erro
e adicionamos um termo,

00:03:29.790 --> 00:03:32.490
que será grande
se os pesos forem grandes.

00:03:32.523 --> 00:03:34.391
Há duas maneiras
de se fazer isso.

00:03:34.424 --> 00:03:38.873
Uma é adicionar as somas
de valores absolutos dos pesos

00:03:38.906 --> 00:03:41.042
vezes um lambda constante.

00:03:41.075 --> 00:03:44.530
Outra é adicionar a soma
dos quadrados dos pesos

00:03:44.563 --> 00:03:46.703
vezes essa mesma constante.

00:03:46.736 --> 00:03:48.242
Como pode ver,

00:03:48.275 --> 00:03:51.852
as duas serão grandes
se os pesos forem grandes.

00:03:51.885 --> 00:03:53.093
O parâmetro lambda nos dirá

00:03:53.126 --> 00:03:56.252
o quanto queremos
penalizar os coeficientes.

00:03:56.285 --> 00:03:59.000
Se lambda for grande,
nós os penalizaremos bastante.

00:03:59.033 --> 00:04:02.765
E se lambda for pequeno,
não os penalizaremos muito.

00:04:02.798 --> 00:04:05.625
E, finalmente, se decidirmos usar
os valores absolutos,

00:04:05.658 --> 00:04:08.877
faremos a regularização L1,

00:04:08.910 --> 00:04:10.895
e se decidirmos usar
os quadrados,

00:04:10.928 --> 00:04:13.888
faremos a regularização L2.

00:04:13.921 --> 00:04:15.178
Ambos são bem populares,

00:04:15.211 --> 00:04:17.852
e, dependendo dos objetivos
ou aplicações,

00:04:17.885 --> 00:04:20.181
aplicamos um ou outro.

00:04:21.716 --> 00:04:23.383
Aqui estão
algumas regras gerais

00:04:23.416 --> 00:04:27.573
para decidir
entre as regularizações L1 e L2.

00:04:27.606 --> 00:04:29.522
Quando aplicamos L1,

00:04:29.555 --> 00:04:32.464
tendemos a terminar
com vetores esparsos.

00:04:32.497 --> 00:04:36.185
Ou seja, pesos pequenos
tenderão a ir para 0.

00:04:36.218 --> 00:04:38.319
e quisermos reduzir
o número de pesos

00:04:38.352 --> 00:04:40.270
e terminar
com um conjunto pequeno,

00:04:40.303 --> 00:04:42.245
poderemos usar L1.

00:04:42.278 --> 00:04:44.285
Isto também é bom
para seleção de recursos.

00:04:44.318 --> 00:04:47.502
Às vezes temos um problema
com centenas de recursos,

00:04:47.535 --> 00:04:50.308
e a regularização L1
nos ajuda a selecionar

00:04:50.341 --> 00:04:52.162
quais são importantes

00:04:52.195 --> 00:04:55.000
e transforma o resto em 0.

00:04:55.033 --> 00:04:57.125
L2, por outro lado,

00:04:57.158 --> 00:04:59.069
tende a não favorecer
vetores esparsos,

00:04:59.102 --> 00:05:03.164
já que tenta manter todos os pesos
homogeneamente pequenos.

00:05:03.197 --> 00:05:04.949
Esta normalmente
gera melhores resultados

00:05:04.982 --> 00:05:06.136
para o treinamento de modelos

00:05:06.169 --> 00:05:08.259
e é o que mais usaremos.

00:05:08.292 --> 00:05:09.779
Agora vamos pensar um pouco.

00:05:09.812 --> 00:05:13.697
Por que a regularização L1
produz vetores com pesos esparsos

00:05:13.730 --> 00:05:18.614
e a regularização L2 produz vetores
com pesos homogeneamente pequenos?

00:05:18.647 --> 00:05:20.766
Aqui está uma ideia do porquê.

00:05:20.799 --> 00:05:23.087
Se pegarmos o vetor (0, 1),

00:05:23.120 --> 00:05:26.576
as somas dos valores absolutos
dos pesos serão 1,

00:05:26.609 --> 00:05:30.298
e as somas dos quadrados
dos pesos também serão 1.

00:05:30.331 --> 00:05:35.635
Mas se pegarmos
o vetor (0.5, 0.5),

00:05:35.668 --> 00:05:39.065
as somas dos valores absolutos
dos pesos ainda serão 1,

00:05:39.098 --> 00:05:44.248
mas as somas dos quadrados
serão 0.25 + 0.25,

00:05:44.281 --> 00:05:46.344
que é 0.5.

00:05:46.377 --> 00:05:51.724
Assim, a regularização L2 preferirá
o ponto vetorial (0.5, 0.5)

00:05:51.757 --> 00:05:53.854
em relação ao vetor (1, 0),

00:05:53.887 --> 00:05:56.873
já que ele produz
uma soma menor de quadrados.

00:05:56.906 --> 00:05:59.626
E, por sua vez,
uma função menor.

