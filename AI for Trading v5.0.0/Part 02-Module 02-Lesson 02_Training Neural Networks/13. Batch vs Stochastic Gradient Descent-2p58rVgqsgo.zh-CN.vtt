WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.440
首先 我们看看梯度下降算法在干什么

00:00:04.440 --> 00:00:08.990
我们位于珠峰山顶上 需要下山

00:00:08.990 --> 00:00:10.852
为了下山

00:00:10.852 --> 00:00:16.500
我们按照高度（即误差函数）的梯度负值的方向

00:00:16.500 --> 00:00:19.280
走了很多步

00:00:19.280 --> 00:00:21.289
每步就叫做 epoch

00:00:21.289 --> 00:00:23.504
当我们提到步长次数时

00:00:23.504 --> 00:00:25.960
我们指的是 epoch 次数

00:00:25.960 --> 00:00:27.809
现在我们看看每个 epoch 中发生的情况

00:00:27.809 --> 00:00:29.760
在每个 epoch 中 我们获得输入

00:00:29.760 --> 00:00:34.920
即所有的数据 让其完成整个神经网络

00:00:34.920 --> 00:00:36.840
然后算出预测

00:00:36.840 --> 00:00:38.729
我们计算误差

00:00:38.728 --> 00:00:41.859
即与实际标签相差多大

00:00:41.859 --> 00:00:44.548
最后 我们反向传播此误差

00:00:44.548 --> 00:00:48.048
以便更新神经网络中的权重

00:00:48.048 --> 00:00:51.405
这样将会提供更好的界线来预测数据

00:00:51.405 --> 00:00:53.895
对所有数据完成这一过程

00:00:53.895 --> 00:00:55.607
如果有很多很多的数据点

00:00:55.606 --> 00:00:57.057
通常都是这样

00:00:57.057 --> 00:00:59.660
那么这些就是很大的矩阵计算过程

00:00:59.658 --> 00:01:03.992
我会用到大量的内存 而这仅仅是一个步长

00:01:03.993 --> 00:01:05.358
如果有很多步长

00:01:05.358 --> 00:01:09.058
可以想象要花费多少时间和运算容量

00:01:09.060 --> 00:01:12.094
有没有什么方法可以加速这一过程？

00:01:12.093 --> 00:01:17.765
有个问题：每次进行一个步长 都需要代入所有数据吗？

00:01:17.765 --> 00:01:19.454
如果数据分布很合理

00:01:19.453 --> 00:01:21.948
那么一小部分数据就可以很好地告诉我们

00:01:21.950 --> 00:01:24.775
梯度是多少

00:01:24.775 --> 00:01:28.299
或许不是最准确的梯度估算结果 但是速度很快

00:01:28.299 --> 00:01:29.825
因为我们要迭代计算

00:01:29.825 --> 00:01:31.599
或许是个好方法

00:01:31.599 --> 00:01:35.459
这时候随机梯度下降就派上用场了

00:01:35.459 --> 00:01:40.489
随机梯度下降的原理很简单

00:01:40.489 --> 00:01:42.424
我们拿出一小部分数据 让它们经历整个神经网络

00:01:42.424 --> 00:01:45.319
根据这些点计算误差函数的梯度

00:01:45.319 --> 00:01:49.030
然后沿着该方向移动一个步长

00:01:49.030 --> 00:01:51.799
现在我们依然要使用所有数据

00:01:51.799 --> 00:01:53.390
所以我们的做法是

00:01:53.390 --> 00:01:56.564
我们将数据拆分为几个批次

00:01:56.563 --> 00:01:59.817
在此示例中 我们有 24 个点

00:01:59.819 --> 00:02:03.405
我们将拆分为 4 批 每批 6 个点

00:02:03.405 --> 00:02:08.359
拿出第一批次的点并用神经网络处理它们

00:02:08.359 --> 00:02:14.000
计算误差和梯度 反向传播以更新权重

00:02:14.000 --> 00:02:15.335
这样就得出新的权重

00:02:15.335 --> 00:02:19.370
它们将定义出更好的界限区域 从左侧可以看出

00:02:19.370 --> 00:02:24.655
现在拿出第二批次的数据 完成相同的流程

00:02:24.655 --> 00:02:29.259
得出更好的权重和更好的界限区域

00:02:29.258 --> 00:02:32.258
然后对第三批次执行相同的流程

00:02:32.258 --> 00:02:37.243
最后对第四批次执行相同的流程 就完成了

00:02:37.245 --> 00:02:39.099
注意对于数据

00:02:39.098 --> 00:02:41.384
我们执行了 4 个步长

00:02:41.383 --> 00:02:43.572
但是对于普通梯度下降

00:02:43.574 --> 00:02:46.710
我们仅对所有数据执行了一个步长

00:02:46.710 --> 00:02:52.405
当然 所采取的四个步长精确度很低

00:02:52.405 --> 00:02:58.566
但在现实中 采取大量稍微不太准确的步长比采取一个很准确的步长要好很多

00:02:58.566 --> 00:03:01.062
在此纳米学位课程的稍后阶段

00:03:01.062 --> 00:03:04.567
你将有机会应用随机梯度下降 看看它的优势

