WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.440
First, let's look at what the gradient descent algorithm is doing.

00:00:04.440 --> 00:00:08.990
So, recall that we're up here in the top of Mount Everest and we need to go down.

00:00:08.990 --> 00:00:10.852
In order to go down,

00:00:10.852 --> 00:00:16.500
we take a bunch of steps following the negative of the gradient of the height,

00:00:16.500 --> 00:00:19.280
which is the error function.

00:00:19.280 --> 00:00:21.289
Each step is called an epoch.

00:00:21.289 --> 00:00:23.504
So, when we refer to the number of steps,

00:00:23.504 --> 00:00:25.960
we refer to the number of epochs.

00:00:25.960 --> 00:00:27.809
Now, let's see what happens in each epoch.

00:00:27.809 --> 00:00:29.760
In each epoch, we take our input,

00:00:29.760 --> 00:00:34.920
namely all of our data and run it through the entire neural network.

00:00:34.920 --> 00:00:36.840
Then we find our predictions,

00:00:36.840 --> 00:00:38.729
we calculate the error, namely,

00:00:38.728 --> 00:00:41.859
how far they are from where their actual labels.

00:00:41.859 --> 00:00:44.548
And finally, we back-propagate this error in order

00:00:44.548 --> 00:00:48.048
to update the weights in the neural network.

00:00:48.048 --> 00:00:51.405
This will give us a better boundary for predicting our data.

00:00:51.405 --> 00:00:53.895
Now this is done for all the data.

00:00:53.895 --> 00:00:55.607
If we have many, many data points,

00:00:55.606 --> 00:00:57.057
which is normally the case,

00:00:57.057 --> 00:00:59.660
then these are huge matrix computations,

00:00:59.659 --> 00:01:03.993
I'd use tons and tons of memory and all that just for a single step.

00:01:03.993 --> 00:01:05.358
If we had to do many steps,

00:01:05.358 --> 00:01:09.059
you can imagine how this would take a long time and lots of computing power.

00:01:09.060 --> 00:01:12.094
Is there anything we can do to expedite this?

00:01:12.093 --> 00:01:17.765
Well, here's a question: do we need to plug in all our data every time we take a step?

00:01:17.765 --> 00:01:19.454
If the data is well distributed,

00:01:19.453 --> 00:01:21.949
it's almost like a small subset of it would give us

00:01:21.950 --> 00:01:24.775
a pretty good idea of what the gradient would be.

00:01:24.775 --> 00:01:28.299
Maybe it's not the best estimate for the gradient but it's quick,

00:01:28.299 --> 00:01:29.825
and since we're iterating,

00:01:29.825 --> 00:01:31.599
it may be a good idea.

00:01:31.599 --> 00:01:35.459
This is where stochastic gradient descent comes into play.

00:01:35.459 --> 00:01:40.489
The idea behind stochastic gradient descent is simply that we take small subsets of data,

00:01:40.489 --> 00:01:42.424
run them through the neural network,

00:01:42.424 --> 00:01:45.319
calculate the gradient of the error function based on

00:01:45.319 --> 00:01:49.030
those points and then move one step in that direction.

00:01:49.030 --> 00:01:51.799
Now, we still want to use all our data, so,

00:01:51.799 --> 00:01:53.390
what we do is the following;

00:01:53.390 --> 00:01:56.564
we split the data into several batches.

00:01:56.563 --> 00:01:59.818
In this example, we have 24 points.

00:01:59.819 --> 00:02:03.405
We'll split them into four batches of six points.

00:02:03.405 --> 00:02:08.359
Now we take the points in the first batch and run them through the neural network,

00:02:08.359 --> 00:02:14.000
calculate the error and its gradient and back-propagate to update the weights.

00:02:14.000 --> 00:02:15.335
This will give us new weights,

00:02:15.335 --> 00:02:19.370
which will define a better boundary region as you can see on the left.

00:02:19.370 --> 00:02:24.655
Now, we take the points in the second batch and we do the same thing.

00:02:24.655 --> 00:02:29.259
This will again give us better weights and a better boundary region.

00:02:29.258 --> 00:02:32.258
Now, we do the same thing for the third batch.

00:02:32.258 --> 00:02:37.244
And finally, we do it for the fourth batch and we're done.

00:02:37.245 --> 00:02:39.099
Notice that with the data,

00:02:39.098 --> 00:02:41.384
we took four steps whereas,

00:02:41.384 --> 00:02:43.573
when we did normal gradient descent,

00:02:43.574 --> 00:02:46.710
we took only one step with all the data.

00:02:46.710 --> 00:02:52.405
Of course, the four steps we took were less accurate but in the practice,

00:02:52.405 --> 00:02:58.566
it's much better to take a bunch of slightly inaccurate steps than to take one good one.

00:02:58.566 --> 00:03:01.063
Later in this nanodegree, you'll have the chance to apply

00:03:01.063 --> 00:03:04.568
stochastic gradient descents and really see the benefits of it.

