WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.705
现在再介绍另一种防止过拟合的方法

00:00:02.705 --> 00:00:04.448
假设这是你

00:00:04.448 --> 00:00:07.589
有一天 你决定要开始运动

00:00:07.589 --> 00:00:09.823
周一你打网球

00:00:09.823 --> 00:00:11.759
周二举重

00:00:11.759 --> 00:00:14.070
周三玩美式橄榄球

00:00:14.070 --> 00:00:16.245
周四打棒球

00:00:16.245 --> 00:00:18.344
周五打篮球

00:00:18.344 --> 00:00:20.644
周六打乒乓球

00:00:20.643 --> 00:00:22.647
运动一周后

00:00:22.649 --> 00:00:25.739
你似乎发现大部分运动都是惯用手在玩

00:00:25.739 --> 00:00:31.254
所以这个手臂的肌肉很明显 但是另一条手臂却不行

00:00:31.254 --> 00:00:33.265
这很让人失望

00:00:33.265 --> 00:00:34.792
你可以怎么办？

00:00:34.792 --> 00:00:36.838
下周改变下策略

00:00:36.840 --> 00:00:39.630
周一 我们将右手绑在后面

00:00:39.630 --> 00:00:44.054
尝试用左手打棒球

00:00:44.054 --> 00:00:46.770
周二 把左手绑到后面

00:00:46.770 --> 00:00:50.500
尝试用右手举重

00:00:50.500 --> 00:00:52.243
到了周三

00:00:52.243 --> 00:00:56.250
再次绑起右手 并用左手玩美式橄榄球

00:00:56.250 --> 00:01:00.850
周四 我们缓缓 用两只手打棒球

00:01:00.850 --> 00:01:05.500
周五 我们把两只手都绑起来 并尝试打篮球

00:01:05.500 --> 00:01:06.875
这样效果不太好

00:01:06.875 --> 00:01:09.409
但是没关系 这是训练过程

00:01:09.409 --> 00:01:10.995
到了周六

00:01:10.995 --> 00:01:15.635
将左手绑起来 并用右手打乒乓球

00:01:15.635 --> 00:01:21.314
一周后 两只手臂的二头肌都锻炼了 很棒

00:01:21.313 --> 00:01:25.028
我们训练神经网络的时候 就会经常这样

00:01:25.030 --> 00:01:26.858
有时候网络的某个部分

00:01:26.858 --> 00:01:30.578
权重非常大 最终对训练起到主要作用

00:01:30.578 --> 00:01:33.368
而网络的另一部分

00:01:33.370 --> 00:01:36.504
并没有起到多大作用 所以没有被训练

00:01:36.504 --> 00:01:39.359
要解决这一问题 有时候在训练中

00:01:39.358 --> 00:01:43.084
我们将这部分关掉 让网络的其他部分接受训练

00:01:43.084 --> 00:01:46.343
更具体的情况是 我们经历一些 epoch

00:01:46.343 --> 00:01:49.673
随机关闭某些节点

00:01:49.674 --> 00:01:52.129
并说不要通过这里节点

00:01:52.129 --> 00:01:54.609
在这种情况下 其他节点需要承担责任

00:01:54.608 --> 00:01:57.905
在训练中起到更大的作用

00:01:57.905 --> 00:02:03.549
例如 在第一个 epoch 中 我们不能使用这个节点

00:02:03.549 --> 00:02:08.664
我们在前向反馈和反向传播时不使用这个节点

00:02:08.663 --> 00:02:10.655
在第二个 epoch 中

00:02:10.655 --> 00:02:13.503
我们不能使用这两个节点

00:02:13.502 --> 00:02:16.203
同样进行前向反馈和反向传播

00:02:16.205 --> 00:02:20.965
在第三个 epoch 中 我们不能使用这些节点

00:02:20.965 --> 00:02:25.289
再次进行前向反馈和反向传播

00:02:25.288 --> 00:02:27.308
在最后一个 epoch 中

00:02:27.310 --> 00:02:29.818
我们不能使用这里的两个节点

00:02:29.818 --> 00:02:31.598
继续这样

00:02:31.598 --> 00:02:36.158
我们放弃这些节点的方式是给算法一个参数

00:02:36.157 --> 00:02:42.217
该参数就是每个节点在特定 epoch 被放弃的概率

00:02:42.217 --> 00:02:46.357
例如 如果设为 0.2 意味着对于每个 epoch

00:02:46.359 --> 00:02:51.264
每个节点被关闭的概率是 20%

00:02:51.264 --> 00:02:53.770
注意 某些节点被关闭的次数可能比其他节点的要多

00:02:53.770 --> 00:02:56.810
其他一些节点可能从来都不会被关闭

00:02:56.810 --> 00:03:00.068
这是正常的 因为我们一遍一遍地重复

00:03:00.068 --> 00:03:03.209
平均下来 每个节点都会得到相同的处理过程

00:03:03.210 --> 00:03:05.560
这个方法叫做 dropout

00:03:05.560 --> 00:03:08.259
它在神经网络训练中非常常见和实用

