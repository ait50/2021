<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Gradient Descent
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Introduction to Neural Networks
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Instructor.html">
       01. Instructor
      </a>
     </li>
     <li class="">
      <a href="02. Introduction.html">
       02. Introduction
      </a>
     </li>
     <li class="">
      <a href="03. Classification Problems 1.html">
       03. Classification Problems 1
      </a>
     </li>
     <li class="">
      <a href="04. Classification Problems 2.html">
       04. Classification Problems 2
      </a>
     </li>
     <li class="">
      <a href="05. Linear Boundaries.html">
       05. Linear Boundaries
      </a>
     </li>
     <li class="">
      <a href="06. Higher Dimensions.html">
       06. Higher Dimensions
      </a>
     </li>
     <li class="">
      <a href="07. Perceptrons.html">
       07. Perceptrons
      </a>
     </li>
     <li class="">
      <a href="08. Why Neural Networks.html">
       08. Why "Neural Networks"?
      </a>
     </li>
     <li class="">
      <a href="09. Perceptrons as Logical Operators.html">
       09. Perceptrons as Logical Operators
      </a>
     </li>
     <li class="">
      <a href="10. Perceptron Trick.html">
       10. Perceptron Trick
      </a>
     </li>
     <li class="">
      <a href="11. Perceptron Algorithm.html">
       11. Perceptron Algorithm
      </a>
     </li>
     <li class="">
      <a href="12. Non-Linear Regions.html">
       12. Non-Linear Regions
      </a>
     </li>
     <li class="">
      <a href="13. Error Functions.html">
       13. Error Functions
      </a>
     </li>
     <li class="">
      <a href="14. Log-loss Error Function.html">
       14. Log-loss Error Function
      </a>
     </li>
     <li class="">
      <a href="15. Discrete vs Continuous.html">
       15. Discrete vs Continuous
      </a>
     </li>
     <li class="">
      <a href="16. Softmax.html">
       16. Softmax
      </a>
     </li>
     <li class="">
      <a href="17. One-Hot Encoding.html">
       17. One-Hot Encoding
      </a>
     </li>
     <li class="">
      <a href="18. Maximum Likelihood.html">
       18. Maximum Likelihood
      </a>
     </li>
     <li class="">
      <a href="19. Maximizing Probabilities.html">
       19. Maximizing Probabilities
      </a>
     </li>
     <li class="">
      <a href="20. Cross-Entropy 1.html">
       20. Cross-Entropy 1
      </a>
     </li>
     <li class="">
      <a href="21. Cross-Entropy 2.html">
       21. Cross-Entropy 2
      </a>
     </li>
     <li class="">
      <a href="22. Multi-Class Cross Entropy.html">
       22. Multi-Class Cross Entropy
      </a>
     </li>
     <li class="">
      <a href="23. Logistic Regression.html">
       23. Logistic Regression
      </a>
     </li>
     <li class="">
      <a href="24. Gradient Descent.html">
       24. Gradient Descent
      </a>
     </li>
     <li class="">
      <a href="25. Logistic Regression Algorithm.html">
       25. Logistic Regression Algorithm
      </a>
     </li>
     <li class="">
      <a href="26. Pre-Notebook Gradient Descent.html">
       26. Pre-Notebook: Gradient Descent
      </a>
     </li>
     <li class="">
      <a href="27. Notebook Gradient Descent.html">
       27. Notebook: Gradient Descent
      </a>
     </li>
     <li class="">
      <a href="28. Perceptron vs Gradient Descent.html">
       28. Perceptron vs Gradient Descent
      </a>
     </li>
     <li class="">
      <a href="29. Continuous Perceptrons.html">
       29. Continuous Perceptrons
      </a>
     </li>
     <li class="">
      <a href="30. Non-linear Data.html">
       30. Non-linear Data
      </a>
     </li>
     <li class="">
      <a href="31. Non-Linear Models.html">
       31. Non-Linear Models
      </a>
     </li>
     <li class="">
      <a href="32. Neural Network Architecture.html">
       32. Neural Network Architecture
      </a>
     </li>
     <li class="">
      <a href="33. Feedforward.html">
       33. Feedforward
      </a>
     </li>
     <li class="">
      <a href="34. Backpropagation.html">
       34. Backpropagation
      </a>
     </li>
     <li class="">
      <a href="35. Pre-Notebook Analyzing Student Data.html">
       35. Pre-Notebook: Analyzing Student Data
      </a>
     </li>
     <li class="">
      <a href="36. Notebook Analyzing Student Data.html">
       36. Notebook: Analyzing Student Data
      </a>
     </li>
     <li class="">
      <a href="37. Outro.html">
       37. Outro
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          24. Gradient Descent
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="gradient-descent">
          Gradient Descent
         </h1>
         <p>
          In this lesson, we'll learn the principles and the math behind the gradient descent algorithm.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
         <p>
          Gradient Descent
         </p>
        </h3>
        <video controls="">
         <source src="24. Gradient Descent-rhVIF-nigrY.mp4" type="video/mp4"/>
         <track default="false" kind="subtitles" label="pt-BR" src="24. Gradient Descent-rhVIF-nigrY.pt-BR.vtt" srclang="pt-BR"/>
         <track default="false" kind="subtitles" label="ja-JP" src="24. Gradient Descent-rhVIF-nigrY.ja-JP.vtt" srclang="ja-JP"/>
         <track default="true" kind="subtitles" label="en" src="24. Gradient Descent-rhVIF-nigrY.en.vtt" srclang="en"/>
        </video>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="gradient-calculation">
          Gradient Calculation
         </h1>
         <p>
          In the last few videos, we learned that in order to minimize the error function, we need to take some derivatives. So let's get our hands dirty and actually compute the derivative of the error function. The first thing to notice is that the sigmoid function has a really nice derivative. Namely,
         </p>
         <p>
          <span class="mathquill ud-math">
           \sigma'(x) = \sigma(x) (1-\sigma(x))
          </span>
         </p>
         <p>
          The reason for this is the following, we can calculate it using the quotient formula:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/codecogseqn-49.gif"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          And now, let's recall that if we have
          <br/>
          <span class="mathquill ud-math">
           m
          </span>
          <br/>
          points labelled
          <br/>
          <span class="mathquill ud-math">
           x^{(1)}, x^{(2)}, \ldots, x^{(m)},
          </span>
          <br/>
          the error formula is:
         </p>
         <p>
          <span class="mathquill ud-math">
           E = -\frac{1}{m} \sum_{i=1}^m \left( y_i \ln(\hat{y_i}) + (1-y_i) \ln (1-\hat{y_i}) \right)
          </span>
         </p>
         <p>
          where the prediction is given by
          <br/>
          <span class="mathquill ud-math">
           \hat{y_i} = \sigma(Wx^{(i)} + b).
          </span>
         </p>
         <p>
          Our goal is to calculate the gradient of
          <br/>
          <span class="mathquill ud-math">
           E,
          </span>
          <br/>
          at a point
          <br/>
          <span class="mathquill ud-math">
           x = (x_1, \ldots, x_n),
          </span>
          <br/>
          given by the partial derivatives
         </p>
         <p>
          <span class="mathquill ud-math">
           \nabla E =\left(\frac{\partial}{\partial w_1}E, \cdots, \frac{\partial}{\partial w_n}E, \frac{\partial}{\partial b}E \right)
          </span>
         </p>
         <p>
          To simplify our calculations, we'll actually think of the error that each point produces, and calculate the derivative of this error. The total error, then, is the average of the errors at all the points. The error produced by each point is, simply,
         </p>
         <p>
          <span class="mathquill ud-math">
           E = - y \ln(\hat{y}) - (1-y) \ln (1-\hat{y})
          </span>
         </p>
         <p>
          In order to calculate the derivative of this error with respect to the weights, we'll first calculate
          <br/>
          <span class="mathquill ud-math">
           \frac{\partial}{\partial w_j} \hat{y}.
          </span>
          <br/>
          Recall that
          <br/>
          <span class="mathquill ud-math">
           \hat{y} = \sigma(Wx+b),
          </span>
          <br/>
          so:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/codecogseqn-43.gif"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The last equality is because the only term in the sum which is not a constant with respect to
          <br/>
          <span class="mathquill ud-math">
           w_j
          </span>
          <br/>
          is precisely
          <br/>
          <span class="mathquill ud-math">
           w_j x_j,
          </span>
          <br/>
          which clearly has derivative
          <br/>
          <span class="mathquill ud-math">
           x_j.
          </span>
         </p>
         <p>
          Now, we can go ahead and calculate the derivative of the error
          <br/>
          <span class="mathquill ud-math">
           E
          </span>
          <br/>
          at a point
          <br/>
          <span class="mathquill ud-math">
           x,
          </span>
          <br/>
          with respect to the weight
          <br/>
          <span class="mathquill ud-math">
           w_j.
          </span>
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/codecogseqn-60-2.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          A similar calculation will show us that
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/codecogseqn-58.gif"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          This actually tells us something very important. For a point with coordinates
          <br/>
          <span class="mathquill ud-math">
           (x_1, \ldots, x_n),
          </span>
          <br/>
          label
          <br/>
          <span class="mathquill ud-math">
           y,
          </span>
          <br/>
          and prediction
          <br/>
          <span class="mathquill ud-math">
           \hat{y},
          </span>
          <br/>
          the gradient of the error function at that point is
          <br/>
          <span class="mathquill ud-math">
           \left(-(y - \hat{y})x_1, \cdots, -(y - \hat{y})x_n, -(y - \hat{y}) \right).
          </span>
          <br/>
          In summary, the gradient is
         </p>
         <p>
          <span class="mathquill ud-math">
           \nabla E = -(y - \hat{y}) (x_1, \ldots, x_n, 1).
          </span>
         </p>
         <p>
          If you think about it, this is fascinating. The gradient is actually a scalar times the coordinates of the point! And what is the scalar? Nothing less than a multiple of the difference between the label and the prediction. What significance does this have?
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <form>
          <fieldset>
           <legend>
            <p>
             What does the scalar we obtained above signify? (Check all that are true.)
            </p>
           </legend>
          </fieldset>
          <div>
           <input id="a1505309922086" name="395753" type="checkbox" value="a1505309922086"/>
           <label for="a1505309922086">
            <p>
             Closer the label to the prediction, larger the gradient.
            </p>
           </label>
          </div>
          <div>
           <input id="a1505309930925" name="395753" type="checkbox" value="a1505309930925"/>
           <label for="a1505309930925">
            <p>
             Closer the label to the prediction, smaller the gradient.
            </p>
           </label>
          </div>
          <div>
           <input id="a1505309943570" name="395753" type="checkbox" value="a1505309943570"/>
           <label for="a1505309943570">
            <p>
             Farther the label from the prediction, larger the gradient.
            </p>
           </label>
          </div>
          <div>
           <input id="a1505309949324" name="395753" type="checkbox" value="a1505309949324"/>
           <label for="a1505309949324">
            <p>
             Farther the label to the prediction, smaller the gradient.
            </p>
           </label>
          </div>
         </form>
         <details>
          <summary>
           <strong>
            SOLUTION:
           </strong>
          </summary>
          <ul>
           <li>
            Closer the label to the prediction, smaller the gradient.
           </li>
           <li>
            Farther the label from the prediction, larger the gradient.
           </li>
          </ul>
         </details>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          So, a small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.
         </p>
         <p>
          If this sounds anything like the perceptron algorithm, this is no coincidence! We'll see it in a bit.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="gradient-descent-step">
          Gradient Descent Step
         </h1>
         <p>
          Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights in the following way:
         </p>
         <p>
          <span class="mathquill ud-math">
           w_i' \leftarrow w_i -\alpha [-(y - \hat{y}) x_i],
          </span>
         </p>
         <p>
          which is equivalent to
         </p>
         <p>
          <span class="mathquill ud-math">
           w_i' \leftarrow w_i + \alpha (y - \hat{y}) x_i.
          </span>
         </p>
         <p>
          Similarly, it updates the bias in the following way:
         </p>
         <p>
          <span class="mathquill ud-math">
           b' \leftarrow b + \alpha (y - \hat{y}),
          </span>
         </p>
         <p>
          <em>
           Note:
          </em>
          Since we've taken the average of the errors, the term we are adding should be
          <br/>
          <span class="mathquill ud-math">
           \frac{1}{m} \cdot \alpha
          </span>
          <br/>
          instead of
          <br/>
          <span class="mathquill ud-math">
           \alpha,
          </span>
          <br/>
          but as
          <br/>
          <span class="mathquill ud-math">
           \alpha
          </span>
          <br/>
          is a constant, then in order to simplify calculations, we'll just take
          <br/>
          <span class="mathquill ud-math">
           \frac{1}{m} \cdot \alpha
          </span>
          <br/>
          to be our learning rate, and abuse the notation by just calling it
          <br/>
          <span class="mathquill ud-math">
           \alpha.
          </span>
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="25. Logistic Regression Algorithm.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('24. Gradient Descent')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
