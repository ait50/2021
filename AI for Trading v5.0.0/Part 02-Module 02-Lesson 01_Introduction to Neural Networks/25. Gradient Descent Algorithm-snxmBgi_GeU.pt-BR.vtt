WEBVTT
Kind: captions
Language: pt-BR

00:00:00.325 --> 00:00:03.324
Temos finalmente as ferramentas
para escrever o pseudocódigo

00:00:03.357 --> 00:00:05.125
do algoritmo
de gradiente descendente.

00:00:05.158 --> 00:00:07.207
e ele é assim.

00:00:07.240 --> 00:00:08.779
Primeiro passo:

00:00:08.812 --> 00:00:13.428
comece com pesos aleatórios,
W1 até Wn e b,

00:00:13.461 --> 00:00:15.340
o que nos dá uma linha.

00:00:15.373 --> 00:00:16.507
E não apenas uma linha,

00:00:16.540 --> 00:00:17.812
mas a função
de probabilidade inteira

00:00:17.845 --> 00:00:20.618
dada pelo sigmoide de Wx + b.

00:00:20.651 --> 00:00:22.712
Para cada ponto,
calculamos o erro,

00:00:22.745 --> 00:00:23.841
e, como podemos ver,

00:00:23.874 --> 00:00:26.138
o erro está alto para os pontos
classificados incorretamente

00:00:26.171 --> 00:00:29.120
e baixo para os pontos
classificados corretamente.

00:00:29.153 --> 00:00:33.515
Para cada ponto
com coordenadas de x1 a xn,

00:00:33.548 --> 00:00:37.344
atualizamos wi somando
a taxa de aprendizado de alfa

00:00:37.377 --> 00:00:40.982
vezes o derivativo parcial
da função de erro

00:00:41.015 --> 00:00:42.901
em relação a wi.

00:00:42.934 --> 00:00:44.822
Atualizamos também b
somando alfa

00:00:44.855 --> 00:00:48.338
vezes o derivativo parcial
da função de erro em relação a b.

00:00:48.371 --> 00:00:52.568
Isto nos dá novos pesos, wi´,
e um novo viés, b´.

00:00:52.601 --> 00:00:55.231
Agora já calculamos
os derivativos parciais

00:00:55.264 --> 00:01:01.161
e sabemos que são y-chapéu - y . xi
para o derivativo em relação a wi

00:01:01.194 --> 00:01:05.238
e y-chapéu - y para o derivativo
em relação a b.

00:01:05.271 --> 00:01:08.339
É assim que atualizamos
os pesos.

00:01:10.176 --> 00:01:13.634
Repita este processo
até o erro ficar pequeno

00:01:13.667 --> 00:01:15.765
ou podemos repetir
um número fixo de vezes.

00:01:15.798 --> 00:01:17.201
O número de vezes
é chamado de "epochs",

00:01:17.234 --> 00:01:18.889
e vamos aprender isto
mais tarde.

00:01:18.922 --> 00:01:22.049
Isto parece familiar,
já vimos algo assim antes?

00:01:22.082 --> 00:01:24.019
Olhamos os pontos,
e o que cada um está fazendo

00:01:24.052 --> 00:01:26.954
é somar um múltiplo de si mesmo
aos pesos da linha

00:01:26.987 --> 00:01:29.889
para que a linha
se aproxime em direção a ele

00:01:29.922 --> 00:01:31.659
se ela estiver
classificada incorretamente.

00:01:31.692 --> 00:01:34.747
É exatamente
o que o algoritmo perceptron faz.

00:01:34.780 --> 00:01:36.897
No próximo vídeo,
vamos ver as similaridades,

00:01:36.930 --> 00:01:39.129
porque essa similaridade
é um pouco suspeita.

