WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.000
既然我们已经定义了神经网络

00:00:03.000 --> 00:00:04.810
我们需要学习如何训练它们

00:00:04.810 --> 00:00:07.290
训练神经网络实际上意味着

00:00:07.290 --> 00:00:10.495
确定边缘所对应的参数，以此来更好的对数据建模 

00:00:10.495 --> 00:00:12.180
所以为了学习如何训练这些参数

00:00:12.180 --> 00:00:16.800
我们要仔细观察它们如何处理输入 得到一个输出

00:00:16.800 --> 00:00:19.820
我们观察最简单的神经网络 感知器

00:00:19.820 --> 00:00:23.400
这个感知器接收 (x1,x2) 形式的数据点

00:00:23.400 --> 00:00:27.195
其中标签是 Y=1

00:00:27.195 --> 00:00:29.385
这表明点为蓝色

00:00:29.385 --> 00:00:34.680
现在感知器通过 w1x1+w2x2+b 的线性方程进行定义

00:00:34.680 --> 00:00:41.595
其中 w1 和 w2 是边缘的权重 b 是节点的偏差

00:00:41.595 --> 00:00:43.555
这里 w1 大于 w2

00:00:43.555 --> 00:00:46.200
所以我们表示 绘制标签为 w1 的边缘

00:00:46.200 --> 00:00:49.820
比标签为 w2 的边缘更厚

00:00:49.820 --> 00:00:53.280
现在感知器要绘制点 (x1,x2)

00:00:53.280 --> 00:00:57.240
输出点为蓝色的概率

00:00:57.240 --> 00:01:01.173
这个点位于红色区域 那么输出是个很小的数字

00:01:01.173 --> 00:01:03.795
因为这个点很可能不是蓝色的

00:01:03.795 --> 00:01:07.045
这个过程是前馈过程

00:01:07.045 --> 00:01:11.070
我们发现这是个较差的模型 因为实际上这个点是蓝色的

00:01:11.070 --> 00:01:12.570
考虑到第三个坐标

00:01:12.570 --> 00:01:14.820
y = 1

00:01:14.820 --> 00:01:17.010
现在如果我们有一个更加复杂的神经网络

00:01:17.010 --> 00:01:18.570
那么过程是相同的

00:01:18.570 --> 00:01:22.050
这里更大的权重对应更粗的边缘

00:01:22.050 --> 00:01:26.280
更小的权重对应更细的边缘 并且神经网络绘制出

00:01:26.280 --> 00:01:29.070
上图和下图中的点

00:01:29.070 --> 00:01:35.025
得到的输出也是从上面模型中得到较小的数字

00:01:35.025 --> 00:01:38.580
这个点位于红色区域 表明它是蓝色的概率较小

00:01:38.580 --> 00:01:43.140
和来自第二个模型的数字较大

00:01:43.140 --> 00:01:44.895
因为点位于蓝色区域

00:01:44.895 --> 00:01:47.280
表明成为蓝色的概率较大

00:01:47.280 --> 00:01:51.650
现在因为两个模型合并成为这个非线性模型

00:01:51.650 --> 00:01:53.180
输出层只绘制了

00:01:53.180 --> 00:01:57.485
这个点 说明了点为蓝色的概率

00:01:57.485 --> 00:02:00.620
正如你看到的 这是个较差的模型

00:02:00.620 --> 00:02:03.750
因为把这个蓝色的点放在了红色区域

00:02:03.750 --> 00:02:08.280
这个过程叫做前馈 我们要进一步仔细观察

00:02:08.280 --> 00:02:13.070
这里我们得到神经网络和其他符号法 所以偏差位于外面

00:02:13.070 --> 00:02:15.260
现在我们得到权重矩阵

00:02:15.260 --> 00:02:21.285
矩阵 w1 (1 为上标) 表示第一层 各项为权重 w11

00:02:21.285 --> 00:02:23.310
到 w32

00:02:23.310 --> 00:02:26.175
注意这些偏差写成 w31

00:02:26.175 --> 00:02:30.110
和 w32 这是为了方便

00:02:30.110 --> 00:02:31.520
在下一层中

00:02:31.520 --> 00:02:36.115
我们也有一个矩阵 用 w2 (2 为上标) 表示第二层

00:02:36.115 --> 00:02:38.840
这一层包含的权重告诉我们 如何把

00:02:38.840 --> 00:02:43.700
第一层的线性模型合并起来 得到第二层的非线性模型

00:02:43.700 --> 00:02:45.060
这里需要一些数学知识

00:02:45.060 --> 00:02:47.135
我们的输入是 (x1, x2,1) 的形式

00:02:47.135 --> 00:02:51.000
其中 1 来自于偏置单元

00:02:51.000 --> 00:02:55.660
现在我们把它乘以矩阵 w1 得到这些输出

00:02:55.660 --> 00:03:01.250
然后我们应用 sigmoid 函数 把输出转化成 0 到 1 之间的值

00:03:01.250 --> 00:03:04.130
那么这些值得到的向量形式把 1

00:03:04.130 --> 00:03:08.280
作为偏置单元 乘以第二个矩阵

00:03:08.280 --> 00:03:12.110
这样得到一个输出 并通过 sigmoid 函数

00:03:12.110 --> 00:03:16.290
得到最后的输出 即 y-hat

00:03:16.290 --> 00:03:21.155
Y-hat 是标记为蓝点的预测或概率

00:03:21.155 --> 00:03:23.275
这就是神经网络的用途

00:03:23.275 --> 00:03:25.760
它们利用输入向量 然后对其应用

00:03:25.760 --> 00:03:29.170
一系列的线性模型和 sigmoid 函数

00:03:29.170 --> 00:03:32.825
这些图组合后成为高级的非线性图

00:03:32.825 --> 00:03:37.310
最后公式是 y-hat 等于 σ w2

00:03:37.310 --> 00:03:42.995
σ w1 (x)

00:03:42.995 --> 00:03:48.025
重复一下 我们再次在多层感知器或神经网络上重复一次

00:03:48.025 --> 00:03:51.105
为了计算预测 y-hat

00:03:51.105 --> 00:03:53.380
我们从单位向量 x 开始

00:03:53.380 --> 00:03:55.560
然后应用第一个矩阵和

00:03:55.560 --> 00:04:00.405
一个 sigmoid 函数 得到第二层中的值

00:04:00.405 --> 00:04:05.360
其次我们应用第二个矩阵和另一个 sigmoid 函数 得到第三层中的值

00:04:05.360 --> 00:04:13.315
如此循环往复 直到我们得出最后的预测 y-hat

00:04:13.315 --> 00:04:16.430
这是神经网络使用的前馈过程

00:04:16.430 --> 00:04:20.000
可以得到输入向量的预测

