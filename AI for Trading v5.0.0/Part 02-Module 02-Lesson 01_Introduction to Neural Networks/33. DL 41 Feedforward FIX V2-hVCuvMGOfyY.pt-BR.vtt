WEBVTT
Kind: captions
Language: pt-BR

00:00:00.033 --> 00:00:02.930
Agora que definimos
o que são redes neurais,

00:00:02.963 --> 00:00:04.718
precisamos aprender
a treiná-las.

00:00:04.751 --> 00:00:06.050
Treiná-las significa:

00:00:06.083 --> 00:00:08.094
que parâmetros
elas devem ter nos limites

00:00:08.127 --> 00:00:10.548
para que elas moldem bem
nossos dados?

00:00:10.581 --> 00:00:12.252
Para aprendermos a treiná-las,

00:00:12.285 --> 00:00:14.797
temos que prestar atenção
em como elas processam o input

00:00:14.830 --> 00:00:16.575
para obter um output.

00:00:16.608 --> 00:00:18.887
Vamos dar uma olhada
na rede neural mais simples,

00:00:18.920 --> 00:00:20.346
o perceptron.

00:00:20.379 --> 00:00:24.313
Este perceptron recebe
um ponto de dado x1,x2,

00:00:24.346 --> 00:00:27.059
em que o rótulo é y = 1.

00:00:27.092 --> 00:00:29.270
Ou seja, o ponto é azul.

00:00:29.303 --> 00:00:31.871
Agora o perceptron é definido
por uma equação linear,

00:00:31.904 --> 00:00:35.949
digamos, w1x1 + w2x2 + b,

00:00:35.982 --> 00:00:39.480
em que w1 e w2
são os pesos dos limites

00:00:39.513 --> 00:00:41.754
e b é o viés do nó.

00:00:41.787 --> 00:00:43.388
Aqui, w1 é maior que w2,

00:00:43.421 --> 00:00:46.428
então vamos denotar isso
desenhando o limite com rótulo w1

00:00:46.461 --> 00:00:49.655
bem mais espesso
que o limite com rótulo w2.

00:00:49.688 --> 00:00:54.272
O que o perceptron faz
é diagramar o ponto x1,x2

00:00:54.305 --> 00:00:57.409
e gera a probabilidade
de que o ponto é azul.

00:00:57.442 --> 00:00:59.239
Aqui o ponto está
na área vermelha,

00:00:59.272 --> 00:01:01.050
então a saída é
um número pequeno,

00:01:01.083 --> 00:01:03.920
já que o ponto
provavelmente não é azul.

00:01:03.953 --> 00:01:06.891
Este processo é conhecido
como "alimentação direta".

00:01:06.924 --> 00:01:10.883
Podemos ver que é um modelo ruim
porque o ponto é de fato azul.

00:01:10.916 --> 00:01:14.718
Assim, a terceira coordenada,
y, é igual a 1.

00:01:14.751 --> 00:01:16.891
Se tivermos uma rede neural
mais complicada,

00:01:16.924 --> 00:01:18.578
o processo será o mesmo.

00:01:18.611 --> 00:01:21.756
Aqui, temos limites espessos
que correspondem a pesos grandes

00:01:21.789 --> 00:01:24.819
e limites finos que correspondem
a pesos pequenos.

00:01:24.852 --> 00:01:28.344
A rede neural diagrama o ponto
no gráfico de cima

00:01:28.377 --> 00:01:30.301
e também no gráfico de baixo.

00:01:30.334 --> 00:01:34.754
Os outputs será um número pequeno
do modelo de cima.

00:01:34.787 --> 00:01:36.452
O ponto está na área vermelha,

00:01:36.485 --> 00:01:39.673
ou seja, é pouco provável
que seja azul,

00:01:39.706 --> 00:01:43.020
e um número grande
do segundo modelo,

00:01:43.053 --> 00:01:44.658
já que o ponto
está na área azul,

00:01:44.691 --> 00:01:47.657
ou seja,
é muito provável que seja azul.

00:01:47.690 --> 00:01:51.358
Agora, como os dois modelos
foram combinados neste modelo linear

00:01:51.391 --> 00:01:54.132
e a camada de saída
diagrama apenas o ponto,

00:01:54.165 --> 00:01:57.695
isso nos diz a probabilidade
do ponto ser azul.

00:01:57.728 --> 00:02:00.037
Como pode ver,
é um modelo ruim,

00:02:00.070 --> 00:02:02.236
porque coloca o ponto
na área vermelha,

00:02:02.269 --> 00:02:03.998
e o ponto é azul.

00:02:04.031 --> 00:02:06.284
Novamente, este processo é chamado
de "alimentação direta",

00:02:06.317 --> 00:02:08.231
e vamos dar
mais atenção a ele.

00:02:08.264 --> 00:02:10.633
Aqui, temos nossa rede neural
e as outras notações,

00:02:10.666 --> 00:02:13.227
ou seja,
o viés está do lado de fora.

00:02:13.260 --> 00:02:15.234
Agora temos
uma matriz de pesos.

00:02:15.267 --> 00:02:18.827
A matriz elevada a 1,
que denota a primeira camada,

00:02:18.860 --> 00:02:23.434
e as entradas são
os pesos w11 até w32.

00:02:23.467 --> 00:02:28.269
Veja que os vieses
foram escritos como w31 e w32.

00:02:28.302 --> 00:02:30.393
É apenas por conveniência.

00:02:30.426 --> 00:02:32.549
Na próxima camada,
temos também uma matriz.

00:02:32.582 --> 00:02:35.961
Esta é w elevada a 2
para a segunda camada.

00:02:35.994 --> 00:02:37.901
Esta camada contém
os pesos que nos dizem

00:02:37.934 --> 00:02:40.269
como combinar os modelos lineares
da primeira camada

00:02:40.302 --> 00:02:43.550
para obter o modelo não linear
da segunda camada.

00:02:43.583 --> 00:02:45.408
Agora o que acontece
é matemática.

00:02:45.441 --> 00:02:47.958
Temos o input
na forma x1,x2,1,

00:02:47.991 --> 00:02:50.753
em que 1
vem da unidade de viés.

00:02:50.786 --> 00:02:55.562
Multiplicamos isto pela matriz w1
para obter estas saídas.

00:02:55.595 --> 00:02:58.805
Depois aplicamos a função sigmoide
para transformar as saídas

00:02:58.838 --> 00:03:01.409
em valores entre 0 e 1.

00:03:01.442 --> 00:03:02.703
Então o vetor formata
estes valores

00:03:02.736 --> 00:03:05.162
e obtém 1
para a unidade de viés

00:03:05.195 --> 00:03:08.224
multiplicada
pela segunda matriz.

00:03:08.257 --> 00:03:11.866
Isto retorna uma saída
que é jogada em uma função sigmoide

00:03:11.899 --> 00:03:15.576
para obter a saída final,
que é y-chapéu.

00:03:15.609 --> 00:03:17.641
y-chapéu é a predição

00:03:17.674 --> 00:03:20.975
ou probabilidade
do ponto ter rótulo azul.

00:03:21.008 --> 00:03:23.146
É isto o que
as redes neurais fazem.

00:03:23.179 --> 00:03:25.170
Elas pegam o vetor de input

00:03:25.203 --> 00:03:27.253
e aplicam uma sequência
de modelos lineares

00:03:27.286 --> 00:03:29.005
e funções sigmoides.

00:03:29.038 --> 00:03:30.377
Estes mapas,
quando combinados,

00:03:30.410 --> 00:03:33.029
se tornam um mapa
altamente não linear.

00:03:33.062 --> 00:03:38.210
A fórmula final é simplesmente
y-chapéu = sigmoide de w2

00:03:38.243 --> 00:03:43.257
combinado ao sigmoide de w1,
aplicado a x.

00:03:43.290 --> 00:03:46.053
Apenas por redundância,
fazemos isto de novo

00:03:46.086 --> 00:03:47.427
em um perceptron
de múltiplas camadas,

00:03:47.460 --> 00:03:48.821
ou uma rede neural.

00:03:48.854 --> 00:03:51.057
Para calcular
a predição y-chapéu,

00:03:51.090 --> 00:03:53.246
começamos
com a unidade de vetor x,

00:03:53.279 --> 00:03:57.431
então aplicamos a primeira matriz
e uma função sigmoide

00:03:57.464 --> 00:04:00.301
para obter os valores
da segunda camada.

00:04:00.334 --> 00:04:04.311
Depois aplicamos a segunda matriz
e outra função sigmoide

00:04:04.344 --> 00:04:06.712
para obter valores
da terceira camada,

00:04:06.745 --> 00:04:09.365
e assim por diante,

00:04:09.398 --> 00:04:13.485
até obtermos
a predição final, y-chapéu.

00:04:13.518 --> 00:04:15.407
Este é o processo
de alimentação direta

00:04:15.440 --> 00:04:16.666
que as redes neurais usam

00:04:16.699 --> 00:04:19.657
para obter a predição
do vetor de input.

