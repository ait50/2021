WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.309
现在我们可以开始训练神经网络了

00:00:04.309 --> 00:00:06.448
我们快速回顾一下前馈的概念

00:00:06.450 --> 00:00:10.469
我们有一个感知器 其中有一个标签为正的输入点

00:00:10.470 --> 00:00:15.050
感知器的方程是 w1x1 + w2x2 + b

00:00:15.050 --> 00:00:19.804
其中 w1 和 w2 是权重 b 是偏差

00:00:19.803 --> 00:00:21.009
感知器给出一个点

00:00:21.010 --> 00:00:25.405
返回该点是蓝色的概率

00:00:25.405 --> 00:00:29.345
这个例子中的概率比较小 因为该点位于红色区域

00:00:29.344 --> 00:00:32.070
因此 这是个不太理想的感知器

00:00:32.070 --> 00:00:36.164
因为它预测该点是红色 但实际上该点是蓝色

00:00:36.164 --> 00:00:39.609
我们回忆下我们在梯度下降算法中的做法

00:00:39.609 --> 00:00:42.283
我们执行了一种被称为反向传播的操作

00:00:42.283 --> 00:00:44.878
我们朝着相反的方向前进

00:00:44.880 --> 00:00:48.885
我们问这个点 “你需要模型为你做什么”

00:00:48.884 --> 00:00:50.829
点回答说

00:00:50.829 --> 00:00:55.204
“我被错误分类了 我想让分类的边界离我近点”

00:00:55.204 --> 00:00:59.893
我们看到直线通过更新权重 离它更近了

00:00:59.895 --> 00:01:01.625
在此情况下

00:01:01.625 --> 00:01:07.239
我们假设它告诉权重 w1 变小些 权重 w2 变大些

00:01:07.239 --> 00:01:08.694
当然这只是一个演示

00:01:08.694 --> 00:01:10.379
真实情况不一定是这样

00:01:10.379 --> 00:01:12.045
我们得到新的权重

00:01:12.045 --> 00:01:19.490
w1’ 和 w2’ 定义了一条新的直线 它现在离点更近了

00:01:19.489 --> 00:01:22.170
我们的做法就像从误差之巅下来

00:01:22.170 --> 00:01:23.780
对吧？

00:01:23.780 --> 00:01:29.864
高度将是误差函数 E(W)

00:01:29.864 --> 00:01:32.679
我们计算误差函数的梯度

00:01:32.680 --> 00:01:35.857
就像在问点希望模型做什么

00:01:35.855 --> 00:01:40.339
当我们朝着梯度的相反方向下山

00:01:40.340 --> 00:01:43.969
我们就减小了误差 从山上下来

00:01:43.968 --> 00:01:45.303
这样就得到了新的误差 E(W’)

00:01:45.305 --> 00:01:49.932
以及一个误差更小的新模型 W’

00:01:49.932 --> 00:01:53.480
意味着我们得到了离点更近的新直线

00:01:53.480 --> 00:01:58.130
我们继续这一流程 以最小化误差

00:01:58.129 --> 00:01:59.890
这就是单个感知器的反向传播过程

00:01:59.890 --> 00:02:02.760
对于多层级感知器 应该如何进行操作呢？

00:02:02.760 --> 00:02:06.745
我们依然通过相同的流程来减小误差并从山上下来

00:02:06.745 --> 00:02:11.055
但是现在误差函数更为复杂

00:02:11.055 --> 00:02:12.775
更像是乞力马扎罗山

00:02:12.775 --> 00:02:15.789
不再是简单的误差之巅

00:02:15.788 --> 00:02:19.554
我们计算误差函数和它的梯度

00:02:19.555 --> 00:02:25.290
然后朝着梯度的相反方向前进

00:02:25.288 --> 00:02:28.643
以便找到新的模型

00:02:28.645 --> 00:02:32.719
它的误差函数 E(W’) 更小 会为我们提供更准确的预测

00:02:32.718 --> 00:02:36.894
我们持续这一流程 以便最小化误差

00:02:36.895 --> 00:02:40.149
我们再看看多层级感知器中的前向反馈

00:02:40.149 --> 00:02:45.990
传入点的坐标是 (x1,x2) 标签是 y = 1

00:02:45.990 --> 00:02:50.570
它对应的是隐藏层的线性模型

00:02:50.568 --> 00:02:54.018
然后通过层级间的结合

00:02:54.020 --> 00:02:58.280
这个点的最终结果通过整个非线性模型的输出层得到

00:02:58.280 --> 00:03:01.400
该点是蓝色的概率等于

00:03:01.400 --> 00:03:05.060
该点在最终模型中的位置

00:03:05.060 --> 00:03:07.188
现在请集中注意力

00:03:07.188 --> 00:03:11.093
因为这是训练神经网络的关键 我们讲到反向传播了

00:03:11.093 --> 00:03:13.848
和之前一样 查看一下这个模型得到的误差

00:03:13.848 --> 00:03:16.158
这个模型不太好 因为它预测这个点是红色

00:03:16.158 --> 00:03:19.364
但实际上这个点是蓝色

00:03:19.365 --> 00:03:21.320
我们问该点

00:03:21.318 --> 00:03:26.578
“为了分类正确 你希望该模型怎么做？”

00:03:26.580 --> 00:03:31.615
这个点说“我希望这个蓝色区域离我近点”

00:03:31.615 --> 00:03:35.195
让蓝色区域靠近点是什么意思？

00:03:35.193 --> 00:03:39.048
我们看看隐藏层中的两个线性模型

00:03:39.050 --> 00:03:42.735
哪个模型效果更好？

00:03:42.735 --> 00:03:45.740
似乎上面这个模型的分类结果很糟糕

00:03:45.740 --> 00:03:50.230
而下面的模型分类正确

00:03:50.229 --> 00:03:55.454
所以我们希望来自下面这个模型的影响增大 来自上面这个模型的影响缩小

00:03:55.455 --> 00:03:58.880
我们要做的是减小来自上面这个模型的权重

00:03:58.878 --> 00:04:02.518
并增大来自下面这个模型的权重

00:04:02.520 --> 00:04:05.909
那么我们得到的模型看起来

00:04:05.908 --> 00:04:10.033
更像下面的模型 而不是上面的模型

00:04:10.033 --> 00:04:12.013
我们可以更加深入地解释

00:04:12.014 --> 00:04:15.463
我们可以从各个线性模型出发 问这个点

00:04:15.465 --> 00:04:20.250
“为了更好地对你进行分类 这些模型应该怎么做？”

00:04:20.250 --> 00:04:22.139
该点说

00:04:22.139 --> 00:04:24.831
“上面这个模型对我的分类不正确

00:04:24.833 --> 00:04:28.635
我想这条线离我近些

00:04:28.634 --> 00:04:33.084
第二个模型对我的分类正确

00:04:33.084 --> 00:04:37.370
所以我想这条线离我远点”

00:04:37.370 --> 00:04:41.670
模型据此更新权重

00:04:41.670 --> 00:04:46.000
假设它将增大这两个权重 并减小那两个权重

00:04:46.000 --> 00:04:50.735
在更新了所有权重后

00:04:50.735 --> 00:04:53.569
隐藏层中所有模型的预测更加准确

00:04:53.569 --> 00:04:57.588
输出层中模型的预测也更为准确

00:04:57.588 --> 00:05:02.125
注意 在这个视频中 为了解释得更加清楚 我们故意省略了偏差

00:05:02.125 --> 00:05:06.649
在现实中 当你更新权重时 你也会更新偏差单位

00:05:06.649 --> 00:05:08.658
如果你喜欢更为严谨的解释

00:05:08.660 --> 00:05:12.070
别担心 我们稍后将详细地计算所有梯度

