WEBVTT
Kind: captions
Language: ja-JP

00:00:00.000 --> 00:00:04.309
実際にニューラルネットワークをトレーニングする準備ができました

00:00:04.309 --> 00:00:06.449
フィードフォワードを思い出しましょう

00:00:06.450 --> 00:00:10.469
パーセプトロンに正のラベルが付いた点があります

00:00:10.470 --> 00:00:15.050
式はw1x1+w2x2+bです

00:00:15.050 --> 00:00:19.804
ここでw1とw2は重み bはバイアスです

00:00:19.804 --> 00:00:21.009
パーセプトロンは点をプロットし

00:00:21.010 --> 00:00:25.405
点が青である確率を返します

00:00:25.405 --> 00:00:29.345
この例では点は赤の領域にあるため 確率は小さい値です

00:00:29.344 --> 00:00:32.070
点が実際には青なのに赤であると予測するため

00:00:32.070 --> 00:00:36.164
これは不正なパーセプトロンです

00:00:36.164 --> 00:00:39.609
勾配降下法アルゴリズムについて思い出してください

00:00:39.609 --> 00:00:42.284
逆伝搬と呼ばれる処理を実行しました

00:00:42.284 --> 00:00:44.879
逆方向の処理です

00:00:44.880 --> 00:00:48.885
「そのモデルに何をさせたいですか?」と点に尋ねました

00:00:48.884 --> 00:00:50.829
点は「私は誤分類されているので

00:00:50.829 --> 00:00:55.204
この境界線を自分に近づけてほしい」と言いました

00:00:55.204 --> 00:00:59.894
線を近づけるには 重みを更新すればいいのです

00:00:59.895 --> 00:01:01.625
つまり重みw1に低くなるように指示し

00:01:01.625 --> 00:01:07.239
重みw2に高くなるように指示します

00:01:07.239 --> 00:01:08.694
これは単なる例示であり

00:01:08.694 --> 00:01:10.379
正確なものではありません

00:01:10.379 --> 00:01:12.045
重みw1とw2が更新され

00:01:12.045 --> 00:01:19.490
点により近い線を定義するようになりました

00:01:19.489 --> 00:01:22.170
まるでエラレスト山を

00:01:22.170 --> 00:01:23.780
下っているようですね

00:01:23.780 --> 00:01:29.864
山の高さはエラー関数E(W)であり

00:01:29.864 --> 00:01:32.679
エラー関数の勾配を計算するのです

00:01:32.680 --> 00:01:35.857
これは点に対し モデルに何をさせたいかと尋ねるのと同じことです

00:01:35.856 --> 00:01:40.340
勾配の負の方向に進むため

00:01:40.340 --> 00:01:43.969
エラーを減らして山を下ります

00:01:43.969 --> 00:01:45.304
これにより新しいエラーE(W')と

00:01:45.305 --> 00:01:49.932
エラーが小さくなったモデルW'が生成され

00:01:49.932 --> 00:01:53.480
線が点に近づきます

00:01:53.480 --> 00:01:58.130
エラーを最小限に抑えるために このプロセスを繰り返します

00:01:58.129 --> 00:01:59.890
これはパーセプトロンが1つの場合です

00:01:59.890 --> 00:02:02.760
多層パーセプトロンの場合はどうでしょう?

00:02:02.760 --> 00:02:06.745
やはり同じプロセスでエラーを減らし

00:02:06.745 --> 00:02:11.055
山を下りますが

00:02:11.055 --> 00:02:12.775
今度はエラー関数がより複雑であるため

00:02:12.775 --> 00:02:15.789
山はエラレストではなくキリマンジャロですしかしやることは同じで

00:02:15.788 --> 00:02:19.554
エラー関数とその勾配を計算します

00:02:19.555 --> 00:02:25.290
より小さいエラーE(W')を持つ新しいモデルE(W')を見つけるため

00:02:25.289 --> 00:02:28.644
勾配の負の方向に下っていきます

00:02:28.645 --> 00:02:32.719
これにより予測の精度が高まります

00:02:32.719 --> 00:02:36.895
エラーを最小限に抑えるために このプロセスを繰り返します

00:02:36.895 --> 00:02:40.149
それでは多層パーセプトロンにおけるフィードフォワードをもう一度見てみましょう

00:02:40.149 --> 00:02:45.990
点の座標は(x1,x2) ラベルはy=1です

00:02:45.990 --> 00:02:50.570
隠れ層に対応する線形モデルにプロットされます

00:02:50.569 --> 00:02:54.019
この層が結合されると 結果として出力層で生成される

00:02:54.020 --> 00:02:58.280
非線形モデルに点がプロットされます

00:02:58.280 --> 00:03:01.400
その点が青である確率は

00:03:01.400 --> 00:03:05.060
最終モデルでの点の位置から計算されます

00:03:05.060 --> 00:03:07.189
これが ニューラルネットワークをトレーニングするための鍵

00:03:07.189 --> 00:03:11.094
つまり逆伝搬であるため 細心の注意を払ってください

00:03:11.094 --> 00:03:13.849
今回もエラーをチェックします

00:03:13.849 --> 00:03:16.159
点が実際には青なのに赤であると予測するので

00:03:16.159 --> 00:03:19.365
これは優れたモデルではありません

00:03:19.365 --> 00:03:21.320
そこで点に対し「正しく分類されるために

00:03:21.319 --> 00:03:26.579
このモデルに何をさせたいですか?」と尋ねます

00:03:26.580 --> 00:03:31.615
点は「この青の領域を自分に近づけてほしい」と言います

00:03:31.615 --> 00:03:35.195
領域を近づけるとはどういうことでしょうか?

00:03:35.194 --> 00:03:39.049
隠れ層の2つの線形モデルを見てみましょう

00:03:39.050 --> 00:03:42.735
この2つのモデルのどちらがより優れていますか?

00:03:42.735 --> 00:03:45.740
上のモデルは点の分類が誤っており

00:03:45.740 --> 00:03:50.230
下のモデルは分類が正しいようです

00:03:50.229 --> 00:03:55.454
そこで上のモデルにはあまり耳を傾けず 下のモデルに対する聴取を増やします

00:03:55.455 --> 00:03:58.880
つまり上のモデルからの重みを減らし

00:03:58.879 --> 00:04:02.519
下のモデルからの重みを増やします

00:04:02.520 --> 00:04:05.909
そのため最終的なモデルは

00:04:05.909 --> 00:04:10.034
上のモデルより下のモデルに近くなります

00:04:10.034 --> 00:04:12.014
ただしまだできることがあります

00:04:12.014 --> 00:04:15.464
線形モデルの点に「分類の精度を高めるために

00:04:15.465 --> 00:04:20.250
これらのモデルは何ができますか?」と尋ねます

00:04:20.250 --> 00:04:22.139
点はこのように言います

00:04:22.139 --> 00:04:24.832
「上のモデルは誤分類しているので

00:04:24.833 --> 00:04:28.635
この線を私の近くに動かしたいです

00:04:28.634 --> 00:04:33.084
下のモデルは分類が正しいので

00:04:33.084 --> 00:04:37.370
この線を私から遠ざけたいです」

00:04:37.370 --> 00:04:41.670
モデルにこの変更を加えると 重みが更新されます

00:04:41.670 --> 00:04:46.000
これら2つを増やし これら2つを減らします

00:04:46.000 --> 00:04:50.735
すべての重みを更新すると

00:04:50.735 --> 00:04:53.569
隠れ層のすべてのモデルの予測が改善し

00:04:53.569 --> 00:04:57.589
出力層のモデルでも予測が改善します

00:04:57.589 --> 00:05:02.125
このビデオでは わかりやすくするためにバイアスユニットを省略しています

00:05:02.125 --> 00:05:06.649
実際には重みを更新すると バイアスユニットも更新されます

00:05:06.649 --> 00:05:08.659
形式にこだわる方も心配は要りません

00:05:08.660 --> 00:05:12.070
この後すぐにこれらの勾配を細かく計算します

