WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.653
和之前一样

00:00:02.653 --> 00:00:06.278
我们在神经网络图中标记权重 以便更好地分类点

00:00:06.280 --> 00:00:07.679
但是这一次 我们将使用严谨的表达式

00:00:07.679 --> 00:00:10.349
所以请保持注意集中 一大堆数学知识就要来了

00:00:10.349 --> 00:00:13.785
在左侧是单个感知器 带有输入向量

00:00:13.785 --> 00:00:18.879
权重 偏差和 sigmoid 函数 它们都在这个节点内

00:00:18.879 --> 00:00:22.004
右侧是预测所使用的公式

00:00:22.004 --> 00:00:27.050
输入先经过一个线性方程 再经过 sigmoid 函数

00:00:27.050 --> 00:00:30.820
下面是误差公式

00:00:30.820 --> 00:00:34.520
它是所有点的平均结果

00:00:34.520 --> 00:00:38.799
蓝色项和红色项分别代表蓝点和红点

00:00:38.798 --> 00:00:41.185
为了从误差之巅上下来

00:00:41.185 --> 00:00:43.269
我们需要计算梯度

00:00:43.268 --> 00:00:48.608
梯度是误差函数关于权重（从 w1 到 wn）和偏差的偏导数

00:00:48.609 --> 00:00:55.630
所形成的向量

00:00:55.630 --> 00:00:58.420
它们所对应的是这些边

00:00:58.420 --> 00:01:01.399
在多层感知器中该如何进行反向传播？

00:01:01.399 --> 00:01:05.405
这种情况稍微复杂些 但本质上几乎一样

00:01:05.405 --> 00:01:07.233
这是我们的预测结果

00:01:07.233 --> 00:01:13.503
其实就是一些复合函数 包括矩阵乘法和 sigmoid 函数

00:01:13.504 --> 00:01:16.055
误差函数几乎保持不变

00:01:16.055 --> 00:01:19.234
只是 ŷ 稍微复杂些

00:01:19.233 --> 00:01:21.649
梯度几乎一样

00:01:21.650 --> 00:01:23.799
只是表达式长了很多

00:01:23.799 --> 00:01:25.909
它是一个庞大的向量

00:01:25.909 --> 00:01:29.984
每一项分别是误差函数关于每个权重的偏导数

00:01:29.983 --> 00:01:32.582
这些项对应所有的边

00:01:32.584 --> 00:01:34.489
如果要写的更严谨一些

00:01:34.489 --> 00:01:39.500
我们知道预测结果是 sigmoid 函数和矩阵乘法的复合

00:01:39.500 --> 00:01:42.000
这些是矩阵

00:01:42.000 --> 00:01:45.765
梯度由所有这些偏导数组成

00:01:45.765 --> 00:01:47.579
这里看起来像个矩阵

00:01:47.578 --> 00:01:49.198
但实际上是个很长的向量

00:01:49.200 --> 00:01:51.730
梯度下降法将进行如下步骤

00:01:51.730 --> 00:01:53.430
我们拿出每个权重 wij(k)

00:01:53.430 --> 00:01:59.206
我们对其进行更新

00:01:59.206 --> 00:02:06.125
加上一个小数字（也就是学习速率）乘以 E 相对于该权重的偏导数

00:02:06.125 --> 00:02:07.819
这是梯度下降的步骤

00:02:07.819 --> 00:02:14.270
这样将得到更新后的权重 wij’(k)

00:02:14.270 --> 00:02:16.919
于是就得到了一个全新的模型

00:02:16.919 --> 00:02:20.270
它包含了新的权重 可以对点进行更准确的分类

