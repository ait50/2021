WEBVTT
Kind: captions
Language: pt-BR

00:00:00.309 --> 00:00:02.158
Vamos estudar
o gradiente descendente

00:00:02.191 --> 00:00:03.644
com mais detalhes matemáticos.

00:00:03.677 --> 00:00:06.008
Nossa função
é uma função dos pesos

00:00:06.041 --> 00:00:07.771
e pode ser representada assim.

00:00:07.804 --> 00:00:11.164
Ela tem uma estrutura matemática,
portanto não é mais o Monte Everest,

00:00:11.197 --> 00:00:13.815
está mais
para Monte "Matemahorn".

00:00:13.848 --> 00:00:16.036
Estamos em algum lugar
no Monte "Matemahorn"

00:00:16.069 --> 00:00:18.264
e precisamos descer.

00:00:18.297 --> 00:00:23.295
Então agora os inputs
das funções são W1 e W2,

00:00:23.328 --> 00:00:26.763
e a função de erro
é dada por E.

00:00:26.796 --> 00:00:30.314
Então o gradiente de E
é dado pela soma do vetor

00:00:30.347 --> 00:00:36.238
dos derivativos parciais de E
em relação a W1 e W2.

00:00:36.271 --> 00:00:39.533
Este gradiente nos diz
a direção na qual queremos ir

00:00:39.566 --> 00:00:42.643
se quisermos aumentar
a função de erro ao máximo.

00:00:42.676 --> 00:00:45.711
Assim, se pegarmos
o negativo do gradiente,

00:00:45.744 --> 00:00:48.964
saberemos como diminuir
a função de erro ao máximo.

00:00:48.997 --> 00:00:51.623
E é exatamente
o que vamos fazer.

00:00:51.656 --> 00:00:52.836
No ponto onde estamos,

00:00:52.869 --> 00:00:58.752
pegaremos o negativo do gradiente
da função de erro nesse ponto.

00:00:58.983 --> 00:01:01.432
Então damos um passo
nessa direção.

00:01:01.465 --> 00:01:02.987
Assim que dermos um passo,

00:01:03.020 --> 00:01:04.414
estaremos em uma posição
mais baixa.

00:01:04.447 --> 00:01:09.358
Então fazemos isso de novo,
outra vez e novamente,

00:01:09.391 --> 00:01:12.786
até conseguirmos chegar
à base da montanha.

00:01:12.819 --> 00:01:15.061
É assim que calculamos
o gradiente.

00:01:15.094 --> 00:01:20.040
Começamos com a predição inicial
y-chapéu = sigmoide(Wx+b).

00:01:20.073 --> 00:01:23.300
E digamos que esta predição é ruim
porque o erro é grande,

00:01:23.333 --> 00:01:25.276
já que estamos
no alto da montanha.

00:01:25.309 --> 00:01:26.777
A predição fica assim,

00:01:26.810 --> 00:01:32.760
y-chapéu = sigmoide
(w,x, + ... + wnxn + b).

00:01:33.519 --> 00:01:37.263
Agora a função de erro é dada
pela fórmula que vimos antes.

00:01:37.296 --> 00:01:40.741
Mas o que importa aqui
é o gradiente da função de erro.

00:01:40.774 --> 00:01:44.104
O gradiente da função de erro
é exatamente o vetor

00:01:44.137 --> 00:01:47.286
formado pelo derivativo parcial
da função de erro

00:01:47.319 --> 00:01:50.077
em relação
aos pesos e ao viés.

00:01:50.110 --> 00:01:54.267
Agora damos um passo
em direção ao negativo do gradiente.

00:01:54.300 --> 00:01:55.515
Assim como antes,

00:01:55.548 --> 00:01:57.420
não queremos fazer
nenhuma mudança dramática,

00:01:57.453 --> 00:02:00.275
então introduzimos um alfa
com uma taxa de aprendizado menor.

00:02:00.308 --> 00:02:02.863
Por exemplo, 0.1.

00:02:02.896 --> 00:02:06.362
E multiplicamos o gradiente
por esse número.

00:02:06.395 --> 00:02:08.833
Dar um passo é
exatamente igual

00:02:08.866 --> 00:02:12.271
a atualizar os pesos e o viés,
como a seguir.

00:02:12.304 --> 00:02:15.756
O peso de wi será agora wi´

00:02:15.789 --> 00:02:21.575
dado por wi menos alfa
vezes o derivativo parcial do erro

00:02:21.608 --> 00:02:24.224
em relação a wi.

00:02:24.257 --> 00:02:29.326
O viés agora será b´
dado por b menos alfa

00:02:29.359 --> 00:02:33.953
vezes o derivativo do erro
em relação a b.

00:02:33.986 --> 00:02:37.895
Isto nos leva a uma predição
com uma função de erro mais baixa.

00:02:37.928 --> 00:02:40.755
Podemos concluir
que a predição que temos agora,

00:02:40.788 --> 00:02:43.337
com os pesos W´ e b´,

00:02:43.370 --> 00:02:45.293
é melhor do que
a que tínhamos antes,

00:02:45.326 --> 00:02:47.905
com pesos W e b.

00:02:47.938 --> 00:02:50.604
Isto é exatamente
o passo gradiente descendente.

