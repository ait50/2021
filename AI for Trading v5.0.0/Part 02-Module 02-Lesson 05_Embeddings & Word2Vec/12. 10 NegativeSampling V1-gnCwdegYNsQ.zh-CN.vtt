WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.530
上个模型训练时间很长

00:00:02.530 --> 00:00:05.115
其实有一些加快训练流程的方式

00:00:05.115 --> 00:00:09.445
在本视频中 我将介绍一种方式 叫做负采样

00:00:09.445 --> 00:00:11.120
虽然这是一个新的 notebook

00:00:11.120 --> 00:00:13.230
但是其中包含的信息

00:00:13.230 --> 00:00:16.690
与上个 notebook 很类似 包括这个结构示意图

00:00:16.690 --> 00:00:21.350
这是当前结构 我们对输出应用了 softmax 层级

00:00:21.350 --> 00:00:24.170
因为我们处理的是成千上万个字词

00:00:24.170 --> 00:00:27.645
所以 softmax 层级将包含成千上万个单元

00:00:27.645 --> 00:00:29.285
但是对于任何一个输入

00:00:29.285 --> 00:00:32.300
只有一个真实的上下文目标

00:00:32.300 --> 00:00:34.215
也就是说 当我们训练模型时

00:00:34.215 --> 00:00:37.310
虽然我们只关心一个真实输出

00:00:37.310 --> 00:00:41.660
但是这两个层级之间的权重都会轻微变化

00:00:41.660 --> 00:00:45.840
所以我们只会以有意义的方式更新很少的权重

00:00:45.840 --> 00:00:50.270
我们可以从 softmax 层级逼近损失

00:00:50.270 --> 00:00:54.535
方法是一次仅更新一小部分权重

00:00:54.535 --> 00:00:58.520
我们将更新已知正确的目标输出对应的权重

00:00:58.520 --> 00:01:00.815
但同时更新一小部分错误或噪点目标

00:01:00.815 --> 00:01:06.445
通常大约为 100 个 而不是全部的 60,000 个

00:01:06.445 --> 00:01:09.320
这个流程称为负采样

00:01:09.320 --> 00:01:13.865
为了实现该功能 我们需要对模型做出两大更改

00:01:13.865 --> 00:01:17.989
首先 我们不对所有字词应用 softmax 层级

00:01:17.989 --> 00:01:20.915
每次只关心一个输出

00:01:20.915 --> 00:01:23.510
与使用嵌入层

00:01:23.510 --> 00:01:26.000
将输入字词映射到一行嵌入权重相似

00:01:26.000 --> 00:01:28.190
现在我们可以使用另一个嵌入层

00:01:28.190 --> 00:01:31.060
将输出字词映射到一行隐藏权重

00:01:31.060 --> 00:01:33.235
所以我们将有两个嵌入层

00:01:33.235 --> 00:01:35.750
一个针对输入字词 一个针对输出字词

00:01:35.750 --> 00:01:39.410
其次 我们需要使用另一种损失函数

00:01:39.410 --> 00:01:43.950
该函数只关心真实目标和一部分有噪点的及正确的目标上下文字词

00:01:43.950 --> 00:01:46.340
也就是这个损失函数

00:01:46.340 --> 00:01:47.945
公式比较长

00:01:47.945 --> 00:01:50.140
我来介绍下每部分的含义

00:01:50.140 --> 00:01:52.210
先看看第一项

00:01:52.210 --> 00:01:55.000
这是一个负对数运算

00:01:55.000 --> 00:01:59.900
这个小写的 σ 表示 S 型激活函数

00:01:59.900 --> 00:02:04.970
S 型激活函数会将任何输入的范围缩放到 0-1 之间

00:02:04.970 --> 00:02:07.965
我们看看括号里的输入

00:02:07.965 --> 00:02:13.415
uw0 转置矩阵表示输出目标字词的嵌入向量

00:02:13.415 --> 00:02:15.740
它表示对于给定输入字词来说

00:02:15.740 --> 00:02:18.970
已知正确的上下文目标字词对应的嵌入向量

00:02:18.970 --> 00:02:21.305
这个 T 是转置符号

00:02:21.305 --> 00:02:26.275
vwi 表示输入字词的嵌入向量

00:02:26.275 --> 00:02:30.180
通常用 u 表示输出嵌入向量 用 v 表示输入嵌入向量

00:02:30.180 --> 00:02:33.335
之前计算 cosine_similarity 时提到

00:02:33.335 --> 00:02:38.085
这样的转置乘法等效于点积运算

00:02:38.085 --> 00:02:42.500
所以整个第一项的含义是计算正确输出字词向量与输入字词向量的点积

00:02:42.500 --> 00:02:46.850
然后对点积结果应用 Log-Sigmoid 函数

00:02:46.850 --> 00:02:49.800
这部分表示正确目标损失

00:02:49.800 --> 00:02:54.020
接下来我们需要对输出进行采样并获得一些噪点目标字词

00:02:54.020 --> 00:02:56.460
这就是第二部分的作用

00:02:56.460 --> 00:02:58.340
逐个讲解下

00:02:58.340 --> 00:03:03.025
大写的 Σ 表示对所有字词 wi 求和

00:03:03.025 --> 00:03:08.060
Pn(w) 表示这些字词来自噪点分布

00:03:08.060 --> 00:03:10.910
噪点分布是指

00:03:10.910 --> 00:03:13.990
不在输入字词的上下文中的词汇表字词

00:03:13.990 --> 00:03:16.490
也就是说 我们将从词汇表中随机抽取样本字词

00:03:16.490 --> 00:03:19.925
来获得这些不相关的噪点目标字词

00:03:19.925 --> 00:03:21.590
所以 Pn(w)

00:03:21.590 --> 00:03:24.170
是一个任意概率分布

00:03:24.170 --> 00:03:27.160
我们可以决定对要抽样的字词设定多大的权重

00:03:27.160 --> 00:03:30.455
它可以是一个均匀分布 即抽取所有字词的概率是相等的

00:03:30.455 --> 00:03:33.020
也可以根据每个字词

00:03:33.020 --> 00:03:36.145
出现在文本词汇表中的频率进行采样

00:03:36.145 --> 00:03:38.595
即一元模型分布 U(w)

00:03:38.595 --> 00:03:41.570
负采样论文的作者们发现

00:03:41.570 --> 00:03:45.710
最佳分布为 U(w)^(3/4)

00:03:45.710 --> 00:03:49.205
这是最后一部分 看起来和第一项很相似

00:03:49.205 --> 00:03:54.150
这部分会计算噪点向量 U(wi) 与之前的输入向量之间的点积

00:03:54.150 --> 00:03:56.530
并对结果取负数 然后再应用 Log Sigmoid 函数

00:03:56.530 --> 00:03:59.490
解释下整个损失函数的作用

00:03:59.490 --> 00:04:03.390
这个 S 型函数会返回 0-1 之间的概率

00:04:03.390 --> 00:04:06.800
所以第一项会促使

00:04:06.800 --> 00:04:10.280
网络预测正确上下文字词的概率接近 1

00:04:10.280 --> 00:04:13.460
在第二项里 由于我们对 S 型函数的输入取负值

00:04:13.460 --> 00:04:16.010
所以我们会促使

00:04:16.010 --> 00:04:19.380
网络预测错误噪点字词的概率之和接近 0

00:04:19.380 --> 00:04:22.220
Ok在下道练习中

00:04:22.220 --> 00:04:25.200
你将用代码定义这个负采样模型

