WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:05.130
在此 notebook 中 我将演示如何在 PyTorch 中实现 Word2Vec 模型

00:00:05.130 --> 00:00:08.955
你刚刚学习了嵌入的基本原理

00:00:08.955 --> 00:00:14.100
如果数据集包含大量类别或输入维数很高 例如大型词汇表

00:00:14.100 --> 00:00:16.740
我们将跳过独热编码步骤

00:00:16.740 --> 00:00:20.655
因为独热编码会生成很长的输入向量 向量中的元素基本为 0

00:00:20.655 --> 00:00:23.010
当独热向量与权重矩阵相乘时

00:00:23.010 --> 00:00:26.040
结果会返回一行值

00:00:26.040 --> 00:00:28.505
我们将利用这个规律

00:00:28.505 --> 00:00:32.790
例如 假设有一个独热向量 它的第四个索引的值为 1

00:00:32.790 --> 00:00:34.760
我们将此向量与权重矩阵相乘

00:00:34.760 --> 00:00:37.600
结果将返回该矩阵的第四行

00:00:37.600 --> 00:00:38.890
因此我们可以

00:00:38.890 --> 00:00:42.260
直接使用输入数字 而不是独热向量

00:00:42.260 --> 00:00:46.135
然后在嵌入权重矩阵中查询正确的输出

00:00:46.135 --> 00:00:50.405
我们看到 heart 编码为整数 958

00:00:50.405 --> 00:00:53.300
我们可以在嵌入权重矩阵的第 958 行

00:00:53.300 --> 00:00:56.670
查询该字词的嵌入向量

00:00:56.670 --> 00:00:59.395
这个表通常称为查询表

00:00:59.395 --> 00:01:01.345
在文本分析过程中 这种流程很方便

00:01:01.345 --> 00:01:06.110
因为我们已经知道如何将词汇表转换为整数标记

00:01:06.110 --> 00:01:09.990
每个唯一字词都对应一个整数值

00:01:09.990 --> 00:01:13.055
如果词汇表有 10,000 个字词

00:01:13.055 --> 00:01:15.620
则嵌入权重矩阵将有 10,000 行

00:01:15.620 --> 00:01:18.075
我们可以从中查询正确的输出值

00:01:18.075 --> 00:01:20.780
这些输出值在此权重矩阵里用行表示

00:01:20.780 --> 00:01:24.785
它们是该输入字词的向量表示法

00:01:24.785 --> 00:01:27.350
这些表示法称为嵌入

00:01:27.350 --> 00:01:30.980
它们的值数量与权重矩阵的列数相同

00:01:30.980 --> 00:01:33.495
这个宽度称为嵌入维度

00:01:33.495 --> 00:01:35.690
通常以数百个计

00:01:35.690 --> 00:01:39.575
Word2Vec 是一种特殊算法

00:01:39.575 --> 00:01:42.410
该算法指出“在给定文本中 出现在相同上下文里的任何字词”

00:01:42.410 --> 00:01:45.684
“应该具有相似的向量表示法”

00:01:45.684 --> 00:01:47.180
上下文是指

00:01:47.180 --> 00:01:51.110
出现在目标字词前后的字词

00:01:51.110 --> 00:01:53.195
举几个例子

00:01:53.195 --> 00:01:56.295
在这段文本里有几个句子

00:01:56.295 --> 00:01:59.190
这些句子都是在描述喝饮料

00:01:59.190 --> 00:02:02.240
对于某些句子 即使我删除目标字词

00:02:02.240 --> 00:02:04.310
你也能根据上下文信息

00:02:04.310 --> 00:02:06.900
猜出删除的字词是什么

00:02:06.900 --> 00:02:09.845
这段文本为“I often drink coffee in the mornings”

00:02:09.845 --> 00:02:11.380
“When I'm thirsty, I drink water”

00:02:11.380 --> 00:02:13.495
“I drink tea, before I go to sleep”

00:02:13.495 --> 00:02:15.880
这些字词前面的“I drink”

00:02:15.880 --> 00:02:17.855
表明上下文相似

00:02:17.855 --> 00:02:20.330
因此 coffee、water 和 tea

00:02:20.330 --> 00:02:23.014
应该具有相似的词嵌入

00:02:23.014 --> 00:02:25.560
如果我们查看的是一大段文本

00:02:25.560 --> 00:02:30.400
你可能会发现 coffee 与 morning time 紧密相关 等等

00:02:30.400 --> 00:02:34.445
通过查看目标字词和一些上下文信息

00:02:34.445 --> 00:02:38.750
Word2Vec 能够发现字词之间的相似性和关系

00:02:38.750 --> 00:02:40.975
对于相似的字词

00:02:40.975 --> 00:02:44.485
Word2Vec 应该会生成在向量空间里很靠近的向量

00:02:44.485 --> 00:02:47.770
不同的字词相互之间距离较远

00:02:47.770 --> 00:02:50.360
这样我们便能够进行向量运算

00:02:50.360 --> 00:02:52.790
Word2Vec 正是通过这种方式

00:02:52.790 --> 00:02:55.330
发现过去时态和现在时态字词之间的映射关系

00:02:55.330 --> 00:02:57.850
例如 将动词 drink 映射到 drinking

00:02:57.850 --> 00:02:59.220
与将 swam 映射到 swimming

00:02:59.220 --> 00:03:02.030
在向量空间里将是相同的转换

00:03:02.030 --> 00:03:06.095
Word2Vec 的实现方式有两种

00:03:06.095 --> 00:03:09.650
第一种方式会为模型提供上下文

00:03:09.650 --> 00:03:12.250
即目标字词前后的几个字词

00:03:12.250 --> 00:03:14.330
然后让模型预测缺失的目标字词

00:03:14.330 --> 00:03:18.050
即输入上下文字词并输出一个字词

00:03:18.050 --> 00:03:22.385
这称之为连续词袋模型 简称 CBOW 模型

00:03:22.385 --> 00:03:24.590
第二种方式正好相反

00:03:24.590 --> 00:03:28.699
输入目标字词 并让模型预测上下文字词

00:03:28.699 --> 00:03:31.745
即输入一个字词并输出几个上下文字词

00:03:31.745 --> 00:03:33.705
这称之为 Skip-Gram 模型

00:03:33.705 --> 00:03:35.380
我们将以这种方式实现 Word2Vec

00:03:35.380 --> 00:03:37.775
因为经验表明这种方式效果更好

00:03:37.775 --> 00:03:39.830
请注意 对于这两种模型

00:03:39.830 --> 00:03:44.935
都必须记住 上下文是具有指定大小的窗口

00:03:44.935 --> 00:03:49.235
例如目标字词前面的两个字词和后面的两个字词

00:03:49.235 --> 00:03:52.340
对于在时间 t 的输入字词 w

00:03:52.340 --> 00:03:55.725
上下文字词是从 t-2 到 t+2 的字词

00:03:55.725 --> 00:03:59.285
即过去的前两个字词和未来的后两个字词

00:03:59.285 --> 00:04:03.405
注意 上下文不包含原始目标字词

00:04:03.405 --> 00:04:07.570
介绍完此 notebook 和 Word2Vec Skip-Gram 模型后

00:04:07.570 --> 00:04:11.870
接下来我将演示我们要处理的数据并布置第一道练习

