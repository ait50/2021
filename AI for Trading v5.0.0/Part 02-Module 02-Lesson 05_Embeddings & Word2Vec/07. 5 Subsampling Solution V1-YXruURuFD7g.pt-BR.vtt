WEBVTT
Kind: captions
Language: pt-BR

00:00:00.463 --> 00:00:03.132
Aqui está a minha solução
para criarmos uma nova lista

00:00:03.165 --> 00:00:04.348
de palavras treinadas.

00:00:05.397 --> 00:00:07.884
Primeiro, eu calculei
a frequência da ocorrência

00:00:07.917 --> 00:00:10.059
de cada palavra
do vocabulário.

00:00:10.092 --> 00:00:14.123
Armazenei o comprimento do texto
na variável total_count,

00:00:14.156 --> 00:00:16.724
depois criei um dicionário
de frequências.

00:00:16.757 --> 00:00:20.588
Para cada token
e contagem da palavra do dicionário,

00:00:20.621 --> 00:00:22.844
adicionei um item
ao dicionário,

00:00:22.877 --> 00:00:24.716
sendo que a palavra
era a chave

00:00:24.749 --> 00:00:26.956
e o valor era a contagem
da palavra

00:00:26.989 --> 00:00:29.524
sobre a quantidade total
de palavras no texto,

00:00:29.557 --> 00:00:30.700
a frequência.

00:00:30.733 --> 00:00:33.948
Calculei a probabilidade
de descarte como p_drop,

00:00:33.981 --> 00:00:35.244
que é outro dicionário

00:00:35.277 --> 00:00:37.708
que mapeia palavras
para a probabilidade drop.

00:00:37.741 --> 00:00:40.804
Aqui usei a equação
de subamostragem para obter isso,

00:00:40.837 --> 00:00:43.452
que é 1 menos
a raiz quadrada do limite

00:00:43.485 --> 00:00:45.235
sob a frequência da palavra.

00:00:45.268 --> 00:00:48.237
Por fim, criei uma nova lista
de palavras de treinamento.

00:00:48.270 --> 00:00:50.387
Para cada palavra
na lista int_words,

00:00:50.420 --> 00:00:53.515
mantive a palavra
com alguma probabilidade.

00:00:53.548 --> 00:00:57.013
Eu gerei um valor aleatório
entre 0 e 1

00:00:57.046 --> 00:01:01.235
e conferi se o valor era menor
do que 1 menos a probabilidade drop

00:01:01.268 --> 00:01:02.397
da palavra.

00:01:02.430 --> 00:01:05.820
Isso mantém a palavra
com probabilidade

00:01:05.853 --> 00:01:08.132
igual a 1 menos p_drop.

00:01:08.165 --> 00:01:11.475
Se eu tiver uma probabilidade
drop igual a 0,98,

00:01:11.508 --> 00:01:13.355
então a chance
de manter a palavra

00:01:13.388 --> 00:01:15.315
será igual a 1 menos
este p_drop,

00:01:15.348 --> 00:01:17.484
que será igual a 0,02.

00:01:17.517 --> 00:01:20.534
Se eu gerar um valor
menor do que 0,02 -

00:01:20.567 --> 00:01:23.382
que é pouco provável -,
eu manterei a palavra

00:01:23.415 --> 00:01:25.070
na lista de treinamento.

00:01:25.103 --> 00:01:27.130
Existem outras formas
de resolver isso,

00:01:27.163 --> 00:01:30.296
mas gosto de pensar nisso
como: "Quais palavras eu manterei?"

00:01:30.329 --> 00:01:33.606
Eu imprimo as primeiras 30 palavras
dos dados de treinamento.

00:01:33.639 --> 00:01:36.144
Isso deve ser parecido
com os primeiros 30 tokens

00:01:36.177 --> 00:01:37.735
na lista int_words.

00:01:37.768 --> 00:01:41.255
Perceba que a maioria dos 0
e 1 sumiram,

00:01:41.288 --> 00:01:43.535
pois eram as palavras
comuns de antes.

00:01:43.568 --> 00:01:45.720
Então tudo está
conforme o esperado.

00:01:45.753 --> 00:01:47.219
Vamos aos próximos passos:

00:01:47.252 --> 00:01:50.663
definir uma janela de contexto
e criar lotes para os dados.

