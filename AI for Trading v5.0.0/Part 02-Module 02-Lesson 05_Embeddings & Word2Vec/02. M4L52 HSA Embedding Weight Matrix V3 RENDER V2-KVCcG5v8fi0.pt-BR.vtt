WEBVTT
Kind: captions
Language: pt-BR

00:00:00.400 --> 00:00:03.403
Falamos sobre como redes neurais
são projetadas

00:00:03.436 --> 00:00:05.172
para aprender
com dados numéricos.

00:00:05.205 --> 00:00:07.674
No nosso caso,
embedding de palavras é sobre

00:00:07.707 --> 00:00:11.411
melhorar a capacidade de redes
aprenderem com dados de texto.

00:00:11.444 --> 00:00:14.814
A ideia é que embeddings
podem melhorar a capacidade

00:00:14.848 --> 00:00:16.917
de as redes aprenderem
com dados de texto,

00:00:16.950 --> 00:00:19.853
representando estes dados
como vetores de menor dimensão.

00:00:19.886 --> 00:00:22.189
Vamos pensar nisto
em um exemplo.

00:00:22.222 --> 00:00:25.425
Ao lidar com um texto
e dividi-lo em palavras,

00:00:25.458 --> 00:00:27.527
você tem milhares
de palavras diferentes

00:00:27.561 --> 00:00:29.229
num grande conjunto de dados.

00:00:29.262 --> 00:00:32.832
Ao usar estas palavras como input
para uma rede, como uma RNN,

00:00:32.866 --> 00:00:35.035
vimos que podemos usar
one-hot nelas,

00:00:35.068 --> 00:00:39.072
o que significa que teremos
vetores gigantes com 50 mil unidades

00:00:39.105 --> 00:00:41.241
e apenas um deles
é configurado para 1.

00:00:41.274 --> 00:00:43.176
Os outros
são configurados para 0.

00:00:43.210 --> 00:00:45.078
E você passa este vetor
como input

00:00:45.111 --> 00:00:47.214
para uma camada oculta
numa rede.

00:00:47.247 --> 00:00:48.515
O output desta camada

00:00:48.548 --> 00:00:51.051
é calculado multiplicando
o vetor de input

00:00:51.084 --> 00:00:53.086
por uma matriz de pesos
aprendidos.

00:00:53.119 --> 00:00:55.622
O resultado é
uma grande matriz de valores,

00:00:55.655 --> 00:00:58.792
a maioria igual a zero
por causa do vetor one-hot.

00:00:58.825 --> 00:01:01.428
Estes recursos são usados
em valores

00:01:01.461 --> 00:01:03.196
que não têm
nenhuma informação.

00:01:03.230 --> 00:01:05.365
E isto é computacionalmente
ineficiente.

00:01:05.398 --> 00:01:08.068
Para resolver este problema,
podemos usar embeddings,

00:01:08.101 --> 00:01:11.738
que fornecem um atalho para fazer
esta multiplicação de matrizes.

00:01:11.771 --> 00:01:13.207
Para aprender embeddings,

00:01:13.240 --> 00:01:15.642
usamos uma camada linear
completamente conectada,

00:01:15.675 --> 00:01:16.775
como você já viu.

00:01:16.809 --> 00:01:18.711
Chamamos isto
de "camada de embedding",

00:01:18.744 --> 00:01:20.547
e os pesos são
os pesos de embedding.

00:01:20.580 --> 00:01:24.184
Estes pesos são valores aprendidos
no treino do modelo de embedding,

00:01:24.217 --> 00:01:26.319
e formam uma matriz de pesos
bem útil.

00:01:26.353 --> 00:01:27.587
Com esta matriz,

00:01:27.621 --> 00:01:30.824
podemos pular as grandes
multiplicações de antes

00:01:30.857 --> 00:01:33.793
pegando os valores
do output da camada oculta

00:01:33.827 --> 00:01:36.329
diretamente de uma linha
na matriz de pesos.

00:01:36.363 --> 00:01:38.365
Podemos fazer isto
porque a multiplicação

00:01:38.398 --> 00:01:41.368
de um vetor one-hot
com uma matriz de pesos

00:01:41.401 --> 00:01:43.270
retorna só a linha da matriz

00:01:43.303 --> 00:01:47.174
que corresponde ao índice
do 1 ou da unidade de input.

00:01:47.207 --> 00:01:49.609
Em vez de fazer multiplicação
de matrizes,

00:01:49.643 --> 00:01:52.379
usamos a matriz de embedding
como tabela de pesquisa.

00:01:52.412 --> 00:01:55.482
E em vez de representar palavras
como vetores one-hot,

00:01:55.515 --> 00:01:58.385
podemos codificar cada palavra
com um único inteiro.

00:01:58.418 --> 00:02:01.188
Digamos que temos
a palavra "heart" codificada

00:02:01.221 --> 00:02:03.390
como o inteiro 958.

00:02:03.423 --> 00:02:06.493
Para obter os valores
da camada oculta de heart,

00:02:06.526 --> 00:02:10.330
pegamos a 958º linha
da matriz de embedding.

00:02:10.363 --> 00:02:12.933
Este processo é chamado
de "pesquisa de embedding".

00:02:12.966 --> 00:02:15.869
E o número de unidades ocultas
é a dimensão de embedding.

00:02:15.902 --> 00:02:19.105
A tabela de pesquisa de embedding
é uma matriz de pesos

00:02:19.139 --> 00:02:21.441
e a camada de embedding
é uma camada oculta.

00:02:21.474 --> 00:02:25.512
A tabela de pesquisa tem pesos
que são aprendidos no treino,

00:02:25.545 --> 00:02:27.146
como em qualquer matriz
de pesos.

00:02:27.179 --> 00:02:30.417
Este é o básico
de como o embedding funciona.

00:02:30.450 --> 00:02:33.220
Nas próximas seções,
você verá como o Word2Vec

00:02:33.253 --> 00:02:36.321
usa a camada de embedding
para achar representações de vetores

00:02:36.354 --> 00:02:38.225
de palavras
com conteúdo semântico.

