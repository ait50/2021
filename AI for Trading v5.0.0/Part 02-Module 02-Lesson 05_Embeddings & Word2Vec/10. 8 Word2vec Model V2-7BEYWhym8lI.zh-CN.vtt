WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.390
我们已经预处理数据并创建了批次数据

00:00:03.390 --> 00:00:05.665
下面将开始构建网络

00:00:05.665 --> 00:00:08.970
这是要构建的网络的整体结构

00:00:08.970 --> 00:00:10.170
这些是输入

00:00:10.170 --> 00:00:12.910
它们是多批训练字词标记数据

00:00:12.910 --> 00:00:15.090
加载一批数据后会发现

00:00:15.090 --> 00:00:18.670
有很多值在这个输入向量中是重复的

00:00:18.670 --> 00:00:21.670
传入一个很长的整数列表

00:00:21.670 --> 00:00:24.710
然后进入这个隐藏层（即嵌入层）

00:00:24.710 --> 00:00:27.290
嵌入层将查看这些输入整数

00:00:27.290 --> 00:00:30.640
并创建一个查询表

00:00:30.640 --> 00:00:32.940
对于每个潜在整数值

00:00:32.940 --> 00:00:35.720
嵌入权重矩阵里都有对应的一行数据

00:00:35.720 --> 00:00:39.035
矩阵的宽度是我们定义的嵌入维度

00:00:39.035 --> 00:00:42.650
该维度将是嵌入层输出的大小

00:00:42.650 --> 00:00:47.810
然后将这些嵌入传入最后的全连接 softmax 输出层

00:00:47.810 --> 00:00:49.825
在 skip_gram 模型中

00:00:49.825 --> 00:00:52.310
我们将传入一些输入字词

00:00:52.310 --> 00:00:54.800
并训练整个模型生成上下文字词

00:00:54.800 --> 00:00:57.110
对于一个输入值

00:00:57.110 --> 00:01:02.050
目标将是从输入字词所在窗口中随机选择的上下文字词

00:01:02.050 --> 00:01:05.120
输出层将输出一些概率

00:01:05.120 --> 00:01:08.565
表示随机选择的上下文字词为

00:01:08.565 --> 00:01:09.830
“the”、“of”、“9”

00:01:09.830 --> 00:01:12.265
或词汇表中任何其他字词的概率

00:01:12.265 --> 00:01:13.850
我们将使用 softmax 层的输出

00:01:13.850 --> 00:01:17.675
预测目标上下文字词

00:01:17.675 --> 00:01:22.460
我们将查看概率最高的上下文字词

00:01:22.460 --> 00:01:24.680
然后训练整个模型

00:01:24.680 --> 00:01:26.300
使隐藏层生成

00:01:26.300 --> 00:01:28.640
这些输入字词的向量表示法

00:01:28.640 --> 00:01:33.220
所以嵌入查询表中的每行将是某个字词的向量表示法

00:01:33.220 --> 00:01:36.790
例如第 0 行将是“the”的嵌入

00:01:36.790 --> 00:01:39.140
这些向量包含一些语义信息

00:01:39.140 --> 00:01:41.120
我们只对这些

00:01:41.120 --> 00:01:43.475
语义嵌入信息感兴趣

00:01:43.475 --> 00:01:46.040
我们可以对这些嵌入执行有趣的操作

00:01:46.040 --> 00:01:49.370
执行向量运算 看看哪些字词最相似

00:01:49.370 --> 00:01:51.290
或者将这些嵌入当做输入传入另一个模型中

00:01:51.290 --> 00:01:54.045
并使用该模型处理相同的文本输入数据

00:01:54.045 --> 00:01:55.335
训练完毕后

00:01:55.335 --> 00:01:58.250
我们可以删掉最后的 softmax 层级

00:01:58.250 --> 00:02:00.440
因为该层级一开始只是帮助我们训练此模型

00:02:00.440 --> 00:02:02.940
并创建正确的嵌入

00:02:02.940 --> 00:02:04.705
Ok在定义模型之前

00:02:04.705 --> 00:02:06.350
先看一个函数 这个函数能够查看

00:02:06.350 --> 00:02:09.065
这个模型学习的是什么字词关系

00:02:09.065 --> 00:02:11.570
我在介绍 word2vec 的时候提到

00:02:11.570 --> 00:02:13.730
如果将字词表示为向量

00:02:13.730 --> 00:02:17.675
我们将能够在向量空间里对这种字词进行数学运算

00:02:17.675 --> 00:02:19.810
为了查看哪些字词很相似

00:02:19.810 --> 00:02:23.935
我将使用余弦表示相似性

00:02:23.935 --> 00:02:28.585
cos(θ) 会查看两个向量 a 和 b 之间的夹角

00:02:28.585 --> 00:02:30.170
夹角用 θ 表示两个向量

00:02:30.170 --> 00:02:34.850
之间的相似性等于二者夹角的余弦

00:02:34.850 --> 00:02:36.720
如果你熟悉向量运算的话

00:02:36.720 --> 00:02:40.500
就知道 cos(θ) 等于 a 和 b 的标准化点积

00:02:40.500 --> 00:02:42.645
可以这么理解

00:02:42.645 --> 00:02:43.970
当 θ 等于 0 时

00:02:43.970 --> 00:02:46.050
cos(θ) 等于 1

00:02:46.050 --> 00:02:48.820
余弦的最大值是 1

00:02:48.820 --> 00:02:53.695
如果 θ 等于 90 度 即这两个向量正交

00:02:53.695 --> 00:02:55.620
那么 cos(θ) 等于 0

00:02:55.620 --> 00:02:59.350
similarity 是一个 0 到 1 之间的值

00:02:59.350 --> 00:03:02.440
表示两个向量在向量空间里的相似程度

00:03:02.440 --> 00:03:05.320
我们看看这个 cosine_similarity 函数

00:03:05.320 --> 00:03:07.805
参数是嵌入层

00:03:07.805 --> 00:03:10.080
验证大小和验证窗口

00:03:10.080 --> 00:03:13.330
从传入层级里获取嵌入

00:03:13.330 --> 00:03:14.845
这些是层级权重

00:03:14.845 --> 00:03:19.355
然后执行运算并存储这些嵌入向量的大小

00:03:19.355 --> 00:03:21.230
大小将等于

00:03:21.230 --> 00:03:24.400
嵌入向量平方和的平方根

00:03:24.400 --> 00:03:29.510
然后随机选择一些常见和不常见验证字词样本

00:03:29.510 --> 00:03:31.910
常见字词是

00:03:31.910 --> 00:03:33.980
范围从 0 到 1,000 的整数

00:03:33.980 --> 00:03:36.695
不常见字词的范围更高

00:03:36.695 --> 00:03:40.345
索引更小表示字词出现频率更高

00:03:40.345 --> 00:03:44.660
我从常见范围里选择一半的验证样本

00:03:44.660 --> 00:03:46.780
然后从不常见范围里选择另一半的验证样本

00:03:46.780 --> 00:03:51.190
将它们放入 NumPy 数组中并转换为 LongTensor 类型

00:03:51.190 --> 00:03:55.105
然后将这些验证样本传入嵌入层

00:03:55.105 --> 00:03:58.275
获得向量表示法

00:03:58.275 --> 00:04:01.850
我们将这些验证字词编码为向量 a

00:04:01.850 --> 00:04:04.215
我们将计算 a

00:04:04.215 --> 00:04:05.380
与嵌入表中的每个字词向量 b

00:04:05.380 --> 00:04:07.365
之间的相似性

00:04:07.365 --> 00:04:12.000
相似性是 a 和 b 的点积除以大小

00:04:12.000 --> 00:04:15.080
这个点积是验证向量 a

00:04:15.080 --> 00:04:19.095
与嵌入向量 b 的转置矩阵之间的矩阵乘法运算

00:04:19.095 --> 00:04:21.410
在这里除以大小

00:04:21.410 --> 00:04:23.120
与这里的方程并不完全相同

00:04:23.120 --> 00:04:25.630
但是能够获得有效的相似性值

00:04:25.630 --> 00:04:27.390
只是乘以了一个常量

00:04:27.390 --> 00:04:31.225
这个函数将返回验证样本和相似性

00:04:31.225 --> 00:04:34.820
稍后我们便能够输出验证字词

00:04:34.820 --> 00:04:38.440
以及嵌入表中与这些字词语义相似的字词

00:04:38.440 --> 00:04:41.135
这样便于我们检查

00:04:41.135 --> 00:04:44.425
嵌入表是否将语义相似的字词组合到一起

00:04:44.425 --> 00:04:46.335
我们已经提供了这个函数

00:04:46.335 --> 00:04:48.155
你不需要更改任何代码

00:04:48.155 --> 00:04:50.030
现在开始定义模型

00:04:50.030 --> 00:04:52.490
模型将接受一些输入数据

00:04:52.490 --> 00:04:53.875
它有一个嵌入层

00:04:53.875 --> 00:04:56.045
以及一个最终 softmax 输出层

00:04:56.045 --> 00:04:59.010
你需要使用 PyTorch 的嵌入层定义该模型

00:04:59.010 --> 00:05:00.425
你可以点击此链接并详细了解嵌入层

00:05:00.425 --> 00:05:01.965
这是嵌入层的文档

00:05:01.965 --> 00:05:04.725
嵌入层是一种稀疏层

00:05:04.725 --> 00:05:07.430
参数包括输入嵌入的数量

00:05:07.430 --> 00:05:09.020
即嵌入权重查询矩阵里的行数

00:05:09.020 --> 00:05:12.360
另一个参数是嵌入维度

00:05:12.360 --> 00:05:15.200
表示每个嵌入向量的大小

00:05:15.200 --> 00:05:17.885
即嵌入查询表中的列数

00:05:17.885 --> 00:05:21.085
在定义这个层级时 这两个参数是最重要的输入

00:05:21.085 --> 00:05:22.795
还需要在嵌入层后面

00:05:22.795 --> 00:05:24.680
定义一个线性层级

00:05:24.680 --> 00:05:27.790
输入是嵌入大小 输出是预测上下文字词

00:05:27.790 --> 00:05:30.770
还需要对输出应用 softmax 函数

00:05:30.770 --> 00:05:33.415
这样模型就能返回字词概率

00:05:33.415 --> 00:05:35.810
这是该模型的框架代码

00:05:35.810 --> 00:05:37.200
在实例化该模型时

00:05:37.200 --> 00:05:40.490
我们需要传入 n_vocab 的值

00:05:40.490 --> 00:05:41.975
n_vocab 表示词汇表的大小

00:05:41.975 --> 00:05:44.730
并传入 n_embed 的值 表示嵌入维度

00:05:44.730 --> 00:05:48.915
你已经知道如何编写 init 和 forward 函数

00:05:48.915 --> 00:05:51.050
编写完这两个函数后

00:05:51.050 --> 00:05:53.855
你需要使用下面提供的训练循环训练模型

00:05:53.855 --> 00:05:56.355
强烈建议在 GPU 上训练模型

00:05:56.355 --> 00:06:00.035
即使在 GPU 上操作 训练这个模型也要很长时间

00:06:00.035 --> 00:06:04.020
所以建议暂时只训练 1-2 个周期好的

00:06:04.020 --> 00:06:05.505
请完成这道练习

00:06:05.505 --> 00:06:09.930
接下来我将演示如何定义和训练 skip-gram 模型

