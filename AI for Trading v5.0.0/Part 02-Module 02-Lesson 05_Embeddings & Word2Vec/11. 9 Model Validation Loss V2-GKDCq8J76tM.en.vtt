WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.189
This is what we want our model to look like.

00:00:02.189 --> 00:00:05.820
It should take in some inputs and then put those through an embedding layer,

00:00:05.820 --> 00:00:10.530
which produces some embedded vectors that are sent to a final softmax output layer,

00:00:10.529 --> 00:00:12.239
and here's my model definition.

00:00:12.240 --> 00:00:14.324
You can see that it's a pretty simple model.

00:00:14.324 --> 00:00:17.605
First, I'm defining my embedding layer, the self.embed.

00:00:17.605 --> 00:00:19.964
This takes n the length of my word vocabulary.

00:00:19.964 --> 00:00:22.350
This means that it will create an embedding weight matrix that

00:00:22.350 --> 00:00:24.975
has a row for each of the words in our vocabulary,

00:00:24.975 --> 00:00:29.025
and this will output vectors of size n_embed, our embedding dimension.

00:00:29.024 --> 00:00:33.659
Then, I have a fully-connected layer that takes in that embedding dimension as input,

00:00:33.659 --> 00:00:36.914
and its output size is also the length of our vocabulary.

00:00:36.914 --> 00:00:40.234
That's because this output is a series of word class scores

00:00:40.234 --> 00:00:43.524
that tells us the likely context word for a given input word,

00:00:43.524 --> 00:00:46.829
and then I've defined a softmax activation layer here.

00:00:46.829 --> 00:00:49.320
You could have just done this in the forward function too.

00:00:49.320 --> 00:00:50.700
This is just one solution.

00:00:50.700 --> 00:00:52.054
Then at my forward function,

00:00:52.054 --> 00:00:54.894
I'm passing in my input X into the embedding layer.

00:00:54.895 --> 00:00:58.100
This returns are embeddings which moves to our fully-connected layer,

00:00:58.100 --> 00:01:00.079
which returns a series of class scores.

00:01:00.079 --> 00:01:02.570
Finally, a softmax activation function is

00:01:02.570 --> 00:01:06.170
applied and I'll be left with my log probabilities for context words.

00:01:06.170 --> 00:01:07.864
Below in this training section,

00:01:07.864 --> 00:01:09.879
I'm actually going to instantiate this model.

00:01:09.879 --> 00:01:13.969
So here, I've defined an embedding dimension and I've set this to 300,

00:01:13.969 --> 00:01:16.894
but you're welcome to experiment with larger or smaller values.

00:01:16.894 --> 00:01:18.859
The embedding dimension can be thought of as

00:01:18.859 --> 00:01:20.984
the number of word features that we can detect,

00:01:20.984 --> 00:01:23.284
like the length, the type of word, and so on.

00:01:23.284 --> 00:01:27.619
So, this takes in the entire length of our vocabulary and the embedding dimension,

00:01:27.620 --> 00:01:30.070
and I've moved this to a GPU for training.

00:01:30.069 --> 00:01:32.729
Here, you'll see that I'm using negative log-likelihood loss,

00:01:32.730 --> 00:01:35.840
and this is because a softmax in combination with

00:01:35.840 --> 00:01:38.960
negative log-likelihood basically equals cross entropy loss.

00:01:38.959 --> 00:01:43.199
So, this is a great loss for looking at probabilities of context words,

00:01:43.200 --> 00:01:45.109
and I'm using an Adam optimizer,

00:01:45.109 --> 00:01:49.079
which is just my go-to and I'm passing in my model parameters and a learning rate.

00:01:49.079 --> 00:01:52.769
Then I have my training loop and I've decided to train for five epochs.

00:01:52.769 --> 00:01:55.789
In this training, actually took a few hours even on GPU,

00:01:55.790 --> 00:01:57.830
so I'd recommend that you train for a shorter amount of

00:01:57.829 --> 00:02:01.054
time or wait until I show you how to train more efficiently.

00:02:01.055 --> 00:02:03.170
So, in my training loop, I'm getting batches

00:02:03.170 --> 00:02:05.150
of data by calling the generator function that

00:02:05.150 --> 00:02:09.344
we defined above and passing in my list of chain words and a batch size.

00:02:09.344 --> 00:02:12.340
I'm getting my inputs and my target context words,

00:02:12.340 --> 00:02:14.905
and I'm converting them into LongTensor types,

00:02:14.905 --> 00:02:17.344
and moving these two GPU if it's available,

00:02:17.344 --> 00:02:19.775
and then I'm performing backpropagation as usual,

00:02:19.775 --> 00:02:22.205
and passing my inputs into my skip-gram model

00:02:22.205 --> 00:02:24.520
to get the log probabilities for the context words.

00:02:24.520 --> 00:02:28.719
Then I'm applying my loss function to these contexts words and my targets,

00:02:28.719 --> 00:02:32.280
then performing backpropagation and updating the weights of my model,

00:02:32.280 --> 00:02:36.090
not forgetting to zero out any accumulated gradients before these two steps.

00:02:36.090 --> 00:02:40.865
Then I'm printing out some validation examples using my cosine similarity function.

00:02:40.865 --> 00:02:43.730
Here, I'm passing in my model and a GPU device,

00:02:43.729 --> 00:02:46.965
and I'm getting back some validation examples and their similarities.

00:02:46.965 --> 00:02:49.490
Here, I'm actually using topk sampling to get

00:02:49.490 --> 00:02:52.985
the top six most similar words to a given example.

00:02:52.985 --> 00:02:56.240
Here, I'm iterating through my validation examples.

00:02:56.240 --> 00:02:58.580
I'm printing out the first validation word and then

00:02:58.580 --> 00:03:01.545
the five closest words next to it after a line character,

00:03:01.544 --> 00:03:03.809
and here are some initial results.

00:03:03.810 --> 00:03:07.284
I printed a lot of data after training for five epochs.

00:03:07.284 --> 00:03:10.490
At first, these word associations look pretty random.

00:03:10.490 --> 00:03:12.010
We have and, returns,

00:03:12.009 --> 00:03:13.739
liverpudlians, and so on.

00:03:13.740 --> 00:03:15.740
But as I train, I should see that

00:03:15.740 --> 00:03:18.480
these validation words are getting more and more similar.

00:03:18.479 --> 00:03:21.439
If I scroll down all the way to the end of my training,

00:03:21.439 --> 00:03:24.129
I can see that similar words are nicely grouped together.

00:03:24.129 --> 00:03:27.139
You can see a bunch of number words are grouped together.

00:03:27.139 --> 00:03:30.219
Here, I have a bunch of animals and mammals grouped in one line,

00:03:30.219 --> 00:03:32.989
some lines that are related to states and politics,

00:03:32.989 --> 00:03:35.930
and even lines that are related to a place and a language.

00:03:35.930 --> 00:03:38.390
So, it looks like my word2vec model is learning,

00:03:38.389 --> 00:03:41.479
and I can visualize these embeddings in another way too.

00:03:41.479 --> 00:03:45.244
Another really powerful method for visualization is called t-SNE,

00:03:45.245 --> 00:03:48.993
which stands for t-distributed stochastic neighbor embeddings.

00:03:48.993 --> 00:03:52.909
It's a non-linear dimensionality reduction technique that aims to separate

00:03:52.909 --> 00:03:57.615
data in a way that cluster similar data close together and separates different data.

00:03:57.615 --> 00:04:01.754
In this case, it's an algorithm that I'm loading in from the sklearn library.

00:04:01.754 --> 00:04:04.775
I give it the number of embeddings that I want to visualize,

00:04:04.775 --> 00:04:07.189
and I get these embeddings from the weights of

00:04:07.189 --> 00:04:09.979
our embedded layer which I'm calling by name from our model.

00:04:09.979 --> 00:04:13.094
So, remember that our embedding layer was just named embed,

00:04:13.094 --> 00:04:16.360
and I can get the weights by same model.embed.weight.

00:04:16.360 --> 00:04:20.300
So here, I'm applying t-SNE to 600 of our embeddings,

00:04:20.300 --> 00:04:23.740
and this is what this t-SNE clustering ends up looking like.

00:04:23.740 --> 00:04:26.610
We can actually see that similar words are grouped together.

00:04:26.610 --> 00:04:29.425
Here we have east, west, north, and south.

00:04:29.425 --> 00:04:32.735
If we look to the right, we can see some musical terms: rock,

00:04:32.735 --> 00:04:34.795
music, album, band, and song.

00:04:34.795 --> 00:04:37.475
Lower down, we can see some religious terms,

00:04:37.475 --> 00:04:39.390
some colors over here,

00:04:39.389 --> 00:04:42.699
some academic terms: school, university, and college.

00:04:42.699 --> 00:04:44.170
On the left side here,

00:04:44.170 --> 00:04:48.949
I can see clusters of the months in the year and it looks like a few integer values here.

00:04:48.949 --> 00:04:52.789
So, this clustering indicates that my word2vec model has worked.

00:04:52.790 --> 00:04:56.330
It learns to generate embeddings that hold semantic meaning,

00:04:56.329 --> 00:05:00.719
and this also gives us a cool way to visualize the relationships between words in space.

00:05:00.720 --> 00:05:04.375
So, one problem with this model was that it took quite a while to train,

00:05:04.375 --> 00:05:06.730
and next I'm going to address that challenge.

