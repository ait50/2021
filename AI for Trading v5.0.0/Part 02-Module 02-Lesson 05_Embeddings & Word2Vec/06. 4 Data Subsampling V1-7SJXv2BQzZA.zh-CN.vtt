WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.075
下面我们将实现 Skip-Gram Word2Vec 模型

00:00:04.075 --> 00:00:07.455
首先需要加载必要的数据

00:00:07.455 --> 00:00:10.155
我的文本是用维基百科文章生成的大段文本

00:00:10.155 --> 00:00:13.095
这个文本文件由 Matt Mahoney 提供

00:00:13.095 --> 00:00:14.340
如果你在本地操作

00:00:14.340 --> 00:00:18.060
则需要点击此链接并下载 zip 文件

00:00:18.060 --> 00:00:20.700
然后将此文件移到数据目录中并解压

00:00:20.700 --> 00:00:24.820
你会看到一个叫做 text8 的文件

00:00:24.820 --> 00:00:28.115
我已经将这些数据放入数据目录中

00:00:28.115 --> 00:00:31.600
现在按名称加载该文件并输出前 100 个字符

00:00:31.600 --> 00:00:36.460
文本第一部分讲得是无政府主义和工人阶级

00:00:36.460 --> 00:00:38.275
正确加载数据

00:00:38.275 --> 00:00:40.140
然后预处理数据

00:00:40.140 --> 00:00:42.740
我需要将这段文本拆分为庞大的字词列表

00:00:42.740 --> 00:00:46.130
这样就能构建一个词汇表

00:00:46.130 --> 00:00:48.830
我将在提供的 utils.py 文件中

00:00:48.830 --> 00:00:51.860
使用函数 preprocess 完成预处理流程

00:00:51.860 --> 00:00:54.530
我们看看此文件中的代码

00:00:54.530 --> 00:00:58.700
转到 utils.py 文件并看看 preprocess 函数

00:00:58.700 --> 00:01:02.675
此函数的输入参数是 text 并对 text 执行一些操作

00:01:02.675 --> 00:01:04.470
首先 在这几行

00:01:04.470 --> 00:01:07.250
将所有标点转换为标记

00:01:07.250 --> 00:01:11.415
“.”变成“&lt;period&gt;”标记 等等

00:01:11.415 --> 00:01:13.880
然后使用计数器

00:01:13.880 --> 00:01:16.260
存储某些字词出现在文本中的次数

00:01:16.260 --> 00:01:18.980
计数器是一个集合

00:01:18.980 --> 00:01:22.745
它会返回一个字典 该字典由字词及其出现频率组成

00:01:22.745 --> 00:01:26.255
在这里创建一个修整字词列表

00:01:26.255 --> 00:01:29.800
删除在数据集中出现次数不超过 5 次的字词

00:01:29.800 --> 00:01:32.660
这样能够显著减少数据噪点带来的问题

00:01:32.660 --> 00:01:35.610
并且能够改善向量表示法的质量

00:01:35.610 --> 00:01:38.135
最后返回修整的字词列表

00:01:38.135 --> 00:01:39.700
回到 notebook

00:01:39.700 --> 00:01:43.145
输入 words = utils.preprocess(text)

00:01:43.145 --> 00:01:45.610
输出前 30 个修整字词

00:01:45.610 --> 00:01:49.040
因为文本数据集很庞大 所以运行时间可能较长

00:01:49.040 --> 00:01:51.495
然后生成这样的输出结果

00:01:51.495 --> 00:01:54.005
和之前看到的文本几乎一样

00:01:54.005 --> 00:01:56.205
但是将单词拆分开来并构成列表

00:01:56.205 --> 00:01:59.190
在这里输出关于数据的一些统计信息

00:01:59.190 --> 00:02:03.260
包括 text 的长度 即字词数量

00:02:03.260 --> 00:02:05.460
并输出唯一字词的数量

00:02:05.460 --> 00:02:07.215
要获取唯一字词的数量

00:02:07.215 --> 00:02:09.815
我将使用内置的 Python 数据类型“集合”

00:02:09.815 --> 00:02:11.830
在上节课我们提到

00:02:11.830 --> 00:02:13.580
集合能够删除所有重复的单词

00:02:13.580 --> 00:02:16.405
所以获得这段文本中的唯一字词集合

00:02:16.405 --> 00:02:20.485
这段文本包含 1600 万以上的字词

00:02:20.485 --> 00:02:22.990
并且有 6 万以上的唯一字词

00:02:22.990 --> 00:02:26.350
在后续处理数据时 会用到这些数字

00:02:26.350 --> 00:02:29.030
接下来创建两个字典

00:02:29.030 --> 00:02:32.130
一个将字词转换为整数 另一个将整数转换为字词

00:02:32.130 --> 00:02:34.525
这是常见的标记化步骤

00:02:34.525 --> 00:02:39.675
在 utils.py 文件中使用函数 create_lookup_tables 完成这一步骤

00:02:39.675 --> 00:02:41.950
我们看看这个函数的作用

00:02:41.950 --> 00:02:45.530
此函数的输入参数是一个文本字词列表并返回两个字典

00:02:45.530 --> 00:02:49.750
这两个字典将词汇表映射到整数值并将整数值映射到词汇表

00:02:49.750 --> 00:02:52.675
这个计数器比较有意思

00:02:52.675 --> 00:02:55.425
首先创建一个排过序的词汇表

00:02:55.425 --> 00:02:57.890
这是一个按字词频率升序排列的字词列表

00:02:57.890 --> 00:03:00.720
排序依据是计数器返回的字词数量

00:03:00.720 --> 00:03:04.205
然后按照频率降序顺序分配整数

00:03:04.205 --> 00:03:08.035
所以最常见的字词 B 对应的整数是 0

00:03:08.035 --> 00:03:11.100
第二常见的字词对应 1 以此类推

00:03:11.100 --> 00:03:14.314
所以这个函数会返回两个字典

00:03:14.314 --> 00:03:16.970
获得这两个字典后

00:03:16.970 --> 00:03:19.560
将字词转换为整数并存储在 int_words 列表中

00:03:19.560 --> 00:03:23.955
我将输出前 30 个标记化的字词 看看是否合理

00:03:23.955 --> 00:03:27.650
看看这些值 再看看上面的字词列表

00:03:27.650 --> 00:03:30.210
会发现“the”和“of”

00:03:30.210 --> 00:03:32.755
是字典中最常见的几个字词

00:03:32.755 --> 00:03:35.985
“the”对应的整数是 0

00:03:35.985 --> 00:03:40.615
“of”是第二大常见字词 对应的整数是 1

00:03:40.615 --> 00:03:44.085
词汇表中有 60,000 个以上的字词

00:03:44.085 --> 00:03:48.115
所以所有这些标记值应该是 0 - 60,000 范围的整数值

00:03:48.115 --> 00:03:50.710
我们的目标是实现 word2vec

00:03:50.710 --> 00:03:54.370
word2vec 会查看目标字词的上下文

00:03:54.370 --> 00:03:57.310
我们必须好好地定义上下文

00:03:57.310 --> 00:04:02.105
并查看目标字词周围最相关的字词

00:04:02.105 --> 00:04:04.370
某些字词几乎永远不会相关

00:04:04.370 --> 00:04:06.730
因为它们太常见了

00:04:06.730 --> 00:04:11.170
这些字词出现频率非常高 例如“the”、“of”、“for”

00:04:11.170 --> 00:04:14.510
它们不能提供上下文信息

00:04:14.510 --> 00:04:17.090
如果丢弃这些常见的字词

00:04:17.090 --> 00:04:19.850
就能消除数据中的噪点

00:04:19.850 --> 00:04:22.850
并且提高训练速度和改善向量表示法的质量

00:04:22.850 --> 00:04:25.115
这个流程称为二次抽样

00:04:25.115 --> 00:04:26.965
你的第一个任务是二次抽样数据

00:04:26.965 --> 00:04:28.915
简单说下二次抽样的原理

00:04:28.915 --> 00:04:32.010
对于训练集中的所有 wi

00:04:32.010 --> 00:04:35.800
我们将根据公式算出其丢弃概率 然后做出丢弃的决定

00:04:35.800 --> 00:04:39.390
这个公式为

00:04:39.390 --> 00:04:43.175
1 减去 t 除以字词频率的平方根

00:04:43.175 --> 00:04:45.575
t 为我们设置的阈值

00:04:45.575 --> 00:04:49.370
例如 我们要丢弃索引为 0 的字词“the”

00:04:49.370 --> 00:04:54.200
假设它在有 1600 万个字词的数据集中出现了 100 万次

00:04:54.200 --> 00:04:56.780
这些是估算值

00:04:56.780 --> 00:04:59.480
频率为 100 万除以 1600 万

00:04:59.480 --> 00:05:01.670
分子 1*10^(-5)

00:05:01.670 --> 00:05:03.800
是我设定的阈值

00:05:03.800 --> 00:05:06.590
将这些值代入公式里

00:05:06.590 --> 00:05:11.475
得出字词丢弃概率为 98.7%

00:05:11.475 --> 00:05:15.310
即使丢弃掉大部分“the”

00:05:15.310 --> 00:05:19.740
依然会保留超过 12,000 个“the”

00:05:19.740 --> 00:05:22.750
二次抽样的作用是

00:05:22.750 --> 00:05:25.990
丢弃大量这种常见词

00:05:25.990 --> 00:05:28.330
避免它们影响到其他字词的上下文信息

00:05:28.330 --> 00:05:32.330
同时保留足够的样本并获得这些常见词的词嵌入

00:05:32.330 --> 00:05:35.710
从二次抽样的公式中可以看出

00:05:35.710 --> 00:05:39.385
某个字词的频率越高 丢弃该字词的概率就越高

00:05:39.385 --> 00:05:41.090
我提供了一些代码

00:05:41.090 --> 00:05:42.385
设定了阈值

00:05:42.385 --> 00:05:44.245
并创建了 word_counts 字典

00:05:44.245 --> 00:05:47.800
该字典使用计数器集合并获取编码字词列表

00:05:47.800 --> 00:05:50.590
返回这些字词在列表中的出现次数

00:05:50.590 --> 00:05:53.555
我输出了该列表中的第一个键值对

00:05:53.555 --> 00:05:59.530
结果显示字词标记 5233 在文本中出现了 303 次

00:05:59.530 --> 00:06:02.000
请根据这一信息计算

00:06:02.000 --> 00:06:05.365
词汇表中每个字词的丢弃率

00:06:05.365 --> 00:06:09.050
然后创建新的数据集 train_words

00:06:09.050 --> 00:06:10.890
该数据集基本上和原始字词列表一样

00:06:10.890 --> 00:06:14.510
但是丢弃了一些最常见的字词

00:06:14.510 --> 00:06:16.955
这道练习不算深度学习练习

00:06:16.955 --> 00:06:20.630
其实是编程挑战 准备数据是很重要的技能

00:06:20.630 --> 00:06:22.285
试试这道练习吧

00:06:22.285 --> 00:06:24.330
接下来我会讲解我的实现方式

