WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.390
Now that we've taken the time to preprocess and batch our data,

00:00:03.390 --> 00:00:05.665
it's time to actually start building the network.

00:00:05.665 --> 00:00:08.970
Here, we can see the general structure of the network that we're going to build.

00:00:08.970 --> 00:00:10.169
So, we have our inputs,

00:00:10.169 --> 00:00:12.910
which are going to be like batches of our train word tokens,

00:00:12.910 --> 00:00:15.089
and as we saw when we loaded in a batch,

00:00:15.089 --> 00:00:18.670
a lot of these values will actually be repeated in this input vector.

00:00:18.670 --> 00:00:21.670
So, we're going to be parsing in a long list of integers,

00:00:21.670 --> 00:00:24.710
which are going into this hidden layer, our embedding layer.

00:00:24.710 --> 00:00:27.289
The embedding layer, is responsible for looking at

00:00:27.289 --> 00:00:30.640
these input integers and basically creating a lookup table.

00:00:30.640 --> 00:00:32.939
So, for each possible integer value,

00:00:32.939 --> 00:00:35.719
there will be a row in our embedding weight matrix,

00:00:35.719 --> 00:00:39.034
and the width of the matrix will be the embedding dimension that we define.

00:00:39.034 --> 00:00:42.649
That dimension will be the size of the embedding layers outputs.

00:00:42.649 --> 00:00:47.810
Then these embeddings are fed into a final fully connected softmax output layer.

00:00:47.810 --> 00:00:49.825
Remember, that in the skip gram model,

00:00:49.825 --> 00:00:52.310
we're parsing in some input words and we're training

00:00:52.310 --> 00:00:54.800
this whole model to generate target context words.

00:00:54.799 --> 00:00:57.109
So, for one input value,

00:00:57.109 --> 00:01:02.049
the targets will be randomly selected context words from a window around the input word.

00:01:02.049 --> 00:01:05.119
Our output layer, is going to output the probability that

00:01:05.120 --> 00:01:08.564
a randomly selected context word is going to be the word the,

00:01:08.564 --> 00:01:09.829
or of, or nine,

00:01:09.829 --> 00:01:12.265
or any other word in our vocabulary.

00:01:12.265 --> 00:01:13.849
We're going to be trying to predict

00:01:13.849 --> 00:01:17.674
our target context words using the outputs of the softmax layer.

00:01:17.674 --> 00:01:22.459
Basically, looking at the words with the highest probability that they are context words.

00:01:22.459 --> 00:01:24.679
Then when we train everything, what's going to happen,

00:01:24.680 --> 00:01:26.300
is that our hidden layer is going to form

00:01:26.299 --> 00:01:28.640
these vector representations of the input words.

00:01:28.640 --> 00:01:33.219
So, each row in the embedding look-up table will be a vector representation for a word.

00:01:33.219 --> 00:01:36.789
Row zero, will be the embedding for the word the, for example.

00:01:36.790 --> 00:01:39.140
These vectors contain some semantic meaning,

00:01:39.140 --> 00:01:41.120
and that's what we're really interested in.

00:01:41.120 --> 00:01:43.475
We only really care about these embeddings.

00:01:43.474 --> 00:01:46.039
From these embeddings, we can do some interesting things.

00:01:46.040 --> 00:01:49.370
Performing vector math to see which of our words are most similar,

00:01:49.370 --> 00:01:51.290
or we can use these embeddings as input to

00:01:51.290 --> 00:01:54.045
another model that works with the same text input data.

00:01:54.045 --> 00:01:55.335
So, when we're done training,

00:01:55.334 --> 00:01:58.250
we can actually just get rid of this last softmax layer,

00:01:58.250 --> 00:02:00.439
because it's just there to help us train this model and

00:02:00.439 --> 00:02:02.939
create correct embeddings in the first place.

00:02:02.939 --> 00:02:04.704
Okay. So, right before we define the model,

00:02:04.704 --> 00:02:06.349
I have a function that will help us see

00:02:06.349 --> 00:02:09.064
what kind of word relationships this model is learning.

00:02:09.064 --> 00:02:11.569
When I introduced the idea of word2vec,

00:02:11.569 --> 00:02:13.729
I mentioned that representing words as vectors,

00:02:13.729 --> 00:02:17.674
gives us the ability to mathematically operate on these words in vector space.

00:02:17.675 --> 00:02:19.810
To see which words are similar,

00:02:19.810 --> 00:02:23.935
I'm going to calculate how similar vectors are using cosine similarity.

00:02:23.935 --> 00:02:28.585
Cosine similarity, looks at two vectors a and b and the angle between them,

00:02:28.585 --> 00:02:30.170
theta. It says, "Okay.

00:02:30.169 --> 00:02:34.849
The similarity between these two vectors is just the cosine of the angle between them."

00:02:34.849 --> 00:02:36.719
If you're familiar with vector math,

00:02:36.719 --> 00:02:40.500
that can also be calculated as the normalized dot product of a and b.

00:02:40.500 --> 00:02:42.645
You can really just think of it like this.

00:02:42.645 --> 00:02:43.969
When theta is zero,

00:02:43.969 --> 00:02:46.050
cosine of theta is equal to one.

00:02:46.050 --> 00:02:48.820
This is the maximum value that cosine can take.

00:02:48.819 --> 00:02:53.694
When theta is 90 degrees or rather these vectors are orthogonal to one another,

00:02:53.694 --> 00:02:55.620
then the cosine is going to be zero.

00:02:55.620 --> 00:02:59.349
So, the similarity really ends up being a value between zero and one,

00:02:59.349 --> 00:03:02.439
that indicates how similar two vectors are in vector space.

00:03:02.439 --> 00:03:05.319
So, let's look at this cosine similarity function.

00:03:05.319 --> 00:03:07.805
This function takes in an embedding layer,

00:03:07.805 --> 00:03:10.080
a validation size, and a validation window.

00:03:10.080 --> 00:03:13.330
In here, I'm getting the embeddings from the pasting layer.

00:03:13.330 --> 00:03:14.844
These are just the layer weights.

00:03:14.844 --> 00:03:19.354
Then, I'm doing some math and storing the magnitudes of these embedding vectors.

00:03:19.354 --> 00:03:21.229
That magnitude is just going to be

00:03:21.229 --> 00:03:24.399
the square root of the sum of the embedded vectors squared.

00:03:24.400 --> 00:03:29.510
Then, I'm randomly selecting some common and uncommon validation word examples.

00:03:29.509 --> 00:03:31.909
These are just integers in a range, in this case,

00:03:31.909 --> 00:03:33.979
from zero to 1,000 for common words,

00:03:33.979 --> 00:03:36.694
and for a higher range for uncommon words.

00:03:36.694 --> 00:03:40.344
Recall that lower indices indicate that a word appears more frequently.

00:03:40.344 --> 00:03:44.659
So, I'm generating half of our validation examples from a more common range,

00:03:44.659 --> 00:03:46.780
and half from a more uncommon range.

00:03:46.780 --> 00:03:51.189
These are collected in an np array and then converted into a long tensor type.

00:03:51.189 --> 00:03:55.104
Then I'm passing these validation examples into the embedding layer.

00:03:55.104 --> 00:03:58.274
In return, I get their vector representations back.

00:03:58.275 --> 00:04:01.849
So, these validation words are encoded as our vectors, a,

00:04:01.849 --> 00:04:04.215
and we're going to calculate the similarity between a,

00:04:04.215 --> 00:04:05.379
and each word vector,

00:04:05.379 --> 00:04:07.365
b, in the embedding table.

00:04:07.365 --> 00:04:12.000
We mentioned that the similarity is a dot product of a and b over the magnitude.

00:04:12.000 --> 00:04:15.080
This dot product is just a matrix multiplication between

00:04:15.080 --> 00:04:19.095
the validation vectors a and the transpose of the embedded vectors b.

00:04:19.095 --> 00:04:21.410
Here, I'm dividing by the magnitude,

00:04:21.410 --> 00:04:23.120
and this is not the exact equation here,

00:04:23.120 --> 00:04:25.629
but it will give us valid values for similarities,

00:04:25.629 --> 00:04:27.389
just scaled by a constant.

00:04:27.389 --> 00:04:31.224
This function returns the validation examples and similarities.

00:04:31.225 --> 00:04:34.820
This gives us all we need to later print out the validation words

00:04:34.819 --> 00:04:38.439
and the words in our embedding table that are semantically similar to those words.

00:04:38.439 --> 00:04:41.134
It's going to be a nice way to check that our embedding table

00:04:41.134 --> 00:04:44.425
is grouping together words with similar semantic meanings.

00:04:44.425 --> 00:04:46.335
So, this is a given function,

00:04:46.334 --> 00:04:48.154
you don't have to change anything about this.

00:04:48.154 --> 00:04:50.029
Now, on to defining the model.

00:04:50.029 --> 00:04:52.489
So, we know our model accepts some inputs,

00:04:52.490 --> 00:04:53.875
then it has an embedding layer,

00:04:53.875 --> 00:04:56.045
and a final softmax output layer.

00:04:56.045 --> 00:04:59.009
You'll have to define this using PyTorch's embedding layer,

00:04:59.009 --> 00:05:00.425
which you can read about here.

00:05:00.425 --> 00:05:01.965
Here's the documentation.

00:05:01.964 --> 00:05:04.724
So, the embedding layer is known as a sparse layer type.

00:05:04.725 --> 00:05:07.430
It takes in a number of input embeddings,

00:05:07.430 --> 00:05:09.019
which is going to be the number of rows in

00:05:09.019 --> 00:05:12.359
your embedding weight lookup matrix and an embedding dimension.

00:05:12.360 --> 00:05:15.199
This is the size of each embedding vector.

00:05:15.199 --> 00:05:17.884
The number of columns in your embedding look-up table.

00:05:17.884 --> 00:05:21.084
These two are the most important inputs when defining this layer.

00:05:21.084 --> 00:05:22.794
So, after the embedding layer,

00:05:22.795 --> 00:05:24.680
you'll define a linear layer to go from

00:05:24.680 --> 00:05:27.790
our embedding size to our predicted context words.

00:05:27.790 --> 00:05:30.770
You'll also have to apply a softmax function to the output,

00:05:30.769 --> 00:05:33.414
so that this model returns word probabilities.

00:05:33.415 --> 00:05:35.810
So, here's the skeleton code for this model,

00:05:35.810 --> 00:05:37.199
and when we instantiate this model,

00:05:37.199 --> 00:05:40.490
we're going to be parsing in input values for n_vocab,

00:05:40.490 --> 00:05:41.975
the size of our vocabulary,

00:05:41.975 --> 00:05:44.730
and n_embed, our embedding dimension.

00:05:44.730 --> 00:05:48.915
So, you should be able to complete the init and forward functions for this model.

00:05:48.915 --> 00:05:51.050
When you do that, you should be able to proceed

00:05:51.050 --> 00:05:53.855
with training using the provided training loops below.

00:05:53.855 --> 00:05:56.355
I'd really recommend training on GPU.

00:05:56.355 --> 00:06:00.035
Training this particular model takes quite a while even on GPU.

00:06:00.035 --> 00:06:04.020
So, I'd start training with maybe just one or two epics for now. All right.

00:06:04.019 --> 00:06:05.504
So, I'll leave this as an exercise,

00:06:05.504 --> 00:06:09.930
and next I'll go over one solution for defining a skip-gram model and training it.

