WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.190
我们希望模型结构是这样的

00:00:02.190 --> 00:00:05.820
首先接受输入 然后将输入传入嵌入层

00:00:05.820 --> 00:00:10.530
嵌入层生成嵌入向量并将这些向量传递给最终 softmax 输出层

00:00:10.530 --> 00:00:12.240
这是模型定义部分

00:00:12.240 --> 00:00:14.325
可以看出是一个很简单的模型

00:00:14.325 --> 00:00:17.605
首先使用 self.embed 定义嵌入层

00:00:17.605 --> 00:00:19.965
这个参数表示词汇表的长度

00:00:19.965 --> 00:00:22.350
它将创建一个嵌入权重矩阵

00:00:22.350 --> 00:00:24.975
词汇表中的每个字词对应一行

00:00:24.975 --> 00:00:29.025
这个参数表示输出大小为 n_embed（即嵌入维度）的向量

00:00:29.025 --> 00:00:33.660
接着是一个全连接层 它将嵌入维度作为输入

00:00:33.660 --> 00:00:36.915
输出大小也是词汇表的长度

00:00:36.915 --> 00:00:40.235
这是因为这个输出是一系列字词类别分数

00:00:40.235 --> 00:00:43.525
表示给定输入字词的潜在上下文字词

00:00:43.525 --> 00:00:46.830
然后在这里定义了 softmax 激活层

00:00:46.830 --> 00:00:49.320
也可以在 forward 函数中定义该层级

00:00:49.320 --> 00:00:50.700
这只是其中一种实现方式

00:00:50.700 --> 00:00:52.055
然后我在 forward 函数中

00:00:52.055 --> 00:00:54.895
将输入 x 传入嵌入层

00:00:54.895 --> 00:00:58.100
返回嵌入 这些嵌入将进入全连接层

00:00:58.100 --> 00:01:00.080
并返回一系列类别分数

00:01:00.080 --> 00:01:02.570
最后应用 softmax 激活函数

00:01:02.570 --> 00:01:06.170
返回上下文字词的对数概率

00:01:06.170 --> 00:01:07.865
在下面的训练部分

00:01:07.865 --> 00:01:09.880
实例化该模型

00:01:09.880 --> 00:01:13.970
在这里将嵌入维度设为 300

00:01:13.970 --> 00:01:16.895
你也可以尝试更小或更大的值

00:01:16.895 --> 00:01:18.860
嵌入维度可以看做

00:01:18.860 --> 00:01:20.985
我们能够检测的字词特征数量

00:01:20.985 --> 00:01:23.285
例如长度 字词类型等

00:01:23.285 --> 00:01:27.620
这里的参数是词汇表的整个长度以及嵌入维度

00:01:27.620 --> 00:01:30.070
移到 GPU 上训练

00:01:30.070 --> 00:01:32.730
在这里使用了负对数似然损失

00:01:32.730 --> 00:01:35.840
因为 softmax 与负对数似然损失相结合

00:01:35.840 --> 00:01:38.960
等效于交叉熵损失

00:01:38.960 --> 00:01:43.200
该损失可以查看上下文字词的概率

00:01:43.200 --> 00:01:45.110
我使用了 Adam 优化器 这只是我的选择

00:01:45.110 --> 00:01:49.080
传入模型参数和学习速率

00:01:49.080 --> 00:01:52.770
下面是训练循环 我决定训练 5 个周期

00:01:52.770 --> 00:01:55.790
训练流程在 GPU 上也花费了几个小时

00:01:55.790 --> 00:01:57.830
建议你训练更短的时间

00:01:57.830 --> 00:02:01.055
或用后面课程里的更高效的训练方式

00:02:01.055 --> 00:02:03.170
在训练循环里通过调用

00:02:03.170 --> 00:02:05.150
在上方定义的生成器函数获取批次数据

00:02:05.150 --> 00:02:09.345
传入训练字词列表和批次大小

00:02:09.345 --> 00:02:12.340
获取输入和目标上下文字词

00:02:12.340 --> 00:02:14.905
然后将它们转换为 LongTensor 类型

00:02:14.905 --> 00:02:17.345
如果有 GPU 则将这些移到 GPU 上

00:02:17.345 --> 00:02:19.775
然后进行反向传播

00:02:19.775 --> 00:02:22.205
将 inputs 传入 skip-gram 模型中

00:02:22.205 --> 00:02:24.520
获取上下文字词的对数概率

00:02:24.520 --> 00:02:28.720
然后对这些上下文字词和 targets 应用损失函数

00:02:28.720 --> 00:02:32.280
进行反向传播并更新模型权重

00:02:32.280 --> 00:02:36.090
在这两步之前记得清零累积的梯度

00:02:36.090 --> 00:02:40.865
然后使用 cosine_similarity 函数输出验证样本

00:02:40.865 --> 00:02:43.730
在这里传入模型和 GPU 设备

00:02:43.730 --> 00:02:46.965
获得验证样本及其相似性

00:02:46.965 --> 00:02:49.490
在这里使用 topk 抽样

00:02:49.490 --> 00:02:52.985
获取与给定样本最相似的 6 个字词

00:02:52.985 --> 00:02:56.240
在这里遍历验证样本

00:02:56.240 --> 00:02:58.580
输出第一个验证字词

00:02:58.580 --> 00:03:01.545
并在竖线后面输出与该字词最相似的 5 个字词

00:03:01.545 --> 00:03:03.810
这些是初始结果

00:03:03.810 --> 00:03:07.285
在训练 5 个周期后输出了很多数据

00:03:07.285 --> 00:03:10.490
一开始这些字词之间的关系很随机

00:03:10.490 --> 00:03:12.010
例如 and | returns

00:03:12.010 --> 00:03:13.740
liverpudlians 等

00:03:13.740 --> 00:03:15.740
训练一段时间后

00:03:15.740 --> 00:03:18.480
这些验证字词越来越相似

00:03:18.480 --> 00:03:21.440
下拉到训练的结束部分

00:03:21.440 --> 00:03:24.130
可以看到相似字词组合到了一起

00:03:24.130 --> 00:03:27.140
这些是数字字词

00:03:27.140 --> 00:03:30.220
这些是动物和哺乳动物术语

00:03:30.220 --> 00:03:32.989
有些组合与政府和政治有关

00:03:32.989 --> 00:03:35.930
还有些与地点和语言有关

00:03:35.930 --> 00:03:38.390
看来我的 word2vec 模型取得了学习效果

00:03:38.390 --> 00:03:41.480
还可以用另一种方式可视化这些嵌入

00:03:41.480 --> 00:03:45.245
另一个强大的可视化工具是 t-SNE

00:03:45.245 --> 00:03:48.994
t-SNE 是 t-分布随机近邻嵌入的简称

00:03:48.994 --> 00:03:52.910
它是一种非线性降维技巧

00:03:52.910 --> 00:03:57.615
旨在将相似数据组合到一起并将不同的数据分开

00:03:57.615 --> 00:04:01.755
我将从 sklearn 库中加载该算法

00:04:01.755 --> 00:04:04.775
设定要可视化的嵌入数量

00:04:04.775 --> 00:04:07.190
并从嵌入层的权重中获取这些嵌入

00:04:07.190 --> 00:04:09.980
我将通过名称从模型中调用嵌入层

00:04:09.980 --> 00:04:13.095
嵌入层的名称为 embed

00:04:13.095 --> 00:04:16.360
我可以使用 model.embed.weight 获取权重

00:04:16.360 --> 00:04:20.300
在这里对 600 个嵌入应用 t-SNE

00:04:20.300 --> 00:04:23.740
t-SNE 聚类最终看起来是这样的

00:04:23.740 --> 00:04:26.610
可以看出 相似的字词组合到了一起

00:04:26.610 --> 00:04:29.425
例如 east、west、north 和 south

00:04:29.425 --> 00:04:32.735
在右边可以看到一些音乐术语

00:04:32.735 --> 00:04:34.795
rock、music、album、band 和 song

00:04:34.795 --> 00:04:37.475
下面是一些宗教术语

00:04:37.475 --> 00:04:39.390
这里是一些颜色术语

00:04:39.390 --> 00:04:42.700
这里是一些关于学校的术语：school、university 和 college

00:04:42.700 --> 00:04:44.170
在左边

00:04:44.170 --> 00:04:48.950
有一些月份术语 这里是一些整数值

00:04:48.950 --> 00:04:52.790
这些聚类表明我的 word2vec 模型成功了

00:04:52.790 --> 00:04:56.330
它学会生成包含语义信息的嵌入

00:04:56.330 --> 00:05:00.720
并且我们可以通过这种方式直观地可视化字词之间的空间关系

00:05:00.720 --> 00:05:04.375
不过这个模型有个问题 它训练时间很长

00:05:04.375 --> 00:05:06.730
接下来我们将解决这一问题

