WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.399
“词条 (token)” 是“符号”的高级表达

00:00:03.399 --> 00:00:08.375
一般指具有某种意义 无法再分拆的符号

00:00:08.375 --> 00:00:10.865
在自然语言处理中

00:00:10.865 --> 00:00:13.919
词条通常是单独的词

00:00:13.919 --> 00:00:19.634
因此 分词 (tokenization) 就是将每个句子分拆成一系列词

00:00:19.635 --> 00:00:26.025
最简单的方法是使用 split 方法 返回词列表

00:00:26.024 --> 00:00:29.584
请注意 默认情况下在空格字符处分拆

00:00:29.585 --> 00:00:32.840
包括普通空格 标签

00:00:32.840 --> 00:00:34.580
新行等

00:00:34.579 --> 00:00:39.079
这种方法还很智能 能够忽略一个序列中的两个或多个连续空格字符

00:00:39.079 --> 00:00:41.375
因此不会返回空字符串

00:00:41.375 --> 00:00:45.469
但是 你可以使用可选参数对它进行控制

00:00:45.469 --> 00:00:49.414
目前为止 我们只使用了 Python 的内置功能

00:00:49.414 --> 00:00:54.655
但是使用诸如NLTK库 这样的一些操作就会简单许多

00:00:54.655 --> 00:00:58.490
NLTK 也就是“自然语言工具包”的意思

00:00:58.490 --> 00:01:02.359
在 NLTK 中分拆文本的最常见方法是

00:01:02.359 --> 00:01:07.189
使用 nltk.tokenize 中的 word tokenize 函数

00:01:07.189 --> 00:01:11.679
这与 split 的功能相同 但稍微智能一些

00:01:11.680 --> 00:01:15.360
在尝试传入未标准化的原始文本时

00:01:15.359 --> 00:01:19.909
你会发现 根据标点符号位置的不同 对它们的处理也不同

00:01:19.909 --> 00:01:23.509
头衔 Dr 后面的点号

00:01:23.510 --> 00:01:27.645
与 Dr 保留在一起 作为一个词条

00:01:27.644 --> 00:01:28.849
可想而知

00:01:28.849 --> 00:01:35.269
NLTK 使用某种规则或模式决定如何处理每个标点符号

00:01:35.269 --> 00:01:38.625
比如 如果你想翻译文本

00:01:38.625 --> 00:01:40.909
可能需要将文本分拆成句子

00:01:40.909 --> 00:01:45.289
可以通过 NLTK 使用 sent tokenize 来实现这一点

00:01:45.290 --> 00:01:49.045
然后可以根据需要将每个句子分拆成词

00:01:49.045 --> 00:01:52.040
NLTK 提供多种分词器

00:01:52.040 --> 00:01:56.030
包括基于正则表达式的分词器

00:01:56.030 --> 00:02:00.155
可以只需一步 即可清除标点符号并将其分词

00:02:00.155 --> 00:02:04.500
还包括推文分词器 它能识别 twitter 句柄

00:02:04.500 --> 00:02:07.120
话题标签和表情符号

00:02:07.120 --> 00:02:11.000
查看 nltk.tokenize 包 了解更多详细信息

