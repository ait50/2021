WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.524
Correct. The first way to split them does not help us at all.

00:00:03.524 --> 00:00:05.370
We ended up with very similar sets of

00:00:05.370 --> 00:00:08.085
red and blue points and we learn nothing about the data.

00:00:08.085 --> 00:00:09.915
The second way did a decent job.

00:00:09.914 --> 00:00:11.819
We managed to get most of the blue ones on

00:00:11.820 --> 00:00:14.669
one side and most of the red ones on the other side.

00:00:14.669 --> 00:00:16.980
So, now we know a bit more about it.

00:00:16.980 --> 00:00:18.690
And the third one did fantastic.

00:00:18.690 --> 00:00:20.490
It managed to send all the blue points on

00:00:20.489 --> 00:00:23.039
one side and all the red ones on the other side.

00:00:23.039 --> 00:00:24.914
Now we know a lot about our data.

00:00:24.914 --> 00:00:28.750
Actually, we'll learn how to calculate something called the information gain,

00:00:28.750 --> 00:00:31.300
and it will be zero for the first splitting,

00:00:31.300 --> 00:00:34.299
0.28 for the second, and one for the third one.

00:00:34.299 --> 00:00:36.820
And the formula for information gain is very simple.

00:00:36.820 --> 00:00:38.500
It's just the change in entropy.

00:00:38.500 --> 00:00:40.203
Let me be more specific.

00:00:40.203 --> 00:00:42.190
For every node in the Decision Tree,

00:00:42.189 --> 00:00:45.714
we can calculate the entropy of the data in the parent node.

00:00:45.715 --> 00:00:49.630
And then, we calculate the entropies of the two children.

00:00:49.630 --> 00:00:52.630
The information gain is a difference between

00:00:52.630 --> 00:00:56.890
the entropy of the parent and the average entropy of the children.

00:00:56.890 --> 00:00:58.420
So, in our first example,

00:00:58.420 --> 00:01:01.454
you can calculate the entropy of the parent and its one.

00:01:01.454 --> 00:01:04.504
The entropy of each of the children is 0.72.

00:01:04.504 --> 00:01:08.342
So the average of the entropy of the children is actually 0.72.

00:01:08.343 --> 00:01:14.060
Thus, the change in entropy is one minus 0.72, which is 0.28.

00:01:14.060 --> 00:01:18.125
For this example, now, we can calculate the entropies of the children which are both one.

00:01:18.125 --> 00:01:22.099
Thus, the change in entropy is one minus one which is zero.

00:01:22.099 --> 00:01:25.399
This is a terrible splitting because they gave us zero information.

00:01:25.400 --> 00:01:26.870
And finally, in this example,

00:01:26.870 --> 00:01:29.000
the entropy of the children are both zero,

00:01:29.000 --> 00:01:30.349
since as we saw,

00:01:30.349 --> 00:01:33.664
a set where the points are the same color has zero entropy.

00:01:33.665 --> 00:01:37.315
Thus, the information gain is one minus zero which is one.

00:01:37.314 --> 00:01:41.174
This split gave us the largest information gain and as you can see,

00:01:41.174 --> 00:01:43.784
it is the best split because it managed to

00:01:43.784 --> 00:01:47.054
perfectly cut the data into the blue points and red points.

00:01:47.055 --> 00:01:51.116
So, here's our summary. The three cuts with the values of information gain.

00:01:51.115 --> 00:01:52.694
If the tree had to choose,

00:01:52.694 --> 00:01:54.389
it would go for the third cut,

00:01:54.390 --> 00:01:58.200
which is the one that gives it the largest amount of information gain.

