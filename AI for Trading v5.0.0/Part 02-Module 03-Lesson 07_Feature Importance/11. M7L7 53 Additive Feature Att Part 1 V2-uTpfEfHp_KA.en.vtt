WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.040
Additive feature attribution.

00:00:02.040 --> 00:00:04.859
So, additive feature attribution methods are

00:00:04.860 --> 00:00:08.730
simple models that are used to explain complex models.

00:00:08.730 --> 00:00:13.785
So, you can see the formula that I pasted here on page three of the paper.

00:00:13.785 --> 00:00:16.199
So, if I went to the paper itself,

00:00:16.199 --> 00:00:19.349
let's say, we'll go to page three.

00:00:19.350 --> 00:00:25.170
So, I'm just showing this formula right here inside the Jupyter Notebook.

00:00:25.170 --> 00:00:27.225
It looks a little bit intimidating,

00:00:27.225 --> 00:00:28.500
but it's more or less.

00:00:28.500 --> 00:00:31.335
You can think of it as you're summing things together.

00:00:31.335 --> 00:00:36.134
You're simply adding a couple of things together and you're getting a result.

00:00:36.134 --> 00:00:39.329
This is how I'm trying to explain what this formula means.

00:00:39.329 --> 00:00:43.189
So, think of our tree model as the complex model

00:00:43.189 --> 00:00:47.155
that we wish to explain with a simple linear model.

00:00:47.155 --> 00:00:51.484
So, notice this is a linear model because it's just simply adding things up.

00:00:51.484 --> 00:00:54.589
So, the above formula is saying that we can

00:00:54.590 --> 00:00:57.590
take a single data point with the three features.

00:00:57.590 --> 00:01:00.440
So, the three features from our example in

00:01:00.439 --> 00:01:03.964
this Notebook and a complex model makes a prediction.

00:01:03.965 --> 00:01:08.090
We can divide up that prediction among the three features based on

00:01:08.090 --> 00:01:12.135
how important those features are to the complex model's prediction,

00:01:12.135 --> 00:01:15.800
and also based on whether the feature value pushes

00:01:15.799 --> 00:01:19.909
the prediction in the positive or negative direction.

00:01:19.909 --> 00:01:22.549
So, we'll have an example of that might make this more concrete,

00:01:22.549 --> 00:01:26.929
but you can think of all these features as being stacked up

00:01:26.930 --> 00:01:31.745
together like a seven-layer data or like a lasagna or like a tiramisu cake.

00:01:31.745 --> 00:01:36.905
Can you tell them hungry? So, this is related to the idea of coalition game theory.

00:01:36.905 --> 00:01:41.810
Because imagine a team of basketball players score is 100 points in a game.

00:01:41.810 --> 00:01:45.560
We're trying to attribute part of the final score to each member

00:01:45.560 --> 00:01:49.340
of the team based on their contributions or importance.

00:01:49.340 --> 00:01:52.650
So, one point to note is most of the time you may think,

00:01:52.650 --> 00:01:56.344
so each players scored a certain number of positive points.

00:01:56.344 --> 00:01:58.640
You can imagine if a player made

00:01:58.640 --> 00:02:03.530
some mistakes maybe you could attribute negative points to them as well.

00:02:03.530 --> 00:02:10.030
All of the contributions of all these players adds up to those 100 points for the game.

00:02:10.030 --> 00:02:12.650
So, when the contribution of each feature are added

00:02:12.650 --> 00:02:15.395
up to equal the complex model's prediction,

00:02:15.395 --> 00:02:18.560
the linear combination or contributions is

00:02:18.560 --> 00:02:23.710
the simple linear model that is being used to explain the complex model.

00:02:23.710 --> 00:02:28.925
So, this is the idea of machine-learning interpretability to explain

00:02:28.925 --> 00:02:35.900
a complex non-linear model using a simple explainable linear model.

