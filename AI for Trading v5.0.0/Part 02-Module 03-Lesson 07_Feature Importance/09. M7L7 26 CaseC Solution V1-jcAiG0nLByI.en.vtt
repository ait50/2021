WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.099
Okay. So now let's go over the solution for case C. So in this case,

00:00:05.099 --> 00:00:07.129
we want to use all three features.

00:00:07.129 --> 00:00:11.490
So we use our function get_subset passing in X,

00:00:11.490 --> 00:00:13.949
and we want to choose feature zero,

00:00:13.949 --> 00:00:15.884
feature one, and feature two.

00:00:15.884 --> 00:00:19.329
Here we want to instantiate a decision tree regressor.

00:00:19.329 --> 00:00:23.984
So sklearn.tree.DecisionTreeRegressor.

00:00:23.984 --> 00:00:28.859
Then we want to fit it to the data S_union_i,

00:00:28.859 --> 00:00:31.484
and the labels are in y.

00:00:31.484 --> 00:00:38.174
So the sample input for all three features is going to be one for all three features,

00:00:38.174 --> 00:00:39.719
and remember we need to reshape

00:00:39.719 --> 00:00:44.070
that one-dimensional array to be two-dimensional with one row,

00:00:44.070 --> 00:00:47.490
and in this case it's going to be three columns.

00:00:48.289 --> 00:00:58.320
We're going to make a prediction using this model.predict on this sample input.

00:00:58.450 --> 00:01:01.340
So as we expect,

00:01:01.340 --> 00:01:05.525
it has the correct prediction that when both feature zero and feature,

00:01:05.525 --> 00:01:09.185
our one, then the output is one.

00:01:09.185 --> 00:01:14.365
Now, let's calculate the model when it predicts what just features wanting two.

00:01:14.364 --> 00:01:20.489
So S is going to be get_subset X,

00:01:20.489 --> 00:01:24.449
and we're going to choose feature one and feature two.

00:01:25.180 --> 00:01:29.985
S is going to contain an essence of decision tree regressor.

00:01:29.984 --> 00:01:34.519
So sklearn.tree.DecisionTreeRegressor.

00:01:34.519 --> 00:01:38.049
Then we're going to fit it to the data that we give it,

00:01:38.049 --> 00:01:43.219
which is in S. Those are the features and Y are the labels.

00:01:43.219 --> 00:01:49.614
Okay. So the sample input for these two features going to be one and one,

00:01:49.614 --> 00:01:55.884
and then remember to reshape the one-dimensional array into two-dimensional with one row,

00:01:55.885 --> 00:01:57.855
and in this case,

00:01:57.855 --> 00:01:59.835
it's going to be two columns.

00:01:59.834 --> 00:02:08.560
So now, we're going to take the model and predict on the sample input,

00:02:08.560 --> 00:02:12.530
and it's 0.5 as we expect.

00:02:12.530 --> 00:02:17.849
So now, the difference is going to be the prediction of

00:02:17.849 --> 00:02:23.900
S_union_i minus the prediction when it just has the set of S features.

00:02:23.900 --> 00:02:28.310
So the difference is one minus 0.5,

00:02:28.310 --> 00:02:31.625
which is 0.5, and that's the marginal contribution.

00:02:31.625 --> 00:02:33.930
Now, calculate the weights.

00:02:33.930 --> 00:02:38.409
So that means calculate that expression that contains all those factorials.

00:02:38.409 --> 00:02:41.780
So size S is going to be S.shape.

00:02:41.830 --> 00:02:45.245
It's going to be index one of that,

00:02:45.245 --> 00:02:47.509
it's going to be the same as what we saw earlier.

00:02:47.509 --> 00:02:52.310
So calc.weights, it's going to take size S,

00:02:52.310 --> 00:02:54.530
then M is the total number of features.

00:02:54.530 --> 00:02:56.675
This should give us two divided by six,

00:02:56.675 --> 00:02:59.475
which is one-third, which is what we see here.

00:02:59.474 --> 00:03:02.810
Great. So now, one more case which is quite interesting,

00:03:02.810 --> 00:03:06.539
is case D. So please continue on with the next video.

