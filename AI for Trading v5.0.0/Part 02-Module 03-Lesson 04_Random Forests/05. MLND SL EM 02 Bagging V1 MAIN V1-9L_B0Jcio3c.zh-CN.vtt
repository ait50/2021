WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.465
我们开始介绍bagging （自助聚集） 吧

00:00:01.465 --> 00:00:04.514
这是一些红蓝数据点

00:00:04.514 --> 00:00:06.448
为了简化 

00:00:06.450 --> 00:00:09.259
假设我们的弱学习器用的是最简单的学习器

00:00:09.259 --> 00:00:11.650
单节点的决策树

00:00:11.650 --> 00:00:14.130
所以 这些弱学习器要么是水平线

00:00:14.130 --> 00:00:16.980
要么是垂直线 表示这一测都是正例

00:00:16.980 --> 00:00:18.750
而另一测都是负例

00:00:18.750 --> 00:00:21.359
回到我们的数据让我们做以下的事情

00:00:21.359 --> 00:00:23.318
因为数据量可能巨大

00:00:23.518 --> 00:00:27.409
通常 我们不想用整个数据集重复训练多个模型

00:00:27.410 --> 00:00:29.289
这样成本太高

00:00:29.489 --> 00:00:32.670
相反 我们只抽取它的子集

00:00:32.670 --> 00:00:36.000
然后用每一个子集训练弱学习器

00:00:36.000 --> 00:00:38.969
最后 我们再想办法把这些弱学习器组合起来

00:00:38.969 --> 00:00:41.149
这是我们的第一部分数据

00:00:41.149 --> 00:00:42.344
和第一个模型

00:00:42.344 --> 00:00:44.018
或第一个学习器

00:00:44.020 --> 00:00:46.100
记住这个

00:00:46.100 --> 00:00:50.039
这是第二部分数据和第二个学习器

00:00:50.039 --> 00:00:53.890
看看我选择的数据子集和学习器  看上去我是不是太幸运了

00:00:53.890 --> 00:00:55.820
一般来说 这些学习器可能很糟糕

00:00:55.820 --> 00:00:57.280
但如果数据足够大

00:00:57.280 --> 00:00:58.685
通过选择一个随机子集

00:00:58.685 --> 00:01:01.000
通常会给我们很好的数据直觉

00:01:01.000 --> 00:01:03.155
同时会加快处理过程

00:01:03.155 --> 00:01:06.549
最后是第三部分数据和第三个学习器

00:01:06.549 --> 00:01:09.049
注意 我没有对数据分区

00:01:09.049 --> 00:01:11.750
也就是说 我们允许重复采样 各数据子集会有重复的点

00:01:11.750 --> 00:01:15.515
甚至根本不考虑某些点

00:01:15.515 --> 00:01:19.159
在每一步 我们都选择一个完全随机的数据子集。

00:01:19.159 --> 00:01:21.405
现在我们有三个弱学习器

00:01:21.405 --> 00:01:24.305
如何组合它们？投票表决怎么样?

00:01:24.305 --> 00:01:27.375
我们在数据上实行这些弱学习器

00:01:27.375 --> 00:01:29.355
如果有两个或更多判断是蓝色

00:01:29.355 --> 00:01:32.893
就认为是蓝色  如果有两个或更多判断是红色  就认为是红色

00:01:33.093 --> 00:01:34.788
如果我们有偶数个模型

00:01:34.790 --> 00:01:37.025
我们可以选择任何希望的方式来打破联系

00:01:37.025 --> 00:01:39.450
尽管有很多点和模型

00:01:39.450 --> 00:01:41.340
但很难想象各个弱学习器之间会有依赖关系

00:01:41.340 --> 00:01:43.730
也许会发生吧

00:01:43.730 --> 00:01:46.185
这就是我们让弱学习器们投票的结果

00:01:46.185 --> 00:01:51.185
就这样这就是 bagging （自助聚集）算法

