WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.465
So let's start with bagging.

00:00:01.465 --> 00:00:04.514
Here's our data in the form of some red and blue points,

00:00:04.514 --> 00:00:06.449
and for simplicity, we'll say that

00:00:06.450 --> 00:00:09.259
our weak learners will be the simplest possible learner.

00:00:09.259 --> 00:00:11.650
A Decision Tree of one-node.

00:00:11.650 --> 00:00:14.130
So, all of them are either horizontal or a

00:00:14.130 --> 00:00:16.980
vertical line that says on this side everything is positive,

00:00:16.980 --> 00:00:18.750
and on this side everything is negative.

00:00:18.750 --> 00:00:21.359
So back to our data. Let's do the following.

00:00:21.359 --> 00:00:23.519
Since our data may be huge,

00:00:23.519 --> 00:00:27.410
in general, we don't want to train many models on the same data.

00:00:27.410 --> 00:00:29.490
This would be very expensive.

00:00:29.489 --> 00:00:32.670
Instead, we'll just take subsets of it and

00:00:32.670 --> 00:00:36.000
train a weak learner on each one of these subsets.

00:00:36.000 --> 00:00:38.969
Then we'll figure out how to combine these learners.

00:00:38.969 --> 00:00:41.149
So here's our first subset of data,

00:00:41.149 --> 00:00:42.344
and our first model,

00:00:42.344 --> 00:00:44.019
our first learner is this one.

00:00:44.020 --> 00:00:46.100
We'll remember this one.

00:00:46.100 --> 00:00:50.039
Now, here's our second subset of data and our second learner.

00:00:50.039 --> 00:00:53.890
If it looks like I'm being too lucky picking my data and my learners, yes I am.

00:00:53.890 --> 00:00:55.820
In general these learners can be terrible,

00:00:55.820 --> 00:00:57.280
but if our data is large enough,

00:00:57.280 --> 00:00:58.685
picking a random subset,

00:00:58.685 --> 00:01:01.000
normally gives us good intuition on the data.

00:01:01.000 --> 00:01:03.155
Plus, it makes the process run quickly.

00:01:03.155 --> 00:01:06.549
Finally our third subset of data on our third learner.

00:01:06.549 --> 00:01:09.049
Notice that I never partition the data.

00:01:09.049 --> 00:01:11.750
We are completely allowed to repeat points among

00:01:11.750 --> 00:01:15.515
our subsets and to even not consider some of the points at all.

00:01:15.515 --> 00:01:19.159
At every step, we pick a fully random subset of data.

00:01:19.159 --> 00:01:21.405
Now we have three weak learners.

00:01:21.405 --> 00:01:24.305
How do we combine them? Well, what about voting?

00:01:24.305 --> 00:01:27.375
We over impose them over the data and say,

00:01:27.375 --> 00:01:29.355
if two or more of them say blue,

00:01:29.355 --> 00:01:33.094
then blue, and if two or more of them say red, then red.

00:01:33.094 --> 00:01:34.789
If we have an even number of models,

00:01:34.790 --> 00:01:37.025
we can pick any way we want to break ties.

00:01:37.025 --> 00:01:39.450
Although with lots of points and lots of models,

00:01:39.450 --> 00:01:41.340
it's hard to imagine that would get a tie somewhere.

00:01:41.340 --> 00:01:43.730
It may happen though, and voila,

00:01:43.730 --> 00:01:46.185
this is what we obtain when we make the models vote.

00:01:46.185 --> 00:01:49.000
So that's it. That's the bagging algorithm.

