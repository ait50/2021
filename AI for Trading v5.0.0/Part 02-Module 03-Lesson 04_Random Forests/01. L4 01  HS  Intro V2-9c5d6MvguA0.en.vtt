WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.940
You've already learned about decision trees.

00:00:02.940 --> 00:00:07.560
Decision trees are very useful predictive models that have a number of advantages,

00:00:07.559 --> 00:00:11.399
including the facts that they are simple to understand and interpret,

00:00:11.400 --> 00:00:13.830
require little data preparation,

00:00:13.830 --> 00:00:17.789
can handle many datatypes and work well with large datasets.

00:00:17.789 --> 00:00:22.019
A number of properties of tree-based methods are particularly appealing for

00:00:22.019 --> 00:00:26.324
the types of analysis problems we encounter in financial trading contexts.

00:00:26.324 --> 00:00:28.454
These include the following,

00:00:28.454 --> 00:00:31.349
that there is no need to scale input data,

00:00:31.350 --> 00:00:34.035
that the methods are robust to outliers,

00:00:34.034 --> 00:00:36.179
missing values, and noisy data.

00:00:36.179 --> 00:00:38.850
That unlike some other algorithms,

00:00:38.850 --> 00:00:43.945
a higher degree of correlation between features doesn't affect predictions,

00:00:43.945 --> 00:00:46.564
but does affect feature importance.

00:00:46.564 --> 00:00:49.099
That the methods are hierarchical.

00:00:49.100 --> 00:00:54.140
That they can learn the interactions between factors on their own, for example,

00:00:54.140 --> 00:00:57.140
they can infer that a mean reversion factor may work

00:00:57.140 --> 00:01:01.429
better at a time when average trading volume is relatively high,

00:01:01.429 --> 00:01:03.619
and create a split accordingly.

00:01:03.619 --> 00:01:07.314
So, tree-based methods in general are great.

00:01:07.314 --> 00:01:11.314
However, the one type of tree-based method we've learned so far,

00:01:11.314 --> 00:01:14.420
decision trees, has a number of limitations.

00:01:14.420 --> 00:01:17.825
Decision trees are typically not competitive with

00:01:17.825 --> 00:01:21.984
other supervised learning methods in terms of prediction accuracy.

00:01:21.984 --> 00:01:25.159
They are also very prone to overfitting.

00:01:25.159 --> 00:01:30.379
Approaches that involve producing multiple trees and combining their predictions,

00:01:30.379 --> 00:01:34.158
often result in dramatic improvements in prediction accuracy,

00:01:34.159 --> 00:01:38.195
at the expense of some loss in interpretability.

00:01:38.194 --> 00:01:43.879
Combining many models together to yield a more powerful model is called ensembling.

00:01:43.879 --> 00:01:49.869
The randomization introduced via some ensemble methods helps reduce overfitting,

00:01:49.870 --> 00:01:55.225
which is among our core concerns when using machine learning in finance contexts.

00:01:55.224 --> 00:01:59.959
In this lesson, we're going to talk about some types of ensembling methods.

