WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.399
Now in order to go further with Decision Trees,

00:00:02.399 --> 00:00:05.415
we need to learn an important concept called entropy.

00:00:05.415 --> 00:00:08.400
Entropy comes from physics and to explain it,

00:00:08.400 --> 00:00:11.339
we'll use the example of the three states of water.

00:00:11.339 --> 00:00:13.544
These are solid, which is ice,

00:00:13.544 --> 00:00:16.769
liquid, and gas, which is water vapor.

00:00:16.769 --> 00:00:20.894
Let's think of the particles inside ice, water, and vapor.

00:00:20.894 --> 00:00:25.140
Ice is pretty rigid in that its particles don't have many places to go.

00:00:25.140 --> 00:00:26.984
They mostly stay where they are.

00:00:26.984 --> 00:00:31.214
Water is a little less rigid in which a particle has a few places to move around.

00:00:31.214 --> 00:00:33.689
And water vapor is in the other end of the spectrum.

00:00:33.689 --> 00:00:37.664
A particle has many possibilities of where to go and can move around a lot.

00:00:37.664 --> 00:00:40.094
So, entropy measures precisely this,

00:00:40.094 --> 00:00:43.710
how much freedom does a particle have to move around?

00:00:43.710 --> 00:00:46.245
Thus, the entropy of ice is low,

00:00:46.244 --> 00:00:48.539
the entropy of liquid water is medium,

00:00:48.539 --> 00:00:51.195
and the entropy of water vapor is high.

00:00:51.195 --> 00:00:53.765
The notion of entropy can also work in probability.

00:00:53.765 --> 00:00:57.465
Let's look at these three configurations of balls inside buckets.

00:00:57.465 --> 00:00:59.925
The first bucket has four red balls.

00:00:59.924 --> 00:01:02.872
The second one has three red and one blue,

00:01:02.872 --> 00:01:05.079
and the third one has two red and two blue.

00:01:05.079 --> 00:01:09.185
And let's say balls from each color are completely indistinguishable.

00:01:09.185 --> 00:01:11.719
So we could say that entropy is given by how much

00:01:11.719 --> 00:01:14.453
balls are allowed to move around if we put them in a line.

00:01:14.453 --> 00:01:17.059
We can see that the first bucket is very rigid.

00:01:17.060 --> 00:01:18.769
No matter how we organize the balls,

00:01:18.769 --> 00:01:21.049
we always get the same state,

00:01:21.049 --> 00:01:22.384
so it has low entropy.

00:01:22.385 --> 00:01:25.445
In the second one, we can reorganize the balls in four ways,

00:01:25.444 --> 00:01:27.349
so it has medium entropy.

00:01:27.349 --> 00:01:30.199
For the third one, we have six ways of reorganizing the balls,

00:01:30.200 --> 00:01:31.745
so it has high entropy.

00:01:31.745 --> 00:01:33.920
This is not the exact definition of entropy,

00:01:33.920 --> 00:01:35.329
but it gives us an idea,

00:01:35.329 --> 00:01:38.090
that the more rigid the set is or the more homogeneous,

00:01:38.090 --> 00:01:41.284
the less entropy you'll have, and vice versa.

00:01:41.284 --> 00:01:43.640
Another way to see entropy is in terms of knowledge.

00:01:43.640 --> 00:01:46.676
If we were to pick a random ball from each of the buckets,

00:01:46.676 --> 00:01:49.415
how much do we know about the color of this ball?

00:01:49.415 --> 00:01:52.280
In the first bucket, we know for sure that the ball is red,

00:01:52.280 --> 00:01:53.945
so we have high knowledge.

00:01:53.944 --> 00:01:58.519
In the second bucket, it's very likely to be red and not very likely to be blue.

00:01:58.519 --> 00:02:00.244
So if we bet that it's red,

00:02:00.245 --> 00:02:01.820
we'll be right most of the time.

00:02:01.819 --> 00:02:04.339
So we have medium knowledge of the color of the ball.

00:02:04.340 --> 00:02:08.509
In the third bucket, we know much less since it's equally likely to be blue or red.

00:02:08.509 --> 00:02:10.834
So here, we have low knowledge.

00:02:10.835 --> 00:02:13.775
And it turns out that knowledge and entropy are opposites.

00:02:13.775 --> 00:02:15.215
The more knowledge one has,

00:02:15.215 --> 00:02:17.330
the less entropy, and vice versa.

00:02:17.330 --> 00:02:20.945
Thus, we conclude that the first bucket has low entropy,

00:02:20.944 --> 00:02:22.849
the second one has medium entropy,

00:02:22.849 --> 00:02:24.859
and the third one has high entropy.

00:02:24.860 --> 00:02:26.195
Now when I say opposites,

00:02:26.194 --> 00:02:29.134
I don't mean additive inverse or multiplicative inverses.

00:02:29.134 --> 00:02:32.044
I only mean it in the colloquial sense of the word.

00:02:32.044 --> 00:02:33.514
When one of them is big,

00:02:33.514 --> 00:02:36.364
then the other one is small, and vice versa.

00:02:36.365 --> 00:02:37.670
Over the next few videos,

00:02:37.669 --> 00:02:39.634
we'll cook up a formula for entropy,

00:02:39.634 --> 00:02:41.359
namely, one that gives us low,

00:02:41.360 --> 00:02:44.000
medium, and high values for these buckets.

