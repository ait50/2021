WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.015
Okay, so now, let's go ahead and build a Decision Tree.

00:00:03.015 --> 00:00:04.860
Our algorithm will be very simple.

00:00:04.860 --> 00:00:07.650
Look at the possible splits that each column gives,

00:00:07.650 --> 00:00:09.605
calculate the information gain,

00:00:09.605 --> 00:00:11.335
and pick the largest one.

00:00:11.335 --> 00:00:14.740
So, let's calculate the entropy of the parent, which is this data.

00:00:14.740 --> 00:00:17.574
We'll calculate the entropy of the column of the labels.

00:00:17.574 --> 00:00:19.390
So there are three Pokemon Go's,

00:00:19.390 --> 00:00:21.865
two WhatsApp, and one Snapchat.

00:00:21.864 --> 00:00:25.119
The entropy is negative three over six,

00:00:25.120 --> 00:00:27.595
logarithm base two of three over six,

00:00:27.594 --> 00:00:28.869
minus two over six,

00:00:28.870 --> 00:00:30.429
logarithm base two of two over six,

00:00:30.429 --> 00:00:31.660
minus one over six,

00:00:31.660 --> 00:00:33.625
logarithm base two of one over six.

00:00:33.625 --> 00:00:36.700
This gives us 1.46.

00:00:36.700 --> 00:00:38.478
Now, if we split them by gender,

00:00:38.478 --> 00:00:39.865
we get two sets,

00:00:39.865 --> 00:00:43.344
one with one Pokemon Go and two WhatsApp,

00:00:43.344 --> 00:00:46.174
and one with one Snapchat and two Pokemon Go.

00:00:46.174 --> 00:00:50.174
The entropies for these sets are both 0.92.

00:00:50.174 --> 00:00:55.829
Thus, the average entropy of the children of this node is 0.92,

00:00:55.829 --> 00:01:00.089
and the information gain is 1.46 minus 0.92,

00:01:00.090 --> 00:01:01.845
which is zero point 54.

00:01:01.844 --> 00:01:03.884
Now, if we split by occupation,

00:01:03.884 --> 00:01:06.254
we get one set of three Pokemon Go's,

00:01:06.254 --> 00:01:09.269
and one of two WhatsApps, and one Snapchat.

00:01:09.269 --> 00:01:12.000
The first set has entropy zero,

00:01:12.000 --> 00:01:15.180
and the other has entropy 0.92.

00:01:15.180 --> 00:01:17.940
Therefore, the average of these is 0.46,

00:01:17.939 --> 00:01:23.054
and the information gain is 1.46 minus 0.46, which is one.

00:01:23.055 --> 00:01:28.065
So to summarize, splitting by the gender column gave us an information gain of 0.54,

00:01:28.064 --> 00:01:31.664
and splitting by the occupation column gave us an information gain of one.

00:01:31.665 --> 00:01:33.780
The algorithm says, pick the column with

00:01:33.780 --> 00:01:36.719
the highest information gain, which is occupation.

00:01:36.719 --> 00:01:40.185
So we split by occupation. We'll get two sets.

00:01:40.185 --> 00:01:41.640
One is very nice,

00:01:41.640 --> 00:01:43.844
since everybody downloaded Pokemon Go,

00:01:43.844 --> 00:01:46.304
and in the other one, we can still do better.

00:01:46.305 --> 00:01:49.050
We can split based on the gender column.

00:01:49.049 --> 00:01:51.179
Since now, we get two very nice sets,

00:01:51.180 --> 00:01:53.145
one where everybody downloaded WhatsApp,

00:01:53.144 --> 00:01:55.754
and the other one where everybody downloaded Snapchat.

00:01:55.754 --> 00:01:58.189
And we're done, here's our Decision Tree.

00:01:58.189 --> 00:01:59.679
If we want to do this for

00:01:59.680 --> 00:02:02.740
continuous features instead of discrete features, we can still do it.

00:02:02.739 --> 00:02:04.359
We'll let you think about the details.

00:02:04.359 --> 00:02:09.390
So basically, the idea is to think of all the possible vertical and horizontal cuts,

00:02:09.390 --> 00:02:13.013
and seeing which one maximizes the entropy,

00:02:13.014 --> 00:02:17.080
and then iterating over and over as we build a Decision Tree.

00:02:17.080 --> 00:02:20.560
Here, we can see that our first cut is vertical at the value five.

00:02:20.560 --> 00:02:25.134
Our next cut will be a horizontal cut at the height seven.

00:02:25.134 --> 00:02:30.250
And our final cut will be horizontal at height two.

00:02:30.250 --> 00:02:34.270
And finally, we have our Decision Tree that cuts our data in two.

