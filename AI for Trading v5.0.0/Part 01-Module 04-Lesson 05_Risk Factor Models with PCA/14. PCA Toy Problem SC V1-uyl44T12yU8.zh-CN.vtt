WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.669
大家好在此 notebook 中

00:00:02.669 --> 00:00:07.674
我们将学习如何通过 PCA 降维

00:00:07.674 --> 00:00:12.365
降维是 PCA 的主要用途之一

00:00:12.365 --> 00:00:14.089
在之前的课程中

00:00:14.089 --> 00:00:19.295
你已经学习了 PCA 的原理以及特征向量和特征值

00:00:19.295 --> 00:00:23.835
在此 notebook 中 我们将学习如何向小型数据集应用 PCA

00:00:23.835 --> 00:00:28.690
首先讲解下什么是降维

00:00:28.690 --> 00:00:33.484
假设我们有这样的二维数据

00:00:33.484 --> 00:00:38.479
可以看出 大部分数据点都紧挨着这条直线

00:00:38.479 --> 00:00:44.433
数据主要在这个方向变化很大

00:00:44.433 --> 00:00:48.324
但是这个方向没什么变化

00:00:48.325 --> 00:00:52.760
我们仅通过查看数据点如何沿着这条直线分布

00:00:52.759 --> 00:00:57.420
就能解释数据的大部分方差

00:00:57.420 --> 00:01:01.130
所以我们可以将所有数据点投射到这条直线上

00:01:01.130 --> 00:01:06.445
将这个二维数据降成一维数据

00:01:06.444 --> 00:01:09.529
通过将数据投射到直线上

00:01:09.530 --> 00:01:13.359
我们能够减少描述数据所需的变量数量

00:01:13.359 --> 00:01:15.650
因为只需一个数字就能指定

00:01:15.650 --> 00:01:19.085
数据点沿着直线的位置

00:01:19.084 --> 00:01:24.239
所以描述原始数据的两个变量

00:01:24.239 --> 00:01:30.089
可以替换成一个新的表示这种线性关系的变量

00:01:30.090 --> 00:01:33.760
需要注意的是 新的变量只是一种抽象工具

00:01:33.760 --> 00:01:39.020
使我们能以更紧凑的方式表示这个数据

00:01:39.019 --> 00:01:43.795
也许是真实的量 也许不是

00:01:43.795 --> 00:01:47.015
下面看看如何用代码实现

00:01:47.015 --> 00:01:51.140
为了简单起见 我们将使用小型二维数据集

00:01:51.140 --> 00:01:52.700
在后面的 notebook 中

00:01:52.700 --> 00:01:57.270
你将有机会将在此 notebook 中学到的知识应用到实际股票数据上

00:01:57.269 --> 00:02:01.340
首先创建一些随机相关的数据集

00:02:01.340 --> 00:02:07.140
在此代码中 你可以选择数据范围和相关性

00:02:07.140 --> 00:02:13.240
代码会输出一个包含数据点的图表 并且显示出相关性值

00:02:13.240 --> 00:02:17.905
我们将数据范围设成了 10-80

00:02:17.905 --> 00:02:24.319
数据点在 x 轴和 y 轴的范围都是从 10 到 80

00:02:24.319 --> 00:02:30.659
注意 相关性为 0 表示完全没有关系

00:02:30.659 --> 00:02:34.889
相关性为 1 表示完全相关

00:02:34.889 --> 00:02:39.789
你可以改变相关性并创建你想要的数据

00:02:39.789 --> 00:02:42.030
创建好数据后

00:02:42.030 --> 00:02:46.574
PCA 的下一步是让数据以零居中

00:02:46.574 --> 00:02:50.339
通常还会归一化数据

00:02:50.340 --> 00:02:53.015
称之为均值归一化

00:02:53.014 --> 00:02:55.709
虽然必须居中数据

00:02:55.710 --> 00:02:58.115
但是不一定要归一化数据

00:02:58.115 --> 00:03:00.665
我归一化了

00:03:00.664 --> 00:03:05.384
均值归一化不仅使数据以零居中

00:03:05.384 --> 00:03:08.435
而且让数据均匀地

00:03:08.435 --> 00:03:11.140
围绕零在很小的间隔内分布

00:03:11.139 --> 00:03:12.604
可以看出

00:03:12.604 --> 00:03:16.519
数据范围不再是 10到 80

00:03:16.520 --> 00:03:21.920
归一化之后 数据现在的范围是 -3 到 3

00:03:21.919 --> 00:03:25.484
这样可以提高算法的收敛速度

00:03:25.485 --> 00:03:27.275
居中数据后

00:03:27.275 --> 00:03:29.450
我们就可以进行主成分分析了

00:03:29.449 --> 00:03:33.204
我们将使用软件包 Scikit-Learn

00:03:33.205 --> 00:03:39.115
Scikit-Learn 的 PCA 类可以使我们轻松地对数据进行主成分分析

00:03:39.115 --> 00:03:43.610
首先需要根据一组给定的参数创建一个 PCA 对象

00:03:43.610 --> 00:03:49.080
参数包括要使用的主成分数量

00:03:49.080 --> 00:03:54.190
我们将使用两个成分 因为稍后需要可视化它们

00:03:54.189 --> 00:03:59.884
这些是 PCA 算法将使用的参数

00:03:59.884 --> 00:04:08.139
下一步 使用 fit 方法将数据传递给 PC 对象小贴士

00:04:08.139 --> 00:04:13.379
在 Scikit-Learn 中 PCA 算法会自动为你居中数据

00:04:13.379 --> 00:04:16.250
所以可以将原始数据集传入 fit 方法中

00:04:16.250 --> 00:04:20.930
而不用传入我们在这里归一化的数据

00:04:20.930 --> 00:04:23.025
拟合数据后

00:04:23.024 --> 00:04:26.479
我们可以使用 PCA() 类的属性

00:04:26.480 --> 00:04:32.634
查看特征向量（亦称为主成分）及其特征值

00:04:32.634 --> 00:04:39.404
PCA() 类有一个重要属性是方差贡献率

00:04:39.404 --> 00:04:42.979
方差贡献率表示

00:04:42.980 --> 00:04:46.780
每个主成分能解释的方差百分比

00:04:46.779 --> 00:04:49.579
通常 特征值最大的主成分

00:04:49.579 --> 00:04:53.759
能解释大部分方差

00:04:53.759 --> 00:04:56.545
我们通常希望保留这些主成分

00:04:56.545 --> 00:04:58.910
例如 从这里可以看到

00:04:58.910 --> 00:05:03.555
第一个主成分解释了 94% 的方差

00:05:03.555 --> 00:05:06.845
并且特征值最大

00:05:06.845 --> 00:05:09.985
获得主成分后

00:05:09.985 --> 00:05:11.545
就可以可视化它们了

00:05:11.545 --> 00:05:15.655
这些是具有第一个主成分和

00:05:15.654 --> 00:05:17.924
第二个主成分的数据

00:05:17.925 --> 00:05:21.259
可以看出 第一个主成分

00:05:21.259 --> 00:05:25.324
沿着数据变化最大的方向

00:05:25.324 --> 00:05:28.370
你经常会遇到的一个问题是

00:05:28.370 --> 00:05:31.610
应该使用多少个主成分

00:05:31.610 --> 00:05:36.754
例如 假设数据集有 1,000 个维度

00:05:36.754 --> 00:05:38.719
你应该将数据集降到 500 个维度

00:05:38.720 --> 00:05:43.955
还是进一步降到 100 个维度？

00:05:43.954 --> 00:05:47.884
通常 选择多少个主成分

00:05:47.884 --> 00:05:52.069
取决于你要从原始数据中保留多少方差

00:05:52.069 --> 00:05:56.204
例如 你可能想保留 90% 的方差

00:05:56.204 --> 00:06:00.024
或者只想保留 50% 的方差

00:06:00.024 --> 00:06:05.359
你可以使用 PCA() 类的方差贡献率

00:06:05.360 --> 00:06:07.670
决定保留多少个主成分

00:06:07.670 --> 00:06:11.074
从而保留指定的方差量

00:06:11.074 --> 00:06:15.654
例如 如果你想保留 90% 的方差

00:06:15.654 --> 00:06:17.449
可以将方差贡献率数组中的元素加起来

00:06:17.449 --> 00:06:22.214
直到达到想要的值

00:06:22.214 --> 00:06:25.519
达到目标值所需的元素数量

00:06:25.519 --> 00:06:28.219
就是保留一定的方差量

00:06:28.220 --> 00:06:31.765
所需的主成分数量

00:06:31.764 --> 00:06:37.223
例如 如果你需要将 5 个元素加起来 才能保留 90% 的方差

00:06:37.223 --> 00:06:40.904
那么需要 5 个主成分

00:06:40.904 --> 00:06:44.599
我们已经知道所有的主成分

00:06:44.600 --> 00:06:48.935
现在将利用 PCA 降维

00:06:48.935 --> 00:06:53.454
因为在这个小示例中使用的数据只有两个维度

00:06:53.454 --> 00:06:57.214
所以我们最多只能降到一维

00:06:57.214 --> 00:06:59.479
现在 将 PCA 算法中的

00:06:59.480 --> 00:07:04.180
主成分数量设为 1

00:07:04.180 --> 00:07:08.550
运行只有一个成分的 PCA 算法后

00:07:08.550 --> 00:07:11.540
我们可以使用 PCA() 类的 transform 方法

00:07:11.540 --> 00:07:15.450
查看转换后的数据看起来如何

00:07:15.449 --> 00:07:17.519
在这个简单示例中

00:07:17.519 --> 00:07:19.819
transform 方法将数据投射到了第一个主成分上

00:07:19.819 --> 00:07:25.709
所以只有一条直线

00:07:25.709 --> 00:07:28.589
在处理更高维度的数据时

00:07:28.589 --> 00:07:31.069
transform 方法会将数据

00:07:31.069 --> 00:07:33.769
投射到更低维度的表面上

00:07:33.769 --> 00:07:37.839
这个表面由你在算法中使用的主成分数量决定

