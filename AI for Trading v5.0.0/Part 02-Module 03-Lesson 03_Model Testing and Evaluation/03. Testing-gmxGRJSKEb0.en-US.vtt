WEBVTT
Kind: captions
Language: en-US

00:00:00.090 --> 00:00:02.290
In this lesson,
we'll use a concepts of regression and

00:00:02.290 --> 00:00:06.400
classification repeatedly, so
let's first recall their definitions.

00:00:06.400 --> 00:00:11.336
A regression model is one that predicts
a value like 4 minus 3, or 6.7.

00:00:11.336 --> 00:00:15.910
In the graph on the left, we have gone
a line that fits the data closely.

00:00:15.910 --> 00:00:17.940
If we have a new value on the x axis,

00:00:17.940 --> 00:00:21.750
we approximate by finding
the corresponding y value over the line.

00:00:21.750 --> 00:00:24.980
A classification problem is when
one wants to determine a state,

00:00:24.980 --> 00:00:29.210
such as positive, negative, or
such as yes, no, or cat, dog.

00:00:29.210 --> 00:00:34.020
In the graph in the right, we have
some blue points labelled positive and

00:00:34.020 --> 00:00:35.940
some red points labelled negative.

00:00:35.939 --> 00:00:38.359
And we have drawn a line
that separates them.

00:00:38.359 --> 00:00:40.269
If we have a new value in the plane,

00:00:40.270 --> 00:00:44.340
we guess its state based on which
of the two regions it belongs.

00:00:44.340 --> 00:00:47.600
So to summarize,
regression returns a numeric value and

00:00:47.600 --> 00:00:49.740
classification returns a state.

00:00:49.740 --> 00:00:53.310
So, now that you've built a model,
how do you convince yourself and

00:00:53.310 --> 00:00:55.500
others that your model is good?

00:00:55.500 --> 00:00:56.969
You do this by testing.

00:00:56.969 --> 00:01:00.250
So letâ€™s look at the picture of BUFF
which shows a small regression example,

00:01:00.250 --> 00:01:02.640
our data corresponds
to the grade points.

00:01:02.640 --> 00:01:05.219
We have trained two
models to fit the data.

00:01:05.219 --> 00:01:07.439
One is a line and
the other one is a curve.

00:01:08.549 --> 00:01:11.920
Now the question is,
which of these two models is better?

00:01:11.920 --> 00:01:14.120
Well the one on the right
fits the data perfectly,

00:01:14.120 --> 00:01:16.042
while the one on the left doesn't.

00:01:16.042 --> 00:01:18.010
So we're tempted to say
the one on the right.

00:01:18.010 --> 00:01:22.350
To see how good the fit is we can
take a new point, this red point.

00:01:22.349 --> 00:01:26.164
The point seems to be well approximated
by the model in the left, but

00:01:26.165 --> 00:01:29.410
is very badly approximated
by the model in the right.

00:01:29.409 --> 00:01:32.500
So it seems that maybe the model in
the left is better than the model in

00:01:32.500 --> 00:01:33.680
the right, after all.

00:01:33.680 --> 00:01:36.630
What makes the model in the left better,
is that even though it doesn't

00:01:36.629 --> 00:01:40.847
fit the data perfectly, it generalizes
better than the one on the right.

00:01:40.847 --> 00:01:43.480
The model in the right tries
to fit the data too well, and

00:01:43.480 --> 00:01:45.020
it ends up memorizing it.

00:01:45.019 --> 00:01:48.839
This is called overfitting, and we
will learn it later in the nanodegree.

00:01:48.840 --> 00:01:53.109
Now the question is, how do we find
a model that generalizes well?

00:01:53.109 --> 00:01:56.250
This is where we introduce
the concept of testing.

00:01:56.250 --> 00:02:01.409
What we'll do here is we'll split our
data into two sets, the training set and

00:02:01.409 --> 00:02:03.099
the testing set.

00:02:03.099 --> 00:02:07.449
In this figure the training set
is the set of grey points, and

00:02:07.450 --> 00:02:09.740
the testing set is
the set of white points.

00:02:09.740 --> 00:02:13.680
And now what we'll do is as the name
suggests, we'll use a training set to

00:02:13.680 --> 00:02:17.875
train the model, and then we test
our results in the testing set.

00:02:17.875 --> 00:02:21.495
So, here we have two copies of our sets
to the training set formed by the gray

00:02:21.495 --> 00:02:24.314
points and the testing set
formed by the white points.

00:02:24.314 --> 00:02:27.974
We can see two models that we train
on the training set, the gray points.

00:02:27.974 --> 00:02:31.905
Here the model on the right seems to
be better than the model on the left.

00:02:31.905 --> 00:02:36.396
But once we test on the testing set,
the white points, we see that the model

00:02:36.395 --> 00:02:40.639
on the left is much better,
as the errors in red are much smaller.

00:02:40.639 --> 00:02:43.189
Thus we conclude that the model
on the left is better.

00:02:43.189 --> 00:02:46.609
Because although it performs a little
less well on the training set,

00:02:46.610 --> 00:02:48.885
it does much better on the testing set.

00:02:48.884 --> 00:02:51.894
We can do the same procedure on
a classification problem such as the one

00:02:51.895 --> 00:02:52.835
here.

00:02:52.835 --> 00:02:56.795
We have trained two classification
models to separate the blue positive and

00:02:56.794 --> 00:02:58.634
the red negative points.

00:02:58.634 --> 00:03:01.974
This one on the left is okay,
as it only makes a few mistakes.

00:03:01.974 --> 00:03:03.814
This one on the right does great,

00:03:03.814 --> 00:03:06.344
because it manages to
separate all the points.

00:03:06.344 --> 00:03:09.740
But our intuition tells us that
the one left may be better,

00:03:09.740 --> 00:03:11.320
as it is more general.

00:03:11.319 --> 00:03:15.150
Meanwhile, the model in the left
treats these outliers as noise, and

00:03:15.150 --> 00:03:19.099
it tries to fit the data in
a simpler and more general way.

00:03:19.099 --> 00:03:21.710
So in order to pick a good model,
we'll take some points and

00:03:21.710 --> 00:03:23.510
call them the testing set.

00:03:23.509 --> 00:03:26.919
The training set is then given by
the points that are filled in,

00:03:26.919 --> 00:03:30.239
while the testing set is given by
the points that are not filled in.

00:03:30.240 --> 00:03:32.010
Now we train the two models.

00:03:32.009 --> 00:03:35.310
Notice that the two models
fit the training set well.

00:03:35.310 --> 00:03:39.030
But once we introduce the testing set,
the model in the left

00:03:39.030 --> 00:03:43.879
only makes one mistake, whereas the
model in the right makes two mistakes.

00:03:43.879 --> 00:03:48.469
Thus by testing we conclude that
the model in the left is better.

00:03:48.469 --> 00:03:51.046
The way to do this in
scikit-learn is very simple using

00:03:51.046 --> 00:03:54.359
the function train-test-split
from the model selection package.

00:03:54.360 --> 00:03:56.816
First, we import train_test_split.

00:03:56.816 --> 00:04:00.195
The train_test_split function
takes the following as arguments.

00:04:00.195 --> 00:04:04.280
The input, the output, and

00:04:04.280 --> 00:04:07.469
the percentage of data we want
to leave as testing data.

00:04:07.469 --> 00:04:11.800
For example, test size equals
0.25 means that 25 percent of our

00:04:11.800 --> 00:04:14.400
data will be used as testing.

00:04:14.400 --> 00:04:16.959
In this example, we have 16 data points.

00:04:16.959 --> 00:04:19.420
So 4 of them will be
used as testing data, and

00:04:19.420 --> 00:04:22.150
the remaining 12 as training data.

00:04:22.149 --> 00:04:25.459
Now there is a golden rule that
we must never, ever break.

00:04:25.459 --> 00:04:30.579
The rule says, thou shalt never use
your testing data for training.

00:04:30.579 --> 00:04:33.000
This is very important when
we select some data for

00:04:33.000 --> 00:04:37.771
testing, we must put it aside, and
not look at it until the very end.

00:04:37.771 --> 00:04:41.379
And we should not use it
to train the algorithm.

00:04:41.379 --> 00:04:44.719
This rule is actually very easy
to accidentally break, and

00:04:44.720 --> 00:04:45.640
you'll see why very soon.

