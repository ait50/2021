WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.370
As the last thing, we'll learn a very useful method to

00:00:02.370 --> 00:00:05.835
recycle our data called, K-Fold Cross Validation.

00:00:05.835 --> 00:00:08.669
As we have learned, testing is done by separating

00:00:08.669 --> 00:00:11.279
our data into a training and a testing set,

00:00:11.279 --> 00:00:13.949
but this is not always ideal as we seem to be throwing away

00:00:13.949 --> 00:00:16.980
some data that could be useful for training our algorithm.

00:00:16.980 --> 00:00:19.910
Is there anything we can do to not throw away these data,

00:00:19.910 --> 00:00:21.789
but at the same time not cheat?

00:00:21.789 --> 00:00:26.399
Well, here's the solution and this is when K-Fold Cross Validation comes into play.

00:00:26.399 --> 00:00:30.750
What we do with K-Fold Cross Validation is to break your data into K buckets.

00:00:30.750 --> 00:00:32.685
In this case, K is four.

00:00:32.685 --> 00:00:35.804
Then, we just train our model K times.

00:00:35.804 --> 00:00:39.269
Each time using a different bucket as our testing set,

00:00:39.270 --> 00:00:42.710
and the remaining points as our training set.

00:00:42.710 --> 00:00:46.160
Then, we average the results to get a final model.

00:00:46.159 --> 00:00:49.114
The command to do this in SKLearn is very simple.

00:00:49.115 --> 00:00:52.579
All we have to do is create a K-fold object where

00:00:52.579 --> 00:00:56.089
the parameters are the size of the data and the size of the testing set.

00:00:56.090 --> 00:01:00.550
It is always recommended to randomize our data to remove any hint of a bias.

00:01:00.549 --> 00:01:02.765
Here, we're still splitting our data into buckets

00:01:02.765 --> 00:01:05.599
except these are chosen randomly instead of in order.

00:01:05.599 --> 00:01:08.780
Randomizing is also easily done in SKLearn by setting

00:01:08.780 --> 00:01:13.340
the shuffled parameter to True when we initialize our K-fold object.

