WEBVTT
Kind: captions
Language: en-US

00:00:00.300 --> 00:00:04.300
Now let's look carefully at the graph
we have obtained underneath.

00:00:04.300 --> 00:00:08.080
This is what's called
a model complexity graph.

00:00:08.080 --> 00:00:11.710
Notice that on the left we have
a model that underfits, and

00:00:11.710 --> 00:00:14.620
gives us high training and
testing errors.

00:00:14.620 --> 00:00:16.930
The model on the right overfits, and

00:00:16.930 --> 00:00:21.330
it gives us a very low training error,
but a high testing error.

00:00:21.330 --> 00:00:24.330
The model in the middle is just right,
and

00:00:24.330 --> 00:00:27.830
it gives us relatively low training and
testing errors.

00:00:27.830 --> 00:00:29.730
This is the model we should pick.

00:00:29.730 --> 00:00:31.630
So here we have a bit
of a clearer picture.

00:00:31.630 --> 00:00:34.530
In the x axis we have
the complexity of the model.

00:00:35.800 --> 00:00:41.700
So we go from linear, degree 1,
all the way to polynomial, degree 4.

00:00:41.700 --> 00:00:45.750
On the left the models show
underfitting or high bias error.

00:00:45.760 --> 00:00:50.500
On the right We see overfitting,
or high variance error.

00:00:50.500 --> 00:00:53.870
And in the middle, around
the value 2 there is a happy point

00:00:53.870 --> 00:00:57.390
where we are neither underfitting nor
overfitting.

00:00:57.390 --> 00:01:03.750
Thus, we decided the best model for
our data is a polynomial of degree 2.

00:01:03.750 --> 00:01:05.330
So far so good, right?

00:01:05.330 --> 00:01:06.700
Whats happening?

00:01:06.700 --> 00:01:06.710
What is this?
Whats happening?

00:01:06.710 --> 00:01:07.270
What is this?

00:01:08.450 --> 00:01:09.720
No, what do we do?

00:01:09.720 --> 00:01:11.190
We broke the golden rule.

00:01:11.190 --> 00:01:14.890
You should never use your
testing data for training.

00:01:14.890 --> 00:01:17.030
We made a huge mistake.

00:01:17.030 --> 00:01:19.470
But let's see, what did we do here?

00:01:19.470 --> 00:01:21.310
Here is the problem.

00:01:21.310 --> 00:01:24.760
We used our testing data
to train our model.

00:01:24.760 --> 00:01:29.120
We're not supposed to use our
testing data until the very end.

00:01:29.120 --> 00:01:32.080
And we used it to make a huge
decision about our model.

00:01:32.080 --> 00:01:33.640
This is completely forbidden.

00:01:33.640 --> 00:01:36.200
You're not allowed to
touch your testing data.

00:01:36.200 --> 00:01:37.910
So how do we fix this?

00:01:37.910 --> 00:01:41.420
How do we make a good decision about our
model without using the testing data?

00:01:42.550 --> 00:01:43.810
Well, no need to panic.

00:01:43.810 --> 00:01:47.000
We can fix this by breaking
our data set even more.

00:01:47.000 --> 00:01:48.800
Now, instead of having a training and

00:01:48.810 --> 00:01:53.090
a testing set,
we'll add a cross validation set.

00:01:53.090 --> 00:01:56.990
Now, the training set will be used for
training the parameters.

00:01:56.990 --> 00:01:59.170
The cross validation
set will be used for

00:01:59.180 --> 00:02:03.780
making decisions about the model,
such as the degree of the polynomial.

00:02:03.780 --> 00:02:08.030
And the testing set will be used for
the final testing of the model.

00:02:08.030 --> 00:02:09.030
Now our graph looks so

00:02:09.030 --> 00:02:12.900
much nicer with a cross validation
error instead of a testing error.

00:02:12.900 --> 00:02:14.310
So let's recap.

00:02:14.310 --> 00:02:18.160
Here we have the model complexity
graph and the example all together.

00:02:18.160 --> 00:02:18.170
On the left, we see underfitting or
an oversimplification of the problem.
Here we have the model complexity
graph and the example all together.

00:02:18.170 --> 00:02:23.410
On the left, we see underfitting or
an oversimplification of the problem.

00:02:23.410 --> 00:02:27.780
This is bad on both the training and
the cross validation set

00:02:27.780 --> 00:02:31.980
because our model simply does not
capture the complexity of our data.

00:02:31.990 --> 00:02:36.910
On the right, we see overfitting or
an over complication of the problem.

00:02:36.910 --> 00:02:36.920
This is great on the training set,
because we are memorizing it, but
On the right, we see overfitting or
an over complication of the problem.

00:02:36.920 --> 00:02:40.700
This is great on the training set,
because we are memorizing it, but

00:02:40.710 --> 00:02:44.390
bad on the cross validation set because
the model does not generalize well.

00:02:45.690 --> 00:02:49.540
And in the middle we have the perfect
model which is good in both the training

00:02:49.540 --> 00:02:51.160
and the testing set.

00:02:51.160 --> 00:02:54.710
I like to imagine underfitting
as coming to an exam unprepared.

00:02:54.710 --> 00:02:57.530
No matter how hard you
try you won't do well.

00:02:57.530 --> 00:03:00.390
Overfitting is like studying for
an exam but instead of

00:03:00.390 --> 00:03:05.300
trying to understand the material,
you just memorize the book word by word.

00:03:05.300 --> 00:03:07.120
You can repeat anything that's given,
but

00:03:07.120 --> 00:03:10.310
you won't be able to answer
new questions about the data.

00:03:10.310 --> 00:03:13.080
A good model is like studying for
an exam and

00:03:13.080 --> 00:03:15.220
coming ready to answer any
questions in the material.

00:03:16.530 --> 00:03:19.500
And here is just a general picture
of the model complexity graph.

00:03:19.500 --> 00:03:22.370
Of course they may not look as pretty,
but in real life you see that most of

00:03:22.370 --> 00:03:25.000
the time your models will exhibit
a behavior like this one.

00:03:25.000 --> 00:03:29.560
Wheres the model gets more complex, the
training error gets smaller and smaller.

00:03:29.560 --> 00:03:33.930
And the testing error starts big,
then decreases and then increases again.

00:03:33.930 --> 00:03:36.440
On the left you underfit.

00:03:36.440 --> 00:03:38.790
On the right, you overfit.

00:03:38.790 --> 00:03:38.800
And the perfect point is here,
On the right, you overfit.

00:03:38.800 --> 00:03:40.420
And the perfect point is here,

00:03:40.430 --> 00:03:42.450
where the graphs just start
to distance from each other.

