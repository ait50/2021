WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.349
最后要介绍的一种模型是循环神经网络

00:00:04.349 --> 00:00:08.369
它可以用于自然语言处理和时序运算

00:00:08.369 --> 00:00:13.414
顾名思义 循环神经网络是一种神经网络

00:00:13.414 --> 00:00:16.619
神经网络是指很多回归

00:00:16.620 --> 00:00:20.535
按序列并行堆叠起来的模型

00:00:20.535 --> 00:00:23.179
回归按序列堆叠是指

00:00:23.179 --> 00:00:26.339
一个回归的输出

00:00:26.339 --> 00:00:30.000
作为输入传入另一个回归中 像链条一样

00:00:30.000 --> 00:00:32.554
并行堆叠是指

00:00:32.554 --> 00:00:34.829
有很多个回归

00:00:34.829 --> 00:00:38.460
这些回归的输出会传入另一个回归层级

00:00:38.460 --> 00:00:43.549
循环神经网络中的循环一词是指

00:00:43.549 --> 00:00:46.609
在用传入的数据进行训练时

00:00:46.609 --> 00:00:50.894
它将部分中间输出当做输入

00:00:50.895 --> 00:00:56.070
RNN 会接受输入并输出预测值

00:00:56.070 --> 00:01:00.009
RNN 还会输出其他信号

00:01:00.009 --> 00:01:04.344
即中间输出 并将该信号传入其本身

00:01:04.344 --> 00:01:10.099
在下个时间步 当 RNN 从数据源中获得另一个输入时

00:01:10.099 --> 00:01:12.769
它会同时使用该输入

00:01:12.769 --> 00:01:17.759
以及上一个中间输出计算下个预测值

00:01:17.760 --> 00:01:20.060
可以将这种信号看做

00:01:20.060 --> 00:01:23.885
RNN 记住过去的相关信息的一种方式

00:01:23.885 --> 00:01:26.079
为了训练循环神经网络

00:01:26.079 --> 00:01:28.575
你需要传入很多过去的数据

00:01:28.575 --> 00:01:32.355
然后使用梯度下降法

00:01:32.355 --> 00:01:36.795
来调整网络的系数 最后就可以看到 RNN 预测未来数据的效果了

00:01:36.795 --> 00:01:40.549
一种常见的循环神经网络形式

00:01:40.549 --> 00:01:44.420
由一个或多个长短记忆单元组成

00:01:44.420 --> 00:01:46.960
长短记忆 (LSTM) 单元

00:01:46.959 --> 00:01:53.959
由多个神经网络组成 每个都负责特定的任务

00:01:53.959 --> 00:01:58.099
LSTM 单元可以看做一个装配线

00:01:58.099 --> 00:02:02.750
不同的装配线工人负责特定的任务

00:02:02.750 --> 00:02:07.069
LSTM 单元将数据作为输入

00:02:07.069 --> 00:02:11.305
并且将上个时期生成的信号也作为输入

00:02:11.305 --> 00:02:14.300
从上个时期获取的信息

00:02:14.300 --> 00:02:17.390
可以看做过去的记忆

00:02:17.389 --> 00:02:21.514
某些工人会消除某些记忆

00:02:21.514 --> 00:02:26.884
其他工人根据传入数据添加更多记忆

00:02:26.884 --> 00:02:31.584
还有一些其他工人决定输出什么

00:02:31.585 --> 00:02:35.875
循环神经网络有两种输出

00:02:35.875 --> 00:02:39.370
一种是对相关变量的预测

00:02:39.370 --> 00:02:43.645
另一种是关于未来本身的中间输出

00:02:43.645 --> 00:02:45.860
可以将中间输出看做

00:02:45.860 --> 00:02:49.470
它希望传递给下个时期的记忆

