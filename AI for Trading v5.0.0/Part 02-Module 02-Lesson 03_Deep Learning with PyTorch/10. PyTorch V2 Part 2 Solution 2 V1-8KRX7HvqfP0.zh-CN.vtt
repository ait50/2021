WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.920
欢迎回来下面介绍下我是如何实现 softmax 函数的

00:00:04.920 --> 00:00:07.215
在分子中

00:00:07.215 --> 00:00:08.850
我们希望计算指数

00:00:08.850 --> 00:00:12.180
因此 torch.exp 很好理解

00:00:12.180 --> 00:00:14.520
我们将对 x 求指数

00:00:14.520 --> 00:00:16.470
x 是输入张量

00:00:16.470 --> 00:00:20.100
在分母中也要计算指数

00:00:20.100 --> 00:00:23.150
因此使用 torch.exp

00:00:23.150 --> 00:00:25.740
然后对所有这些值求和

00:00:25.740 --> 00:00:29.940
要注意的一点是 我们要对一行求和

00:00:29.940 --> 00:00:33.915
即一行中的每列 表示每个样本

00:00:33.915 --> 00:00:36.375
对于一个样本 我们想对这些值求和

00:00:36.375 --> 00:00:38.160
在 torch.sum 中

00:00:38.160 --> 00:00:40.170
将维度设为 1

00:00:40.170 --> 00:00:44.195
这样就会对列求和

00:00:44.195 --> 00:00:46.985
这里的 torch.sum

00:00:46.985 --> 00:00:50.345
将返回一个张量

00:00:50.345 --> 00:00:53.530
也就是有 64 个元素的向量

00:00:53.530 --> 00:00:54.870
现在的问题是

00:00:54.870 --> 00:00:56.880
这是 64 x 10

00:00:56.880 --> 00:01:00.115
这是长为 64 的向量

00:01:00.115 --> 00:01:06.575
它将使该张量里的每个元素除以所有 64 个值

00:01:06.575 --> 00:01:11.450
结果是 64 x 64 张量 这不是我们想要的结果

00:01:11.450 --> 00:01:13.355
我们希望输出是 64 x 10

00:01:13.355 --> 00:01:20.090
因此我们需要将这个张量变形为 64 行

00:01:20.090 --> 00:01:22.345
但是每行只有一个值

00:01:22.345 --> 00:01:23.520
这样的话

00:01:23.520 --> 00:01:27.670
它将查看这个张量里的每行

00:01:27.670 --> 00:01:31.505
将查看这个张量里的对等行

00:01:31.505 --> 00:01:35.180
由于这个张量里的每行只有一个值

00:01:35.180 --> 00:01:40.550
因此它将使这个指数值除以分母张量里的一个值

00:01:40.550 --> 00:01:42.700
听起来有点复杂

00:01:42.700 --> 00:01:49.279
我们首先要理解 PyTorch 中的广播概念

00:01:49.279 --> 00:01:52.100
以及如何使所有这些张量具有正确的形状

00:01:52.100 --> 00:01:55.510
并执行正确的操作

00:01:55.510 --> 00:01:57.525
这么操作后

00:01:57.525 --> 00:02:01.340
将输出传入 softmax 函数

00:02:01.340 --> 00:02:02.390
获得概率

00:02:02.390 --> 00:02:05.285
看看形状 形状为 64 x 10

00:02:05.285 --> 00:02:07.700
如果对每行求和

00:02:07.700 --> 00:02:08.765
会发现和为 1

00:02:08.765 --> 00:02:11.340
符合正常的概率分布

00:02:11.340 --> 00:02:17.870
现在看看如何使用 nn 模块便捷地

00:02:17.870 --> 00:02:22.940
构建强大的神经网络

00:02:22.940 --> 00:02:28.565
你将能够使用相同的框架构建越来越大的神经网络

00:02:28.565 --> 00:02:30.215
大概原理如下

00:02:30.215 --> 00:02:32.375
创建新的类

00:02:32.375 --> 00:02:34.190
称之为 Network

00:02:34.190 --> 00:02:35.300
当然也可以叫其他任何名称

00:02:35.300 --> 00:02:36.800
可以叫 Classifier

00:02:36.800 --> 00:02:38.430
可以叫 MNIST

00:02:38.430 --> 00:02:40.920
叫什么都不重要

00:02:40.920 --> 00:02:46.125
但是必须是 nn.Module 的子类

00:02:46.125 --> 00:02:51.360
然后在 __init__ 方法里

00:02:51.360 --> 00:02:54.925
你需要调用 super() 并运行 nn.Module 的 init 方法

00:02:54.925 --> 00:02:57.120
这么做的原因是

00:02:57.120 --> 00:02:59.285
让 PyTorch 知道 要去注册

00:02:59.285 --> 00:03:00.770
你添加到网络里的

00:03:00.770 --> 00:03:03.290
所有不同层级和操作

00:03:03.290 --> 00:03:04.970
如果没有这部分代码

00:03:04.970 --> 00:03:06.740
PyTorch 将无法跟踪

00:03:06.740 --> 00:03:08.680
你添加到网络里的内容 

00:03:08.680 --> 00:03:14.025
在这里使用 nn.Linear 创建隐藏层

00:03:14.025 --> 00:03:15.230
它的作用是

00:03:15.230 --> 00:03:19.250
为线性转换创建操作

00:03:19.250 --> 00:03:24.370
我们将输入 x 与权重相乘并加上偏差项

00:03:24.370 --> 00:03:26.205
这就是线性转换

00:03:26.205 --> 00:03:29.110
这行代码会调用 nn.Linear

00:03:29.110 --> 00:03:32.330
它将创建一个对象

00:03:32.330 --> 00:03:35.620
对象本身将创建权重参数和偏差参数

00:03:35.620 --> 00:03:39.270
然后将张量传入这个隐藏层（这个对象）

00:03:39.270 --> 00:03:43.540
它将自动为你计算线性转换结果

00:03:43.540 --> 00:03:47.540
你只需传入输入大小

00:03:47.540 --> 00:03:48.770
和输出大小

00:03:48.770 --> 00:03:52.220
即 784 x 256

00:03:52.220 --> 00:03:54.590
我们需要 256 个输出

00:03:54.590 --> 00:03:58.105
是不是有点像对之前网络的重新构建

00:03:58.105 --> 00:04:04.115
同理 我们希望在隐藏单元和输出之间进行线性转换

00:04:04.115 --> 00:04:06.695
我们有 256 个隐藏单元

00:04:06.695 --> 00:04:09.690
有 10 个输出单元

00:04:09.690 --> 00:04:14.120
我们将创建一个输出层 叫做 self.output

00:04:14.120 --> 00:04:17.530
并创建这个线性转换操作

00:04:17.530 --> 00:04:21.470
还需要创建 S 型激活函数

00:04:21.470 --> 00:04:23.150
并为输出创建 softmax 函数

00:04:23.150 --> 00:04:25.135
从而获取概率分布

00:04:25.135 --> 00:04:28.815
现在创建 forward 方法

00:04:28.815 --> 00:04:31.110
forward 是指

00:04:31.110 --> 00:04:34.890
向网络里传入一个张量

00:04:34.890 --> 00:04:36.520
该张量将经过所有这些操作

00:04:36.520 --> 00:04:37.945
最终获得输出

00:04:37.945 --> 00:04:41.120
这里的参数 x 将为输入张量

00:04:41.120 --> 00:04:43.560
然后使其经过隐藏层

00:04:43.560 --> 00:04:47.095
这个和这里定义的线性转换一样

00:04:47.095 --> 00:04:50.510
它将经过 S 型激活函数

00:04:50.510 --> 00:04:55.665
然后经过输出层 即这里的输出线性转换

00:04:55.665 --> 00:04:57.924
然后经过 S 型函数

00:04:57.924 --> 00:05:02.060
最终返回 softmax 的输出

00:05:02.060 --> 00:05:03.725
创建这个网络

00:05:03.725 --> 00:05:07.140
输出网络 看看结果如何

00:05:07.140 --> 00:05:10.400
系统将显示各项操作 但不一定按顺序展示

00:05:10.400 --> 00:05:14.845
但至少会显示我们为这个网络定义的操作

00:05:14.845 --> 00:05:20.375
你还可以函数形式定义 S 型函数或 softmax

00:05:20.375 --> 00:05:24.950
这样会使该类/代码看起来更整洁

00:05:24.950 --> 00:05:27.785
我们可以使用 torch.nn.functional 模块

00:05:27.785 --> 00:05:32.690
大多数情况下 你会看到 import torch.nn.functional 设为大写的 F

00:05:32.690 --> 00:05:35.545
这是一种 PyTorch 代码编写惯例

00:05:35.545 --> 00:05:38.970
定义线性转换 self.hidden self.output

00:05:38.970 --> 00:05:43.335
但是现在在 forward 方法里

00:05:43.335 --> 00:05:47.910
我们可以调用 self.hidden 以获取隐藏层的值

00:05:47.910 --> 00:05:49.785
然后传入 S 型函数里 即 F.sigmoid

00:05:49.785 --> 00:05:53.310
输出层也一样

00:05:53.310 --> 00:05:56.990
创建输出线性转换

00:05:56.990 --> 00:05:59.510
传入 softmax 函数里

00:05:59.510 --> 00:06:01.865
能这么操作的原因是

00:06:01.865 --> 00:06:04.445
当我们创建这些线性转换时

00:06:04.445 --> 00:06:09.665
它们会自己创建权重和偏差矩阵

00:06:09.665 --> 00:06:11.870
但是对于 S 型函数和 softmax

00:06:11.870 --> 00:06:14.330
它们是元素级操作

00:06:14.330 --> 00:06:17.120
不需要创建额外的参数或额外的矩阵

00:06:17.120 --> 00:06:19.550
就能执行这些操作

00:06:19.550 --> 00:06:22.354
因此这些可以是纯函数定义

00:06:22.354 --> 00:06:25.700
无需创建任何对象或类

00:06:25.700 --> 00:06:28.385
但是它们是等同的

00:06:28.385 --> 00:06:33.440
这种网络构建方式和上面的效果一样

00:06:33.440 --> 00:06:36.260
但是采用这种函数格式

00:06:36.260 --> 00:06:39.490
使代码更简洁

00:06:39.490 --> 00:06:44.150
到目前为止 我们仅使用了 S 型函数作为激活函数

00:06:44.150 --> 00:06:45.365
但实际上

00:06:45.365 --> 00:06:47.660
有很多其他激活函数可以使用

00:06:47.660 --> 00:06:49.700
唯一的要求是

00:06:49.700 --> 00:06:53.300
这些激活函数应该是非线性的

00:06:53.300 --> 00:07:00.260
如果你希望网络能学习非线性关系和规律

00:07:00.260 --> 00:07:02.745
希望输出是非线性的

00:07:02.745 --> 00:07:07.010
那么你需要在隐藏层里使用非线性激活函数

00:07:07.010 --> 00:07:09.800
S 型函数就是一个示例

00:07:09.800 --> 00:07:12.065
双曲正切是另一种激活函数

00:07:12.065 --> 00:07:14.510
ReLU 是使用最频繁的激活函数

00:07:14.510 --> 00:07:17.880
它一直是隐藏层的激活函数

00:07:17.880 --> 00:07:20.670
ReLU 是线性修正单元的简称

00:07:20.670 --> 00:07:24.845
它是最简单的非线性函数

00:07:24.845 --> 00:07:28.330
与 S 型函数和双曲正切函数相比

00:07:28.330 --> 00:07:32.810
使用 ReLU 时网络的训练速度快多了

00:07:32.810 --> 00:07:35.465
因此我们通常使用 ReLU

00:07:35.465 --> 00:07:40.160
好的下面你的任务是构建更大型的神经网络

00:07:40.160 --> 00:07:43.170
这次网络将有两个隐藏层

00:07:43.170 --> 00:07:49.010
你将在隐藏层上使用 ReLU 激活函数

00:07:49.010 --> 00:07:55.520
请在 nn 模块里使用面向对象类方法

00:07:55.520 --> 00:07:59.135
构建这样的网络

00:07:59.135 --> 00:08:00.935
它有 784 个输入单元

00:08:00.935 --> 00:08:03.560
第一个隐藏层有 128 个单元

00:08:03.560 --> 00:08:05.930
第二个隐藏层有 64 个单元

00:08:05.930 --> 00:08:09.000
然后是 10 个输出单元加油！

