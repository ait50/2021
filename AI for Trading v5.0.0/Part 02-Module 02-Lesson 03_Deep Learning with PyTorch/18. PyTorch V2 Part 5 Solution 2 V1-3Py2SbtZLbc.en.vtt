WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.669
Hi. Here's my solution for your building and training this network using dropout now.

00:00:05.669 --> 00:00:06.974
Just like I showed you before,

00:00:06.974 --> 00:00:10.589
we can define our dropout module as

00:00:10.589 --> 00:00:14.984
self.dropout and then nn.dropout to give it some drop probability.

00:00:14.984 --> 00:00:16.320
So, in this case, 20 percent,

00:00:16.320 --> 00:00:20.894
and then just adding it to our forward method now on each of our hidden layers.

00:00:20.894 --> 00:00:23.960
Now, our validation code looks basically the same as

00:00:23.960 --> 00:00:26.750
before except now we're using model.eval.

00:00:26.750 --> 00:00:29.315
So, again, this turns our model into

00:00:29.315 --> 00:00:32.725
evaluation or inference mode which turns off dropout.

00:00:32.725 --> 00:00:34.890
Then, like the same way before,

00:00:34.890 --> 00:00:37.255
we just go through our data and the test say,

00:00:37.255 --> 00:00:40.820
calculate the losses and accuracy and after all that,

00:00:40.820 --> 00:00:44.465
we do model.train to set the model back into train mode,

00:00:44.465 --> 00:00:47.080
turn dropout back on,

00:00:47.079 --> 00:00:48.859
and then continue on in train smart.

00:00:48.859 --> 00:00:52.774
So, now, we're using dropout and if you look at

00:00:52.774 --> 00:00:57.460
again the training loss and the validation loss over these epochs that we're training,

00:00:57.460 --> 00:01:00.140
you actually see that the validation loss sticks a

00:01:00.140 --> 00:01:03.445
lot closer to the train loss as we train.

00:01:03.445 --> 00:01:07.805
So, here, with dropout, we've managed to at least reduce overfitting.

00:01:07.805 --> 00:01:15.395
So, the validation losses isn't as low as we got without dropout being is still,

00:01:15.394 --> 00:01:17.609
you can see that it's still dropping.

00:01:17.609 --> 00:01:19.840
So, if we kept training for longer,

00:01:19.840 --> 00:01:25.600
we would most likely manage to get our validation loss lower than without dropout.

