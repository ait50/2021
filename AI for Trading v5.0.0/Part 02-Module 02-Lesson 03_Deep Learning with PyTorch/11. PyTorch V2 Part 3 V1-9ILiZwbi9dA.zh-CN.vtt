WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.995
欢迎回来

00:00:01.995 --> 00:00:05.235
在此视频和 notebook 中

00:00:05.235 --> 00:00:09.195
我将演示如何在 PyTorch 里训练神经网络

00:00:09.195 --> 00:00:15.915
之前我们学习了如何使用 nn 模块在 PyTorch 中定义神经网络

00:00:15.915 --> 00:00:18.270
现在我们将学习

00:00:18.270 --> 00:00:20.610
如何训练网络

00:00:20.610 --> 00:00:22.800
训练是指我们将使用

00:00:22.800 --> 00:00:26.415
神经网络作为通用函数逼近器

00:00:26.415 --> 00:00:28.275
也就是说

00:00:28.275 --> 00:00:30.375
对于任何函数

00:00:30.375 --> 00:00:33.440
我们传入输入

00:00:33.440 --> 00:00:35.840
例如图像 4

00:00:35.840 --> 00:00:38.450
然后获得函数输出

00:00:38.450 --> 00:00:40.970
在这个例子里 输出就是概率分布

00:00:40.970 --> 00:00:43.865
告诉我们各个数字的概率

00:00:43.865 --> 00:00:45.185
在此例中

00:00:45.185 --> 00:00:46.610
如果我们传入图像 4

00:00:46.610 --> 00:00:49.250
我们希望获得一个概率分布

00:00:49.250 --> 00:00:52.240
其中数字 4 的概率最高

00:00:52.240 --> 00:00:54.590
神经网络的一个强大之处是

00:00:54.590 --> 00:00:56.885
如果使用非线性激活函数

00:00:56.885 --> 00:01:02.180
我们传入带有正确标签的正确图像数据集

00:01:02.180 --> 00:01:06.555
即传入一个图像、正确的输出、以及

00:01:06.555 --> 00:01:08.665
正确的标签/类别

00:01:08.665 --> 00:01:13.220
最终 神经网络将能够逼近此函数

00:01:13.220 --> 00:01:17.970
将这些图像变成这个概率分布 这就是我们的目标

00:01:17.970 --> 00:01:21.505
也就是说 我们将学习

00:01:21.505 --> 00:01:26.240
在 PyTorch 中如何构建神经网络并提供输入和输出

00:01:26.240 --> 00:01:31.315
然后调整网络权重 使其逼近该函数

00:01:31.315 --> 00:01:34.940
首先我们需要一个损失函数

00:01:34.940 --> 00:01:37.070
有时候也称为成本函数

00:01:37.070 --> 00:01:40.130
它可以衡量预测错误

00:01:40.130 --> 00:01:42.995
比如 我们传入图像 4

00:01:42.995 --> 00:01:46.510
网络预测为其他不正确的数字

00:01:46.510 --> 00:01:52.135
损失函数就是为了评估

00:01:52.135 --> 00:01:54.195
网络的错误预测与正确预测的偏差程度

00:01:54.195 --> 00:01:56.955
这里我们使用均方误差

00:01:56.955 --> 00:01:59.960
回归问题中经常使用均方误差

00:01:59.960 --> 00:02:04.430
但是在分类问题中使用其他损失函数 例如这个

00:02:04.430 --> 00:02:07.130
损失取决于

00:02:07.130 --> 00:02:11.060
网络的输出或预测

00:02:11.060 --> 00:02:14.080
网络的输出取决于权重

00:02:14.080 --> 00:02:15.785
即网络参数

00:02:15.785 --> 00:02:20.784
我们可以调整权重 使损失最小化

00:02:20.784 --> 00:02:22.755
最小化损失后

00:02:22.755 --> 00:02:26.730
表明网络预测已尽可能达到最佳效果

00:02:26.730 --> 00:02:33.140
这就是整个目标 即调整网络参数以最小化损失

00:02:33.140 --> 00:02:36.350
方法是使用梯度下降法

00:02:36.350 --> 00:02:42.040
梯度是损失函数相对于参数的斜率

00:02:42.040 --> 00:02:45.380
梯度始终指向变化最快的方向

00:02:45.380 --> 00:02:47.180
以山峰为例

00:02:47.180 --> 00:02:49.610
梯度将始终指向上山方向

00:02:49.610 --> 00:02:52.820
假设这座山峰就是损失函数

00:02:52.820 --> 00:02:56.360
上面是很高的损失 下面是很低的损失

00:02:56.360 --> 00:03:01.415
我们希望达到最低损失

00:03:01.415 --> 00:03:02.825
因此需要朝着下山方向移动

00:03:02.825 --> 00:03:05.480
梯度朝着向上的方向

00:03:05.480 --> 00:03:07.100
我们就沿着相反的方向移动

00:03:07.100 --> 00:03:09.335
即沿着负梯度方向

00:03:09.335 --> 00:03:11.540
如果不断向下移动

00:03:11.540 --> 00:03:17.085
最终会抵达这座山的最底部 即达到最低损失

00:03:17.085 --> 00:03:18.680
对于多层神经网络

00:03:18.680 --> 00:03:21.395
我们将使用一种叫做反向传播的算法

00:03:21.395 --> 00:03:26.015
反向传播是微积分中的链式法则的一种应用

00:03:26.015 --> 00:03:29.835
我们向网络中

00:03:29.835 --> 00:03:31.465
传入一些输入数据

00:03:31.465 --> 00:03:35.540
使数据在网络中前向传播 并计算损失

00:03:35.540 --> 00:03:37.325
传入一些数据

00:03:37.325 --> 00:03:39.710
即输入特征 x

00:03:39.710 --> 00:03:43.475
经过这个线性转换 该转换取决于权重和偏差

00:03:43.475 --> 00:03:46.490
然后经过激活函数 例如 S 型函数

00:03:46.490 --> 00:03:49.820
再经过具有其他权重和偏差的另一个线性转换

00:03:49.820 --> 00:03:50.930
然后

00:03:50.930 --> 00:03:52.430
计算损失

00:03:52.430 --> 00:03:55.985
如果对这里的权重 w1 进行小小的改动

00:03:55.985 --> 00:03:58.910
改动将在网络里传播

00:03:58.910 --> 00:04:02.645
最终导致损失有小小的变化

00:04:02.645 --> 00:04:05.330
可以将这个流程看做链式变化

00:04:05.330 --> 00:04:07.850
如果更改这个 这个将改变

00:04:07.850 --> 00:04:09.740
变化传播到这里

00:04:09.740 --> 00:04:12.130
传播到这里 再传播到这里

00:04:12.130 --> 00:04:15.860
对于反向传播来说 我们将使用相同的变化

00:04:15.860 --> 00:04:17.830
但是朝着相反的方向传播

00:04:17.830 --> 00:04:20.660
对于每个操作

00:04:20.660 --> 00:04:23.600
例如损失 线性转换 S 型激活函数

00:04:23.600 --> 00:04:25.940
始终存在某个导数

00:04:25.940 --> 00:04:29.450
在输出和输入之间存在梯度

00:04:29.450 --> 00:04:32.150
我们对每个操作求导

00:04:32.150 --> 00:04:35.930
然后使导数在网络里反向传播

00:04:35.930 --> 00:04:42.040
在每一步 我们将传入梯度乘以操作本身的梯度

00:04:42.040 --> 00:04:45.650
从末尾（损失）开始

00:04:45.650 --> 00:04:48.740
传入这个梯度 即 ∂l/∂L2

00:04:48.740 --> 00:04:53.510
这是损失相对于第二个线性转换的梯度

00:04:53.510 --> 00:05:03.335
再次反向传播 使其乘以 L2 的损失

00:05:03.335 --> 00:05:06.170
这是线性转换相对于

00:05:06.170 --> 00:05:08.840
激活函数输出的导数

00:05:08.840 --> 00:05:10.975
得出这个操作的梯度

00:05:10.975 --> 00:05:16.625
将这个梯度乘以损失的梯度

00:05:16.625 --> 00:05:18.600
得出这两个部分的总梯度

00:05:18.600 --> 00:05:21.865
这个梯度可以往回传递给这个 softmax 函数

00:05:21.865 --> 00:05:25.845
这就是反向传播的一般流程

00:05:25.845 --> 00:05:28.745
我们将梯度反向传递给上一个操作

00:05:28.745 --> 00:05:30.110
使其乘以这里的梯度

00:05:30.110 --> 00:05:31.880
然后再反向传播总梯度

00:05:31.880 --> 00:05:35.630
我们继续针对网络中的每个操作执行这一流程

00:05:35.630 --> 00:05:38.165
最终将回到权重部分

00:05:38.165 --> 00:05:40.985
使我们能够计算

00:05:40.985 --> 00:05:44.480
损失相对于这些权重的梯度

00:05:44.480 --> 00:05:46.160
正如我之前提到的

00:05:46.160 --> 00:05:52.590
梯度指向损失变化最快的方向

00:05:52.590 --> 00:05:53.895
因此最大化它

00:05:53.895 --> 00:05:55.485
如果要最小化损失

00:05:55.485 --> 00:05:59.820
我们可以用权重减去梯度

00:05:59.820 --> 00:06:03.110
这样会生成一组新的权重

00:06:03.110 --> 00:06:06.965
使损失变得更小

00:06:06.965 --> 00:06:09.770
反向传播算法的工作原理是

00:06:09.770 --> 00:06:12.925
在网络中前向传播一次 计算损失

00:06:12.925 --> 00:06:14.675
获得损失后

00:06:14.675 --> 00:06:17.030
我们可以在网络中反向传播并计算梯度

00:06:17.030 --> 00:06:19.175
获得权重的梯度

00:06:19.175 --> 00:06:20.485
然后更新权重

00:06:20.485 --> 00:06:21.660
再次前向传播

00:06:21.660 --> 00:06:24.495
计算损失 再次反向传播 更新权重

00:06:24.495 --> 00:06:25.935
不断重复

00:06:25.935 --> 00:06:29.520
直到损失足够小

00:06:29.520 --> 00:06:32.150
获得梯度后 

00:06:32.150 --> 00:06:34.220
我们可以用权重减去梯度

00:06:34.220 --> 00:06:38.770
我们还在这里使用了 α 表示学习速率

00:06:38.770 --> 00:06:42.110
学习速率可以用来调整梯度

00:06:42.110 --> 00:06:45.864
确保在迭代过程中步长不会太大

00:06:45.864 --> 00:06:48.840
如果更新步长太大

00:06:48.840 --> 00:06:51.530
可能会在最低值周围浮动

00:06:51.530 --> 00:06:55.250
但始终不会达到最低损失值

00:06:55.250 --> 00:07:00.010
我们来看看如何在 PyTorch 中计算损失

00:07:00.010 --> 00:07:02.280
同样使用 nn 模块

00:07:02.280 --> 00:07:06.905
PyTorch 提供了很多不同的损失 包括交叉熵损失

00:07:06.905 --> 00:07:12.265
在分类问题中 我们通常使用这个损失

00:07:12.265 --> 00:07:17.480
在 PyTorch 中 我们会将损失赋值给这个变量 criterion

00:07:17.480 --> 00:07:19.805
如果要使用交叉熵损失

00:07:19.805 --> 00:07:25.060
直接输入 criterion = nn.CrossEntropyLoss 创建这个类

00:07:25.060 --> 00:07:26.690
需要注意的一点是

00:07:26.690 --> 00:07:30.485
打开交叉熵损失的文档会发现

00:07:30.485 --> 00:07:34.730
交叉熵损失的输入

00:07:34.730 --> 00:07:39.060
应该是网络分数/对数

00:07:39.060 --> 00:07:43.310
所以 要使用 softmax 等输出

00:07:43.310 --> 00:07:46.405
softmax 可以提供正常的概率分布

00:07:46.405 --> 00:07:48.975
但是出于计算原因

00:07:48.975 --> 00:07:51.680
通常使用对数作为该损失函数的输入更合适

00:07:51.680 --> 00:07:55.705
对数是 softmax 函数的输入

00:07:55.705 --> 00:07:58.835
因此输入应该是每个类别的分数

00:07:58.835 --> 00:08:02.240
而不是概率本身

00:08:02.240 --> 00:08:06.500
首先我将导入必要的模块

00:08:06.500 --> 00:08:11.370
并下载数据

00:08:11.370 --> 00:08:13.410
然后创建 trainloader 

00:08:13.410 --> 00:08:15.110
然后从中获取数据

00:08:15.110 --> 00:08:17.165
我在这里定义一个模型

00:08:17.165 --> 00:08:19.910
我使用的是 nn.Sequential 如果你不熟悉这个函数

00:08:19.910 --> 00:08:21.860
请参阅上个 notebook 的结尾内容

00:08:21.860 --> 00:08:23.140
Part 2 notebook 里

00:08:23.140 --> 00:08:25.845
将演示如何使用 nn.Sequential

00:08:25.845 --> 00:08:31.300
它可以简洁地定义简单前馈网络

00:08:31.300 --> 00:08:34.730
我在这里仅返回对数

00:08:34.730 --> 00:08:41.250
即输出函数的分数 而不是 softmax 输出本身

00:08:41.250 --> 00:08:42.780
在这里定义损失

00:08:42.780 --> 00:08:45.900
输入 criterions = nn.CrossEntropyLoss

00:08:45.900 --> 00:08:48.140
获取图像和标签数据

00:08:48.140 --> 00:08:51.710
扁平化图像 传入模型中并获得对数

00:08:51.710 --> 00:08:57.995
通过传入对数和真实标签获得实际损失

00:08:57.995 --> 00:09:00.710
同样 从 trainloader 中获取标签

00:09:00.710 --> 00:09:03.920
运行下代码 可以看出 我们计算出了损失

00:09:03.920 --> 00:09:06.995
使用 log-softmax 输出构建模型

00:09:06.995 --> 00:09:10.975
比使用普通的 softmax 更方便

00:09:10.975 --> 00:09:13.860
对于 log-softmax 输出 要获得实际概率

00:09:13.860 --> 00:09:17.250
直接传入 torch.exp计算指数

00:09:17.250 --> 00:09:19.280
对于 log-softmax 输出

00:09:19.280 --> 00:09:24.340
需要使用负对数似然损失或 nn.NLLLoss

00:09:24.340 --> 00:09:26.780
下面你的任务是

00:09:26.780 --> 00:09:29.900
构建一个模型并返回 log-softmax 作为输出

00:09:29.900 --> 00:09:33.980
使用负对数似然损失计算损失

00:09:33.980 --> 00:09:36.095
在使用 log-softmax 时

00:09:36.095 --> 00:09:40.040
要注意关键字参数 dim

00:09:40.040 --> 00:09:45.200
确保正确设置该参数 才能获得符合期望的输出

00:09:45.200 --> 00:09:49.550
试试吧

00:09:49.550 --> 00:09:51.455
如果遇到问题

00:09:51.455 --> 00:09:53.730
可以在 notebook 和下个视频中查看我的答案加油！

