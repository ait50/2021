WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.740
大家好！欢迎学习“深度学习工具 PyTorch”课程

00:00:04.740 --> 00:00:07.530
在这节课 我将演示

00:00:07.530 --> 00:00:11.400
如何使用 PyTorch 构建神经网络并训练网络

00:00:11.400 --> 00:00:14.220
在 notebook 里学习并练习完之后

00:00:14.220 --> 00:00:18.915
你将自己编写代码来构建网络

00:00:18.915 --> 00:00:21.795
这节课的目标是

00:00:21.795 --> 00:00:25.125
构建一个图像分类器

00:00:25.125 --> 00:00:27.060
但首先我们会学习基础知识

00:00:27.060 --> 00:00:31.605
那么 如何在 PyTorch 中构建一个简单的神经网络？

00:00:31.605 --> 00:00:34.905
回顾下神经网络的工作原理

00:00:34.905 --> 00:00:38.470
这里有一些输入值 例如 x1 x2

00:00:38.470 --> 00:00:43.460
我们用它们乘以权重 w 和偏差

00:00:43.460 --> 00:00:47.645
这个 b 表示偏差 我们乘以 1

00:00:47.645 --> 00:00:52.220
将这几个值相加得出 h 然后是激活函数

00:00:52.220 --> 00:00:54.875
即这里的 f(h)

00:00:54.875 --> 00:00:59.830
将这些输入值之和 h 传入激活函数 得出输出 y

00:00:59.830 --> 00:01:02.930
这就是神经网络的基本原理

00:01:02.930 --> 00:01:04.070
这些是输入

00:01:04.070 --> 00:01:06.125
乘以某些权重

00:01:06.125 --> 00:01:10.435
传入激活函数 得出输出

00:01:10.435 --> 00:01:13.880
你可以继续堆叠这个结构

00:01:13.880 --> 00:01:18.250
将这些单元（神经元）的输出传入采用另一组权重的另一个层级

00:01:18.250 --> 00:01:20.835
数学公式是这样的

00:01:20.835 --> 00:01:26.960
输出 y 等于权重 w 和输入值 x

00:01:26.960 --> 00:01:30.320
及偏差值 b 的线性组合

00:01:30.320 --> 00:01:34.070
然后放入激活函数 f 里并得出 y

00:01:34.070 --> 00:01:36.110
也可以写成这样的求和公式

00:01:36.110 --> 00:01:40.510
w^i 乘以 x^i 的和加上偏差项 b

00:01:40.510 --> 00:01:42.345
得出 y

00:01:42.345 --> 00:01:46.640
这个公式的好处是

00:01:46.640 --> 00:01:49.520
可以将输入特征（值）x 当做向量

00:01:49.520 --> 00:01:52.685
并将权重当做另一个向量

00:01:52.685 --> 00:01:58.800
因此这里的相乘求和与两个向量的点积/内积是相同的

00:01:58.800 --> 00:02:01.865
如果将输入和权重当做向量

00:02:01.865 --> 00:02:03.860
可以对这两个向量求点积

00:02:03.860 --> 00:02:06.785
然后得出值 h

00:02:06.785 --> 00:02:09.860
并将 h 传入激活函数里 得出输出 y

00:02:09.860 --> 00:02:16.430
现在我们将权重和输入值当做向量

00:02:16.430 --> 00:02:20.330
向量是一种张量

00:02:20.330 --> 00:02:24.350
张量是向量和矩阵的泛化形式

00:02:24.350 --> 00:02:27.950
这些是普通的数据结构

00:02:27.950 --> 00:02:32.285
一维张量是向量

00:02:32.285 --> 00:02:37.400
这是一个一维值数组

00:02:37.400 --> 00:02:42.665
这里是字符 ‘t’ ‘e’ ‘n’ ‘s’ ‘o’ ‘r’

00:02:42.665 --> 00:02:46.370
像这样的矩阵是二维张量

00:02:46.370 --> 00:02:48.830
这些值按从左到右和从上到下这两个方向排列

00:02:48.830 --> 00:02:51.620
它们分别是行和列

00:02:51.620 --> 00:02:54.500
你可以沿着一行对列执行操作

00:02:54.500 --> 00:02:58.490
或沿着一列对行执行操作

00:02:58.490 --> 00:03:01.745
这个是三维张量

00:03:01.745 --> 00:03:05.990
可以看做三维的 RGB 彩色图像

00:03:05.990 --> 00:03:07.370
对于每个像素

00:03:07.370 --> 00:03:10.490
所有的红绿蓝通道

00:03:10.490 --> 00:03:13.765
都有某个值

00:03:13.765 --> 00:03:15.630
因此在二维图像中

00:03:15.630 --> 00:03:17.070
每个像素都有三个值

00:03:17.070 --> 00:03:19.155
这就是三维张量

00:03:19.155 --> 00:03:21.800
正如我刚刚提到的 张量是一种泛化形式

00:03:21.800 --> 00:03:24.379
因此可以有四维

00:03:24.379 --> 00:03:27.710
五维 六维张量等等

00:03:27.710 --> 00:03:29.930
但是我们通常处理的是

00:03:29.930 --> 00:03:32.800
一维 二维和三维张量

00:03:32.800 --> 00:03:36.695
这些张量是

00:03:36.695 --> 00:03:40.715
pyTorch 和其他神经网络框架中的基本数据结构

00:03:40.715 --> 00:03:43.760
TensorFlow 就是根据张量命名的

00:03:43.760 --> 00:03:46.580
这些就是你要使用的基本数据结构

00:03:46.580 --> 00:03:49.520
你必须熟练掌握它们

00:03:49.520 --> 00:03:52.310
才能够使用

00:03:52.310 --> 00:03:55.865
深度学习框架

00:03:55.865 --> 00:03:59.690
我们开始吧我将演示如何创建张量

00:03:59.690 --> 00:04:03.605
并使用它们构建简单的神经网络

00:04:03.605 --> 00:04:07.565
首先 我们将导入 PyTorch 在这里输入 import torch

00:04:07.565 --> 00:04:09.770
我在这里创建了激活函数

00:04:09.770 --> 00:04:12.355
这是 S 型激活函数

00:04:12.355 --> 00:04:17.880
它是一个完美的 S 型图形 将输入值压缩到了 0 和 1 之间

00:04:17.880 --> 00:04:21.065
非常适合提供概率值

00:04:21.065 --> 00:04:25.195
概率值只能位于 0 到 1 之间

00:04:25.195 --> 00:04:27.770
因此 如果你希望

00:04:27.770 --> 00:04:30.560
神经网络的输出为概率

00:04:30.560 --> 00:04:33.060
则建议使用 S 型激活函数

00:04:33.060 --> 00:04:38.120
我将创建一些虚拟数据 生成一些权重和偏差

00:04:38.120 --> 00:04:41.240
你将使用这些虚拟值进行计算

00:04:41.240 --> 00:04:44.815
并获得简单神经网络的输出

00:04:44.815 --> 00:04:47.520
在这里创建 manual_seed()

00:04:47.520 --> 00:04:50.300
设置将要使用的随机数

00:04:50.300 --> 00:04:53.255
在这里创建特征

00:04:53.255 --> 00:04:57.605
特征是指网络的输入特征/输入数据

00:04:57.605 --> 00:05:00.150
这里是 torch.randn()

00:05:00.150 --> 00:05:04.910
randn 将创建由正态变量组成的张量

00:05:04.910 --> 00:05:08.180
即来自正态分布的随机正态变量

00:05:08.180 --> 00:05:11.250
指定元组大小

00:05:11.250 --> 00:05:15.605
这里我希望特征是一个矩阵

00:05:15.605 --> 00:05:20.675
即包含 1 行和 5 列的二维张量

00:05:20.675 --> 00:05:25.800
可以将它看做有 5 个元素的行向量

00:05:25.800 --> 00:05:28.610
对于矩阵 我们将创建另一个矩阵

00:05:28.610 --> 00:05:33.270
并使用随机正态变量 这次我将使用 randn_like

00:05:33.270 --> 00:05:35.990
它的原理是

00:05:35.990 --> 00:05:39.260
传入一个张量并查看该张量的形状

00:05:39.260 --> 00:05:41.330
然后创建形状相同的另一个张量

00:05:41.330 --> 00:05:42.770
这就是这个“like”的含义

00:05:42.770 --> 00:05:44.930
我将使用随机正态变量创建一个张量

00:05:44.930 --> 00:05:50.115
形状和 features 张量的一样这就是权重

00:05:50.115 --> 00:05:53.115
然后我将创建偏差项

00:05:53.115 --> 00:05:56.240
还是随机正态变量

00:05:56.240 --> 00:05:58.700
现在我只创建一个值

00:05:58.700 --> 00:06:00.715
这是 1 行和 1 列

00:06:00.715 --> 00:06:03.350
现在布置一道练习题

00:06:03.350 --> 00:06:06.050
你需要根据特征 权重

00:06:06.050 --> 00:06:08.450
和偏差张量

00:06:08.450 --> 00:06:11.030
计算这个简单神经网络的输出

00:06:11.030 --> 00:06:13.610
注意 你需要对特征和权重求内积

00:06:13.610 --> 00:06:17.390
即将特征和权重相乘并求和

00:06:17.390 --> 00:06:20.780
然后加上偏差项

00:06:20.780 --> 00:06:26.000
最后传入激活函数 得出网络的输出

00:06:26.000 --> 00:06:27.680
如果你想查看我是如何实现的

00:06:27.680 --> 00:06:30.380
请参阅我的 solution notebook

00:06:30.380 --> 00:06:34.130
或者观看下个视频 我会讲解这道练习的答案

