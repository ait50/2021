WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.540
下面说说我是如何

00:00:03.540 --> 00:00:08.190
计算这个小型简单神经网络输出的

00:00:08.190 --> 00:00:12.330
我们需要将 features 与 weights 相乘

00:00:12.330 --> 00:00:14.070
即 features * weights

00:00:14.070 --> 00:00:18.225
它们是张量 基本上和 NumPy 数组一样

00:00:18.225 --> 00:00:19.845
如果你之前用过 NumPy   应该就会很熟悉

00:00:19.845 --> 00:00:22.770
features 与 weights 相乘时

00:00:22.770 --> 00:00:25.320
两者里的第一个元素互相相乘

00:00:25.320 --> 00:00:27.420
然后是两者里的第二个元素互相相乘

00:00:27.420 --> 00:00:29.895
依次进行 

00:00:29.895 --> 00:00:33.060
得出新的张量

00:00:33.060 --> 00:00:37.020
然后运行 torch.sum 得出一个和

00:00:37.020 --> 00:00:41.100
加上偏差项 传入激活函数并得出 y

00:00:41.100 --> 00:00:44.720
再介绍一种方法 同样是 features * weights

00:00:44.720 --> 00:00:46.295
创建另一个张量

00:00:46.295 --> 00:00:48.710
而张量有一个 .sum 方法

00:00:48.710 --> 00:00:53.645
你可以传入张量   运行 .sum    它会对该张量里的所有值求和

00:00:53.645 --> 00:00:56.550
因此 我们可以运行 torch.sum

00:00:56.550 --> 00:00:58.605
或者采用此方法

00:00:58.605 --> 00:01:01.430
对张量运行 .sum 并求和

00:01:01.430 --> 00:01:04.645
同样 传入激活函数

00:01:04.645 --> 00:01:06.440
这部分的作用是

00:01:06.440 --> 00:01:11.080
用两种方式进行元素级乘法运算并求和

00:01:11.080 --> 00:01:13.180
相乘 然后求和

00:01:13.180 --> 00:01:17.420
但实际上可以使用矩阵乘法进行相同的运算

00:01:17.420 --> 00:01:19.520
大多数情况下

00:01:19.520 --> 00:01:22.070
都建议使用矩阵乘法

00:01:22.070 --> 00:01:24.365
因为矩阵乘法更高效

00:01:24.365 --> 00:01:30.145
这些线性代数运算已经使用现代库加快了速度

00:01:30.145 --> 00:01:32.985
例如在 GPU 上运行的 CUDA

00:01:32.985 --> 00:01:37.550
要在 PyTorch 中对 features 和 weights 这两个张量进行矩阵乘法运算

00:01:37.550 --> 00:01:40.310
我们可以采用两种方法

00:01:40.310 --> 00:01:44.875
分别是 torch.mm 和 torch.matmul

00:01:44.875 --> 00:01:48.590
对于 torch.mm  矩阵乘法更简单

00:01:48.590 --> 00:01:52.760
并且对传入的张量要求更严格

00:01:52.760 --> 00:01:56.420
对于 torch.matmul 它实际上支持广播

00:01:56.420 --> 00:02:02.029
如果你传入大小/形状很奇怪的张量

00:02:02.029 --> 00:02:05.665
那么你可能会获得意料之外的输出结果

00:02:05.665 --> 00:02:09.495
因此我更愿意使用 torch.mm

00:02:09.495 --> 00:02:12.520
它能按照我的预期方式进行运算

00:02:12.520 --> 00:02:15.710
如果我操作错了 它就报错

00:02:15.710 --> 00:02:19.085
而不是继续运算

00:02:19.085 --> 00:02:21.665
但是 如果我们尝试

00:02:21.665 --> 00:02:25.175
对 features 和 weights 使用 torch.mm  就会出错

00:02:25.175 --> 00:02:28.975
这里显示 RuntimeError: size mismatch

00:02:28.975 --> 00:02:33.485
表示我们向 torch.mm 传入了两个张量

00:02:33.485 --> 00:02:35.810
但是大小不匹配 它不能进行矩阵乘法运算

00:02:35.810 --> 00:02:39.815
并在这里列出了大小

00:02:39.815 --> 00:02:42.155
第一个张量

00:02:42.155 --> 00:02:45.905
m1 是 [1 x 5] 第二个张量也是 [1 x 5]

00:02:45.905 --> 00:02:51.360
如果你还记得线性代数课程的知识

00:02:51.360 --> 00:02:53.425
就会发现在进行矩阵乘法运算时

00:02:53.425 --> 00:02:56.690
第一个矩阵的列数

00:02:56.690 --> 00:03:00.520
必须等于第二个矩阵的行数

00:03:00.520 --> 00:03:04.245
因此 weights 张量

00:03:04.245 --> 00:03:07.765
必须是 [5 x 1] 而不是 [1 x 5]

00:03:07.765 --> 00:03:11.339
在构建神经网络时

00:03:11.339 --> 00:03:13.365
如果要查看张量的形状

00:03:13.365 --> 00:03:15.210
你可以使用 tensor.shape

00:03:15.210 --> 00:03:18.500
我们以后会频繁用到这个方法 不仅在 PyTorch 里

00:03:18.500 --> 00:03:22.890
在 TensorFlow 和其他深度学习框架中也不例外

00:03:22.890 --> 00:03:25.760
在构建神经网络时 最常见的错误就是形状错误

00:03:25.760 --> 00:03:28.910
所以 在设计神经网络架构时

00:03:28.910 --> 00:03:32.240
很重要的步骤就是

00:03:32.240 --> 00:03:35.780
使张量的形状保持匹配

00:03:35.780 --> 00:03:38.014
也就是说 在调试过程中

00:03:38.014 --> 00:03:39.980
很大一部分工作是

00:03:39.980 --> 00:03:42.635
查看传入网络中的张量的形状

00:03:42.635 --> 00:03:45.170
记得用 tensor.shape()

00:03:45.170 --> 00:03:47.690
要变形张量

00:03:47.690 --> 00:03:49.790
通常可以采用

00:03:49.790 --> 00:03:51.860
三种不同的方法

00:03:51.860 --> 00:03:53.045
也就是这三种方法

00:03:53.045 --> 00:03:56.075
Reshape() resize() 和 view()

00:03:56.075 --> 00:03:58.205
原理基本相通 即

00:03:58.205 --> 00:04:01.105
对于 weights.reshape

00:04:01.105 --> 00:04:04.805
传入想要的新形状

00:04:04.805 --> 00:04:05.930
在这个练习中

00:04:05.930 --> 00:04:08.900
我们要将 weights 变成 5x1 的矩阵

00:04:08.900 --> 00:04:11.630
输入 .reshape(5, 1)

00:04:11.630 --> 00:04:15.320
reshape 的作用是

00:04:15.320 --> 00:04:18.750
返回一个新的张量 其中的数据和 weights 的一样

00:04:18.750 --> 00:04:22.595
即在内存地址中存储的相同数据

00:04:22.595 --> 00:04:24.275
它将创建一个新的张量

00:04:24.275 --> 00:04:27.775
形状是你要求的形状

00:04:27.775 --> 00:04:31.775
但内存中的实际数据没有改变

00:04:31.775 --> 00:04:33.740
这种情况有时候会发生

00:04:33.740 --> 00:04:38.570
有时候 它会返回克隆版本

00:04:38.570 --> 00:04:41.090
也就是说 它将数据复制到内存的另一个部分

00:04:41.090 --> 00:04:44.215
然后返回在此内存部分存储的张量

00:04:44.215 --> 00:04:47.050
也就是说

00:04:47.050 --> 00:04:50.080
复制数据比直接更改张量形状（不克隆数据）

00:04:50.080 --> 00:04:54.080
效率要低

00:04:54.080 --> 00:04:55.260
要这么做

00:04:55.260 --> 00:04:58.050
我们可以使用 resize_   末尾有个下划线

00:04:58.050 --> 00:05:02.884
下划线表示这个方法是原地操作

00:05:02.884 --> 00:05:04.235
原地操作是指

00:05:04.235 --> 00:05:06.220
根本不更改数据

00:05:06.220 --> 00:05:08.140
只是更改

00:05:08.140 --> 00:05:12.575
位于该内存地址中的数据对应的张量

00:05:12.575 --> 00:05:16.435
resize_ 方法的问题在于

00:05:16.435 --> 00:05:20.560
如果请求的形状比原始张量的元素多或少

00:05:20.560 --> 00:05:23.470
你可能会丢失数据

00:05:23.470 --> 00:05:26.305
或者使用未初始化的内存

00:05:26.305 --> 00:05:30.005
创建虚假的数据

00:05:30.005 --> 00:05:32.510
通常这时候

00:05:32.510 --> 00:05:35.780
如果有一个方法 

00:05:35.780 --> 00:05:38.030
能在你把原始数量的元素更改为不同数量的元素的时候

00:05:38.030 --> 00:05:41.555
进行报错 是不是很好

00:05:41.555 --> 00:05:43.645
我们其实有这么个方法 就是 .view

00:05:43.645 --> 00:05:46.870
.view 是我频繁使用的一个方法

00:05:46.870 --> 00:05:49.535
它会返回一个新张量

00:05:49.535 --> 00:05:52.985
包含的数据和 weights 在内存中的一样

00:05:52.985 --> 00:05:55.345
对的

00:05:55.345 --> 00:05:57.620
它只是返回一个新的张量

00:05:57.620 --> 00:06:00.155
不会更改内存中的任何数据

00:06:00.155 --> 00:06:03.755
如果你想获取新的尺寸

00:06:03.755 --> 00:06:08.250
使张量具有新的形状和不同数量的元素

00:06:08.250 --> 00:06:09.885
就会报错

00:06:09.885 --> 00:06:12.910
使用 .view 可以确保

00:06:12.910 --> 00:06:15.035
在更改 weights 的形状时

00:06:15.035 --> 00:06:17.905
始终获得相同数量的元素

00:06:17.905 --> 00:06:21.740
这就是我在变形张量时 会用到这个方法的原因

00:06:21.740 --> 00:06:23.555
好了 总结下

00:06:23.555 --> 00:06:27.110
如果你要将 weights 变形为 5 行和 1 列

00:06:27.110 --> 00:06:30.420
可以输入 weights.view(5, 1)

00:06:30.420 --> 00:06:33.155
现在 你已经知道如何改变张量的形状

00:06:33.155 --> 00:06:37.280
以及如何进行矩阵乘法运算

00:06:37.280 --> 00:06:39.830
我希望你能使用矩阵乘法

00:06:39.830 --> 00:06:43.330
计算这个小型神经网络的输出

