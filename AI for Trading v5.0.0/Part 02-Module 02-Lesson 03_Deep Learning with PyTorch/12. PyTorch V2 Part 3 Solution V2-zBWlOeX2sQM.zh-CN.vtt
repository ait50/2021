WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:05.955
欢迎回来下面介绍如何创建使用 LogSoftmax 输出的模型

00:00:05.955 --> 00:00:09.915
和之前使用 nn.sequential 创建的模型很像

00:00:09.915 --> 00:00:12.360
网络层级是线性转换、

00:00:12.360 --> 00:00:14.160
ReLU、线性转换 、ReLU、

00:00:14.160 --> 00:00:16.860
线性转换

00:00:16.860 --> 00:00:20.055
然后将这个传入 LogSoftmax 模块

00:00:20.055 --> 00:00:22.410
要注意

00:00:22.410 --> 00:00:25.500
将 LogSoftmax 里的维度设为 1

00:00:25.500 --> 00:00:31.215
它就会针对列计算结果 而不是行

00:00:31.215 --> 00:00:34.970
行对应的是样本

00:00:34.970 --> 00:00:37.640
我们将一批样本传入网络中

00:00:37.640 --> 00:00:40.750
每行都是一个样本

00:00:40.750 --> 00:00:42.800
要注意

00:00:42.800 --> 00:00:46.160
对每个样本计算 softmax 函数

00:00:46.160 --> 00:00:52.240
而不是针对批次中的每个特征计算该函数

00:00:52.240 --> 00:00:56.060
在这里将损失/条件

00:00:56.060 --> 00:00:58.490
定义为负对数似然损失

00:00:58.490 --> 00:01:01.250
从 trainloader 获取图像和标签

00:01:01.250 --> 00:01:04.310
扁平化图像 传入模型中以获得对数

00:01:04.310 --> 00:01:07.380
这里不再是对数

00:01:07.380 --> 00:01:09.844
而是对数概率

00:01:09.844 --> 00:01:11.450
称为 logps

00:01:11.450 --> 00:01:14.345
这里也更改下

00:01:14.345 --> 00:01:17.320
好了我们计算出了损失

00:01:17.320 --> 00:01:19.969
现在我们知道如何计算损失了

00:01:19.969 --> 00:01:23.195
但是如何用损失进行反向传播呢？

00:01:23.195 --> 00:01:26.090
PyTorch 提供了一个非常有用的模块 叫做 Autograd

00:01:26.090 --> 00:01:30.460
它可以自动计算张量的梯度

00:01:30.460 --> 00:01:32.550
它的工作原理是

00:01:32.550 --> 00:01:35.210
PyTorch 将跟踪你对张量执行的所有操作

00:01:35.210 --> 00:01:39.635
然后你可以告诉它进行反向传播

00:01:39.635 --> 00:01:41.630
反向执行每个操作

00:01:41.630 --> 00:01:47.015
并计算相对于输入参数的梯度

00:01:47.015 --> 00:01:53.825
你需要告诉 PyTorch 你想对特定的张量使用 autograd

00:01:53.825 --> 00:01:59.025
例如 创建一个张量 x = torch.zeros

00:01:59.025 --> 00:02:00.590
将其设为标量 例如 1

00:02:00.590 --> 00:02:03.845
然后设置为 requires_grad = True

00:02:03.845 --> 00:02:09.620
这部分告诉 PyTorch 跟踪对张量 x 执行的操作

00:02:09.620 --> 00:02:13.540
如果你想获得梯度 它将为你计算梯度

00:02:13.540 --> 00:02:16.100
如果你创建了一个张量

00:02:16.100 --> 00:02:19.310
但是不想计算它的梯度

00:02:19.310 --> 00:02:21.790
则将这个设为 False

00:02:21.790 --> 00:02:26.195
还可以使用 torch.no_grad

00:02:26.195 --> 00:02:28.400
对在这个上下文中执行的所有操作

00:02:28.400 --> 00:02:31.255
关闭所有梯度

00:02:31.255 --> 00:02:35.000
还可以使用 torch.set_grad_enabled

00:02:35.000 --> 00:02:39.060
全局性地开启或关闭所有梯度 在这里设置 true 或 false 即可

00:02:39.060 --> 00:02:44.520
我们在 PyTorch 中创建张量

00:02:44.520 --> 00:02:49.290
设置 requires_grad = True 然后对该张量执行操作

00:02:49.290 --> 00:02:53.360
操作完毕后 输入 .backwards

00:02:53.360 --> 00:02:56.060
如果使用张量 x

00:02:56.060 --> 00:02:59.000
然后计算出其他张量 z 执行 z.backward

00:02:59.000 --> 00:03:04.595
它将反向经过操作并计算 x 的总梯度

00:03:04.595 --> 00:03:07.370
例如 我创建了这个随机张量

00:03:07.370 --> 00:03:12.320
随机的 2x2 张量 像这样变成方形

00:03:12.320 --> 00:03:15.890
如果你查看 y

00:03:15.890 --> 00:03:18.320
y 是第二个方形张量

00:03:18.320 --> 00:03:20.405
如果查看 y.grad_fn

00:03:20.405 --> 00:03:25.125
它会显示这个 grad 函数是一个幂

00:03:25.125 --> 00:03:28.340
PyTorch 跟踪了这个操作

00:03:28.340 --> 00:03:31.915
知道最后一个操作是幂运算

00:03:31.915 --> 00:03:36.335
现在我们可以对 y 取均值并获得另一个张量 z

00:03:36.335 --> 00:03:40.145
这是一个标量张量 我们缩减了 y

00:03:40.145 --> 00:03:42.710
y 是一个 2 x 2 矩阵/数组

00:03:42.710 --> 00:03:45.850
然后对其取均值 得出 z

00:03:45.850 --> 00:03:49.220
张量的梯度显示在属性 grad 中

00:03:49.220 --> 00:03:53.870
现在我们可以查看张量 x 的梯度

00:03:53.870 --> 00:03:56.330
我们仅进行了前向传播

00:03:56.330 --> 00:03:59.410
尚未计算梯度 因此为 None

00:03:59.410 --> 00:04:01.275
现在如果执行 z.backward

00:04:01.275 --> 00:04:05.550
它将反向经过我们执行的这小部分操作

00:04:05.550 --> 00:04:07.430
我们进行了幂运算和取均值运算

00:04:07.430 --> 00:04:10.795
我们反向传播并计算 x 的梯度

00:04:10.795 --> 00:04:12.740
结果你会发现

00:04:12.740 --> 00:04:15.290
z 相对于 x 的梯度应该是 x/2

00:04:15.290 --> 00:04:18.380
查看梯度

00:04:18.380 --> 00:04:21.680
看看 x/2 它们是一样的

00:04:21.680 --> 00:04:26.435
从数学角度来看 梯度等于预期结果

00:04:26.435 --> 00:04:29.210
这就是在 autograd 和 PyTorch 中

00:04:29.210 --> 00:04:31.430
使用梯度的一般流程

00:04:31.430 --> 00:04:32.810
在计算损失时 我们可以使用它计算梯度

00:04:32.810 --> 00:04:36.580
很有用 对不对

00:04:36.580 --> 00:04:42.350
损失取决于权重和偏差参数

00:04:42.350 --> 00:04:44.960
我们需要权重梯度来执行梯度下降法

00:04:44.960 --> 00:04:48.170
我们可以将权重设为需要梯度的张量

00:04:48.170 --> 00:04:52.960
然后进行前向传播以计算损失

00:04:52.960 --> 00:04:57.230
获得损失后 进行反向传播以计算权重的梯度

00:04:57.230 --> 00:04:59.990
获得这些梯度后 可以执行梯度下降步骤

00:04:59.990 --> 00:05:02.930
我来演示下代码

00:05:02.930 --> 00:05:07.535
像之前一样定义模型 这里是 LogSoftmax 输出

00:05:07.535 --> 00:05:10.220
使用负对数似然损失

00:05:10.220 --> 00:05:13.625
从 trainloader 中获取图像和标签 扁平化图像

00:05:13.625 --> 00:05:15.950
然后从模型中获得对数概率

00:05:15.950 --> 00:05:18.800
传入损失函数里

00:05:18.800 --> 00:05:20.930
获得实际损失

00:05:20.930 --> 00:05:23.750
现在查看模型权重

00:05:23.750 --> 00:05:29.600
Model[0] 对应的是第一个线性转换的参数

00:05:29.600 --> 00:05:31.715
查看权重梯度

00:05:31.715 --> 00:05:34.370
然后从损失开始进行反向传播

00:05:34.370 --> 00:05:38.260
再次查看权重梯度

00:05:38.260 --> 00:05:40.570
在反向传播之前

00:05:40.570 --> 00:05:42.695
没有任何梯度 这是因为我们尚未计算它

00:05:42.695 --> 00:05:45.425
但是在反向传播之后

00:05:45.425 --> 00:05:47.630
我们计算了梯度

00:05:47.630 --> 00:05:51.715
我们可以在梯度下降步骤中使用这些梯度训练网络

00:05:51.715 --> 00:05:54.770
现在你知道如何计算损失

00:05:54.770 --> 00:05:57.830
如何使用这些损失计算梯度

00:05:57.830 --> 00:06:00.980
但是在开始训练之前 还有一步

00:06:00.980 --> 00:06:04.850
你需要知道如何使用这些梯度更新权重

00:06:04.850 --> 00:06:09.230
接下来 我们将使用优化器 优化器来自 PyTorch 的 optim 软件包

00:06:09.230 --> 00:06:14.345
例如 我们可以通过输入 optim.SGD 使用随机梯度下降法

00:06:14.345 --> 00:06:18.184
定义方式是

00:06:18.184 --> 00:06:22.395
从 PyTorch 导入模块 optim 在这里输入 optim.SGD

00:06:22.395 --> 00:06:23.940
传入模型参数

00:06:23.940 --> 00:06:25.895
这些是我们希望优化器更新的参数

00:06:25.895 --> 00:06:29.855
然后设定学习速率

00:06:29.855 --> 00:06:32.245
这样就创建了优化器

00:06:32.245 --> 00:06:36.110
训练流程包含四个不同的步骤

00:06:36.110 --> 00:06:39.440
首先对网络进行前向传播

00:06:39.440 --> 00:06:43.220
然后使用该网络输出计算损失

00:06:43.220 --> 00:06:45.350
使用 loss.backwards

00:06:45.350 --> 00:06:48.540
对网络进行反向传播并计算梯度

00:06:48.540 --> 00:06:52.715
然后使用优化器更新权重

00:06:52.715 --> 00:06:56.090
我在这里只演示训练一步

00:06:56.090 --> 00:07:00.410
然后你将编写实际训练过程 循环地训练网络

00:07:00.410 --> 00:07:04.190
首先

00:07:04.190 --> 00:07:05.795
从 trainloader 获取图像和标签

00:07:05.795 --> 00:07:08.350
然后扁平化图像

00:07:08.350 --> 00:07:11.910
接下来清理梯度

00:07:11.910 --> 00:07:14.970
PyTorch 默认地会累积梯度

00:07:14.970 --> 00:07:18.185
如果你在网络中传播多次

00:07:18.185 --> 00:07:21.830
即 多次执行前向传播和反向传播

00:07:21.830 --> 00:07:24.630
并不断计算梯度

00:07:24.630 --> 00:07:27.100
PyTorch 将不断将这些梯度加起来

00:07:27.100 --> 00:07:29.850
如果不清理梯度

00:07:29.850 --> 00:07:33.680
那么在当前训练中

00:07:33.680 --> 00:07:36.020
会残留上个训练步骤的梯度

00:07:36.020 --> 00:07:38.725
导致网络无法正常训练

00:07:38.725 --> 00:07:40.290
因此在每个训练流程之前

00:07:40.290 --> 00:07:45.710
你需要调用 zero_grad()

00:07:45.710 --> 00:07:48.560
输入 optimizer.zero_grad

00:07:48.560 --> 00:07:52.120
它会在优化器里清理所有参数中的所有梯度

00:07:52.120 --> 00:07:54.480
让我们能够正常训练网络

00:07:54.480 --> 00:07:59.355
这点很容易被忽略

00:07:59.355 --> 00:08:00.570
但是这一步很重要

00:08:00.570 --> 00:08:03.220
请务必记住

00:08:03.220 --> 00:08:06.090
接着 执行前向传播和反向传播

00:08:06.090 --> 00:08:07.725
并更新权重

00:08:07.725 --> 00:08:09.010
获得输出

00:08:09.010 --> 00:08:12.110
使用图像对模型进行前向传播

00:08:12.110 --> 00:08:15.720
使用模型的输出和标签计算损失

00:08:15.720 --> 00:08:20.360
然后反向传播 最后执行优化器步骤

00:08:20.360 --> 00:08:23.675
这是初始权重

00:08:23.675 --> 00:08:26.150
然后计算梯度

00:08:26.150 --> 00:08:28.010
梯度是这样的

00:08:28.010 --> 00:08:30.350
然后执行优化器步骤并更新权重

00:08:30.350 --> 00:08:32.330
权重更改了

00:08:32.330 --> 00:08:35.870
你将遍历训练集

00:08:35.870 --> 00:08:39.560
对于训练集中的每批数据

00:08:39.560 --> 00:08:42.185
你将执行相同的训练流程

00:08:42.185 --> 00:08:44.060
获取数据

00:08:44.060 --> 00:08:46.180
清理梯度

00:08:46.180 --> 00:08:51.320
将图像/输入传入网络中 获得输出

00:08:51.320 --> 00:08:52.925
使用标签计算损失

00:08:52.925 --> 00:08:56.285
然后用损失反向传播并更新权重

00:08:56.285 --> 00:09:01.640
现在该你来为此模型实现训练循环了

00:09:01.640 --> 00:09:05.870
你要将遍历数据集

00:09:05.870 --> 00:09:10.440
从 trainloader 中获取图像和标签

00:09:10.440 --> 00:09:11.710
然后对每批数据执行训练流程

00:09:11.710 --> 00:09:16.000
计算网络输出

00:09:16.000 --> 00:09:18.190
计算损失 对损失进行反向传播

00:09:18.190 --> 00:09:20.120
然后更新权重

00:09:20.120 --> 00:09:23.435
经过整个数据集 1 次称为 1 个周期

00:09:23.435 --> 00:09:26.980
我将周期设成了 5 次

00:09:26.980 --> 00:09:30.005
当然 你也可以改为其他次数

00:09:30.005 --> 00:09:31.565
计算损失后

00:09:31.565 --> 00:09:34.880
我们可以递增这个值以进行跟踪

00:09:34.880 --> 00:09:36.620
最后就能查看损失

00:09:36.620 --> 00:09:38.510
调用 running_loss

00:09:38.510 --> 00:09:40.610
输出训练损失

00:09:40.610 --> 00:09:42.065
当你继续使用数据训练网络时

00:09:42.065 --> 00:09:44.825
如果操作正确的话

00:09:44.825 --> 00:09:47.555
应该看到损失开始降低

00:09:47.555 --> 00:09:49.985
自己尝试一下吧 

00:09:49.985 --> 00:09:52.530
如果需要帮助 请参阅我的解决方案加油！

