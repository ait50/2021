WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.955
在这道练习中

00:00:02.955 --> 00:00:04.920
我们将介绍 PyTorch 中的 RNN

00:00:04.920 --> 00:00:07.715
以及如何使用 RNN 执行简单的时间序列预测

00:00:07.715 --> 00:00:10.770
我们将查看一些数据并创建一个 RNN

00:00:10.770 --> 00:00:14.850
它会根据当前给定数据点准确预测下个数据点

00:00:14.850 --> 00:00:17.485
下面通过一个示例来讲解

00:00:17.485 --> 00:00:18.765
我们开始吧

00:00:18.765 --> 00:00:21.205
导入常用资源

00:00:21.205 --> 00:00:24.960
然后创建一些简单的输入和目标训练数据

00:00:24.960 --> 00:00:28.200
经典示例是使用正弦波作为输入

00:00:28.200 --> 00:00:31.470
因为它的形状变化多样 是个比较有意思的任务

00:00:31.470 --> 00:00:32.730
而且很好预测

00:00:32.730 --> 00:00:37.040
我将创建一个示例输入和目标数据点序列 长为 20

00:00:37.040 --> 00:00:39.475
在这里指定为 seq_length

00:00:39.475 --> 00:00:42.500
RNN 用于处理序列数据

00:00:42.500 --> 00:00:44.030
序列长度是指

00:00:44.030 --> 00:00:46.550
输入的序列长度

00:00:46.550 --> 00:00:49.520
通常 序列长度是指句子中的单词数量

00:00:49.520 --> 00:00:53.735
或者数字数据的长度 例如此处

00:00:53.735 --> 00:00:55.235
在这两行

00:00:55.235 --> 00:00:56.660
生成正弦波的起始部分

00:00:56.660 --> 00:01:00.035
范围是从 0 到 π 时间步

00:01:00.035 --> 00:01:04.845
首先创建一些点 数量为序列长度 20 加一

00:01:04.845 --> 00:01:08.960
然后变形正弦波数据

00:01:08.960 --> 00:01:11.135
为其添加一个维度 input_size 值为 1

00:01:11.135 --> 00:01:14.545
要创建所需长度的输入和目标序列

00:01:14.545 --> 00:01:19.100
在这里指出输入 x 等于数据中的最后一个点之外的所有点

00:01:19.100 --> 00:01:21.890
目标 y 等于第一个点之外的所有点

00:01:21.890 --> 00:01:26.095
x 和 y 应该包含 20 个数据点 并且输入大小是 1

00:01:26.095 --> 00:01:29.865
最后 使用相同的 x 轴显示此数据

00:01:29.865 --> 00:01:35.380
输入 x 用红点表示 目标 y 用蓝点表示并且移动了一位

00:01:35.380 --> 00:01:39.125
查看同一时间步的这两个点

00:01:39.125 --> 00:01:43.045
y 比 x 往前移动了一个时间步

00:01:43.045 --> 00:01:44.705
这正是我们想要的结果

00:01:44.705 --> 00:01:46.490
创建好训练数据后

00:01:46.490 --> 00:01:49.800
下一步是定义学习此数据的 RNN

00:01:49.800 --> 00:01:52.100
我们可以照常定义 RNN

00:01:52.100 --> 00:01:55.470
也就是使用 PyTorch 的 nn 库定义为类

00:01:55.470 --> 00:01:59.220
语法和之前定义 CNN 的相似

00:01:59.220 --> 00:02:01.940
打开 RNN 文档

00:02:01.940 --> 00:02:05.170
看看传入递归层的参数

00:02:05.170 --> 00:02:08.180
这是 RNN 层级的文档

00:02:08.180 --> 00:02:10.130
此层级负责根据输入

00:02:10.130 --> 00:02:12.550
计算隐藏状态

00:02:12.550 --> 00:02:14.260
要定义这样的层级

00:02:14.260 --> 00:02:16.635
我们可以利用以下参数

00:02:16.635 --> 00:02:20.040
输入大小 隐藏大小 层级数量及其他参数

00:02:20.040 --> 00:02:23.370
输入大小表示输入特征的数量

00:02:23.370 --> 00:02:26.600
我们的输入大小是在一个输入大小特征中

00:02:26.600 --> 00:02:29.755
有 20 个序列值

00:02:29.755 --> 00:02:34.605
这与创建 CNN 时输入图像的深度相似

00:02:34.605 --> 00:02:37.220
下个参数是隐藏大小

00:02:37.220 --> 00:02:40.440
表示 RNN 的输出特征数量以及隐藏状态

00:02:40.440 --> 00:02:41.990
然后是层级数量

00:02:41.990 --> 00:02:43.265
如果大于 1

00:02:43.265 --> 00:02:46.130
则表示我们将堆叠两个 RNN

00:02:46.130 --> 00:02:49.430
最后请注意这个 batch_first 参数

00:02:49.430 --> 00:02:52.010
如果它为 true 则表示我们提供的输入和输出张量

00:02:52.010 --> 00:02:55.220
第一个维度大小将为 batch_size

00:02:55.220 --> 00:02:57.580
大多数情况下它都设为 true

00:02:57.580 --> 00:03:00.500
这就是定义 RNN 层级的方式

00:03:00.500 --> 00:03:02.780
之后在 forward 函数中

00:03:02.780 --> 00:03:05.090
它会接受输入和初始隐藏状态

00:03:05.090 --> 00:03:07.965
生成输出和新的隐藏状态

00:03:07.965 --> 00:03:13.495
回到 notebook在这里定义 RNN 层级 self.rnn

00:03:13.495 --> 00:03:16.925
此 RNN 会接受 input_size 和 hidden_dim

00:03:16.925 --> 00:03:20.410
后者定义了此 RNN 的输出特征数量

00:03:20.410 --> 00:03:24.230
然后是 n_layers 它使我们能够创建叠加 RNN

00:03:24.230 --> 00:03:28.115
通常设为 1 到 3 层

00:03:28.115 --> 00:03:30.950
最后 将 batch_first 设为 true

00:03:30.950 --> 00:03:34.645
因为我将变形输入 使 batch_size 变成第一个维度

00:03:34.645 --> 00:03:36.830
要完成此模型

00:03:36.830 --> 00:03:39.675
我需要再添加一层 即最终全连接层

00:03:39.675 --> 00:03:42.859
此层级负责生成一定数量的输出

00:03:42.859 --> 00:03:46.345
数量等于我为此 RNN 设定的 output_size

00:03:46.345 --> 00:03:51.115
在创建 RNN 时传入所有这些参数

00:03:51.115 --> 00:03:53.120
此外注意 我存储了 hidden_dim 的值

00:03:53.120 --> 00:03:56.115
因为之后在 forward 函数中要使用它

00:03:56.115 --> 00:03:58.985
我将在 forward 函数中

00:03:58.985 --> 00:04:01.700
指定一批输入序列如何经过此模型

00:04:01.700 --> 00:04:05.235
注意 此 forward 接受输入 x 和隐藏状态

00:04:05.235 --> 00:04:07.040
首先我将通过调用 x.size(0)

00:04:07.040 --> 00:04:10.205
获取输入的批次大小

00:04:10.205 --> 00:04:14.710
然后将初始输入和隐藏状态传入 RNN 层级

00:04:14.710 --> 00:04:17.940
生成 RNN 输出和新的隐藏状态

00:04:17.940 --> 00:04:22.395
然后对 RNN 输出调用 view 以将其变形为我想要的形状

00:04:22.395 --> 00:04:25.230
在此例中 行数将是批次大小乘以序列长度

00:04:25.230 --> 00:04:28.185
列数将是隐藏维数

00:04:28.185 --> 00:04:30.670
这是扁平化步骤

00:04:30.670 --> 00:04:33.350
我们把输出准备好 然后传入全连接层

00:04:33.350 --> 00:04:37.310
接着将此变形后的输出传入最终全连接层

00:04:37.310 --> 00:04:41.260
在这里返回最终输出和 RNN 生成的隐藏状态

00:04:41.260 --> 00:04:43.365
最后一步

00:04:43.365 --> 00:04:45.560
我将创建一些文本数据

00:04:45.560 --> 00:04:48.290
测试 RNN 能否达到预期效果

00:04:48.290 --> 00:04:50.705
编写 RNN 代码时我最常犯的错误是

00:04:50.705 --> 00:04:53.540
在某个地方弄错数据维度

00:04:53.540 --> 00:04:56.345
因此我将检查下该 RNN 是否符合预期

00:04:56.345 --> 00:05:00.925
在这里创建一个测试 RNN，输入和输出大小为 1

00:05:00.925 --> 00:05:03.840
隐藏维度为 10 层级数量等于 2

00:05:03.840 --> 00:05:06.765
你可以更改隐藏维度和层级数量

00:05:06.765 --> 00:05:10.540
我只是想检查是否能生成预期的形状

00:05:10.540 --> 00:05:14.240
在这里创建一些测试数据 长度为 seq_length

00:05:14.240 --> 00:05:17.180
将该数据转换为张量

00:05:17.180 --> 00:05:19.100
压缩第一个维度

00:05:19.100 --> 00:05:21.620
使其大小为 batch_size = 1

00:05:21.620 --> 00:05:26.140
然后输出此输入大小并将其传入测试 RNN 中

00:05:26.140 --> 00:05:28.280
这个是初始隐藏状态

00:05:28.280 --> 00:05:30.665
这里将设为 none

00:05:30.665 --> 00:05:33.530
它将返回输出和隐藏状态

00:05:33.530 --> 00:05:35.545
同样输出这些大小

00:05:35.545 --> 00:05:39.860
Ok输入大小是三维张量 和我预期的一样

00:05:39.860 --> 00:05:42.505
第一个维度是 1 即 batch_size

00:05:42.505 --> 00:05:44.305
序列长度是 20

00:05:44.305 --> 00:05:46.370
最后是输入特征数量

00:05:46.370 --> 00:05:48.575
也是 1 因为在这里指定为 1

00:05:48.575 --> 00:05:50.980
输出大小是二维张量

00:05:50.980 --> 00:05:54.710
这是因为模型定义的 forward 函数

00:05:54.710 --> 00:05:58.280
将批次大小和序列长度变成了一个参数

00:05:58.280 --> 00:06:03.530
因此批次大小乘以序列长度为 20 输出大小是 1

00:06:03.530 --> 00:06:05.020
最后是隐藏状态

00:06:05.020 --> 00:06:07.130
第一个维度值是 2

00:06:07.130 --> 00:06:10.250
即在模型定义中指定的层数

00:06:10.250 --> 00:06:15.005
第二个值是 1 表示输入的批次大小

00:06:15.005 --> 00:06:19.430
最后一个维度值是 10 表示隐藏维度

00:06:19.430 --> 00:06:23.390
看起来不错 和我预期的一样

00:06:23.390 --> 00:06:26.330
下面我将演示如何训练这样的模型

