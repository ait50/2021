WEBVTT
Kind: captions
Language: pt-BR

00:00:00.271 --> 00:00:04.201
Nesta classe, você vai implementar
uma RNN para caracteres.

00:00:04.234 --> 00:00:08.063
Ou seja, a rede aprenderá um texto,
um caractere de cada vez,

00:00:08.096 --> 00:00:11.038
depois vai gerar um novo texto,
um caractere de cada vez.

00:00:11.727 --> 00:00:14.431
Digamos que queremos gerar
novas peças de Shakespeare,

00:00:14.464 --> 00:00:17.722
como, por exemplo,
"To be or not to be".

00:00:17.755 --> 00:00:21.777
Passamos a sequência à nossa RNN,
um caractere de cada vez.

00:00:21.810 --> 00:00:24.016
Após o treino,
a rede vai gerar um novo texto

00:00:24.049 --> 00:00:25.700
ao prever o próximo caractere

00:00:25.733 --> 00:00:28.395
com base nos caracteres
que ela já viu.

00:00:28.428 --> 00:00:30.154
Então,
para treinar essa rede,

00:00:30.187 --> 00:00:33.606
queremos prever o próximo caractere
na sequência de input.

00:00:33.639 --> 00:00:36.930
Assim, a rede aprenderá a gerar
uma sequência de caracteres

00:00:36.963 --> 00:00:39.384
parecida
com o texto original.

00:00:39.417 --> 00:00:42.792
Vamos imaginar como será
a arquitetura desta rede.

00:00:42.825 --> 00:00:44.698
Primeiro vamos desdobrar
a RNN,

00:00:44.731 --> 00:00:47.497
para ver como tudo isso funciona
como sequência.

00:00:47.897 --> 00:00:49.633
Aqui temos
a nossa camada de input,

00:00:49.666 --> 00:00:51.376
à qual passaremos
os caracteres

00:00:51.409 --> 00:00:53.385
como vetores
com codificação one-hot.

00:00:53.418 --> 00:00:55.486
Esses vetores vão
para a camada oculta.

00:00:55.895 --> 00:00:58.282
A camada oculta é construída
com células LSTM,

00:00:58.315 --> 00:01:00.316
onde o estado oculto
e o estado da célula

00:01:00.349 --> 00:01:03.191
passam de uma célula para a outra
na sequência.

00:01:03.597 --> 00:01:07.355
Na prática, vamos usar
várias camadas de células LSTM.

00:01:07.388 --> 00:01:09.354
Basta empilhá-las assim.

00:01:09.877 --> 00:01:13.014
O output dessas células
vai para a camada de output.

00:01:13.047 --> 00:01:15.959
A camada de output é usada
para prever o próximo caractere.

00:01:15.992 --> 00:01:18.200
Queremos as probabilidades
de cada caractere,

00:01:18.248 --> 00:01:21.975
assim como você fez a classificação
de imagens com a rede convolucional.

00:01:22.377 --> 00:01:26.615
Isso significa que queremos
uma ativação softmax no output.

00:01:26.648 --> 00:01:28.945
Nossos alvos aqui
serão a sequência de input,

00:01:28.978 --> 00:01:30.286
deslocada de uma posição,

00:01:30.319 --> 00:01:34.179
de modo que cada caractere preveja
o próximo caractere na sequência.

00:01:34.212 --> 00:01:36.375
Usaremos de novo
a perda de entropia cruzada

00:01:36.408 --> 00:01:38.423
para treinar
com gradiente descendente.

00:01:38.456 --> 00:01:41.861
Quando a rede estiver treinada,
podemos fornecer um caractere

00:01:41.894 --> 00:01:43.900
e obter uma distribuição
de probabilidade

00:01:43.933 --> 00:01:45.910
para o próximo caractere
mais provável.

00:01:45.943 --> 00:01:48.145
Depois pegamos uma amostra
dessa distribuição

00:01:48.178 --> 00:01:49.788
para chegar
ao próximo caractere.

00:01:50.091 --> 00:01:52.477
Em seguida, podemos pegar
esse caractere,

00:01:52.510 --> 00:01:54.733
passá-lo para a rede
e pegar outro.

00:01:54.766 --> 00:01:59.172
Fazemos isso até construirmos
um texto totalmente novo.

00:01:59.205 --> 00:02:02.281
Vamos treinar esta rede
com o texto de "Anna Karenina",

00:02:02.314 --> 00:02:04.189
que é um dos meus livros
favoritos.

00:02:04.222 --> 00:02:07.628
É de domínio público,
então você pode usá-lo livremente.

00:02:07.661 --> 00:02:09.379
Além disso,
é um romance incrível.

