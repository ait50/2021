WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.679
在这个视频中 我会同你介绍两个相似且效果不错的结构

00:00:04.679 --> 00:00:09.099
但 LSTM 还有很多别的结构 我们支持你更深入地研究

00:00:09.099 --> 00:00:12.019
这个结构很简单 但效果也不错

00:00:12.019 --> 00:00:16.140
叫做门限回归单元 简称 GRU

00:00:16.140 --> 00:00:18.059
它把遗忘门和学习门合并为

00:00:18.059 --> 00:00:21.565
更新门 然后把更新门的结果交由合并门处理

00:00:21.565 --> 00:00:25.410
它只会返出一个工作记忆 而不是一对长期记忆和短期记忆

00:00:25.410 --> 00:00:28.285
但它应用起来相当不错

00:00:28.285 --> 00:00:29.594
我不会详谈这个

00:00:29.594 --> 00:00:31.260
但在讲师注释里 我推荐了

00:00:31.260 --> 00:00:35.005
一些很棒的参考 以便你进一步学习门限回归单元

00:00:35.005 --> 00:00:36.774
这是另一个结构

00:00:36.774 --> 00:00:38.158
我们重温下遗忘门

00:00:38.158 --> 00:00:41.173
遗忘因子 ft 的计算需要

00:00:41.173 --> 00:00:44.875
输入短期记忆和事件的组合

00:00:44.875 --> 00:00:46.615
但长期记忆呢？

00:00:46.615 --> 00:00:49.140
看起来我们在做决定时把它落下了

00:00:49.140 --> 00:00:51.478
为什么长期记忆没资格参与决定

00:00:51.478 --> 00:00:54.515
什么事该被记住 什么不该？ 我们来解决这个问题

00:00:54.515 --> 00:00:56.759
把长期记忆也和

00:00:56.759 --> 00:00:59.798
计算遗忘因子的神经网络关联起来

00:00:59.798 --> 00:01:02.939
从数学层面上讲 输入矩阵会变得更大

00:01:02.939 --> 00:01:06.420
因为我们把它和长期记忆矩阵拼接了起来

00:01:06.420 --> 00:01:09.420
这就叫做窥视孔连接 毕竟现在长期记忆

00:01:09.420 --> 00:01:12.775
在 LSTM 里的决策参与度变高了

00:01:12.775 --> 00:01:15.760
我们可以对每个遗忘节点都这么操作

00:01:15.760 --> 00:01:20.000
这就是我们的成果 一个具备窥视孔连接的 LSTM

