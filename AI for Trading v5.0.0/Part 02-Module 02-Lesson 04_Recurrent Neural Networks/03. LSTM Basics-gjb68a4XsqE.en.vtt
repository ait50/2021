WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.844
So let's recap. We have the following problem: we are watching a TV show and we have

00:00:04.844 --> 00:00:07.450
a long term memory which is that the show is about nature

00:00:07.450 --> 00:00:10.759
and science and lots of forest animal have appeared.

00:00:10.759 --> 00:00:12.839
We also have a short term memory which is what we have

00:00:12.839 --> 00:00:15.403
recently seen which is squirrels and trees.

00:00:15.403 --> 00:00:18.629
And we have a current event which is what we just saw,

00:00:18.629 --> 00:00:21.704
the image of a dog which could also be a wolf.

00:00:21.704 --> 00:00:26.405
And we want these three things to combine to form a prediction of what our image is.

00:00:26.405 --> 00:00:29.760
In this case, the long term memory which says that the show is about

00:00:29.760 --> 00:00:34.060
forest animals will give us a hint that the picture is of a wolf and not a dog.

00:00:34.060 --> 00:00:37.109
We also want the three pieces of information, long term memory,

00:00:37.109 --> 00:00:38.579
short term memory, and the event,

00:00:38.579 --> 00:00:40.743
to help us update the long term memory.

00:00:40.743 --> 00:00:42.679
So let's say we keep the fact that the show is about

00:00:42.679 --> 00:00:44.844
nature and we forget that it's about science.

00:00:44.844 --> 00:00:46.505
And we also remember that the show is about

00:00:46.505 --> 00:00:49.590
forest animals and trees since we recently saw a tree.

00:00:49.590 --> 00:00:52.609
So we add a bit and remove a bit to the long term memory.

00:00:52.609 --> 00:00:54.509
And finally we also want to use

00:00:54.509 --> 00:00:57.664
these three pieces of information to help us update the short term memory.

00:00:57.664 --> 00:00:59.399
So let's say in our short term memory you want to

00:00:59.399 --> 00:01:01.380
forget that the show has trees and remember

00:01:01.380 --> 00:01:06.055
that it has wolves since the trees happened a few images ago and we just saw a wolf.

00:01:06.055 --> 00:01:08.099
So basically we have an architecture like this and we

00:01:08.099 --> 00:01:10.769
use even more animals to represent our stages of memory.

00:01:10.769 --> 00:01:15.544
The long term memory is represented by an elephant since elephants have long term memory.

00:01:15.543 --> 00:01:17.619
The short term memory will be represented by

00:01:17.620 --> 00:01:22.129
a forgetful fish and the event will still be represented by the Wolf we just saw.

00:01:22.129 --> 00:01:25.650
So LSTM works as follows: the three pieces of information go inside

00:01:25.650 --> 00:01:27.930
the node and then some math happens and

00:01:27.930 --> 00:01:30.760
then the new pieces of information get updated and come out.

00:01:30.760 --> 00:01:31.974
There is a long term memory,

00:01:31.974 --> 00:01:35.010
a short term memory and the prediction of the event.

00:01:35.010 --> 00:01:38.799
More specifically the architecture of the LSTM contains a few gates.

00:01:38.799 --> 00:01:40.189
It contains a forget gate,

00:01:40.188 --> 00:01:43.824
a learn gate, a remember gate, and a use gate.

00:01:43.825 --> 00:01:45.954
And here's basically how they work.

00:01:45.953 --> 00:01:47.429
So the long term memory goes to

00:01:47.430 --> 00:01:51.650
the forget gate where it forgets everything that it doesn't consider useful.

00:01:51.650 --> 00:01:55.530
The short term memory and the event are joined together in the learn gate,

00:01:55.530 --> 00:01:57.599
containing the information that we've recently

00:01:57.599 --> 00:02:00.930
learned and it removes any unnecessary information.

00:02:00.930 --> 00:02:04.019
Now the long term memory that we haven't forgotten yet plus the new

00:02:04.019 --> 00:02:07.864
information that we've learned get joined together in the remember gate.

00:02:07.864 --> 00:02:11.454
This gate puts these two together and since it's called remember gate,

00:02:11.454 --> 00:02:14.860
what it does is it outputs an updated long term memory.

00:02:14.860 --> 00:02:17.460
So this is what we'll remember for the future.

00:02:17.460 --> 00:02:19.530
And finally, the use gate is the one that decides

00:02:19.530 --> 00:02:21.938
what information we use from what we previously

00:02:21.938 --> 00:02:23.938
know plus what we just learned to make

00:02:23.938 --> 00:02:27.060
a prediction so it also takes those inputs the long term memory,

00:02:27.060 --> 00:02:31.093
and the new information joins them and decides what to output.

00:02:31.093 --> 00:02:35.508
The output becomes both the prediction and the new short term memory.

00:02:35.508 --> 00:02:37.559
And so the big unfolded picture that we have is as

00:02:37.560 --> 00:02:39.658
follows: we have the long term memory and

00:02:39.658 --> 00:02:44.370
the short term memory coming in which we call LTM and STM.

00:02:44.370 --> 00:02:48.245
And then an event and an output are coming in and out of the LSTM.

00:02:48.245 --> 00:02:50.324
And then this passes to the next node,

00:02:50.324 --> 00:02:53.004
and so on and so forth.

00:02:53.003 --> 00:02:55.929
So in general at time t we label everything with

00:02:55.930 --> 00:03:02.000
an underscore t as we can see information passes from time t -1 to time t.

