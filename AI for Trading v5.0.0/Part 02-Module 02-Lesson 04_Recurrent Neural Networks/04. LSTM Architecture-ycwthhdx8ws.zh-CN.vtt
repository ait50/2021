WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.740
为了学习 LSTM 的结构

00:00:02.740 --> 00:00:05.429
我们先快速重温下 RNN 的结构

00:00:05.429 --> 00:00:10.379
基本上 我们需要拿事件 Et 和 记忆 Mt-1

00:00:10.380 --> 00:00:12.460
它们来自上一个时间点

00:00:12.460 --> 00:00:15.150
把它们代入一个简单的 tanh

00:00:15.150 --> 00:00:20.184
或 sigmoid 激活函数里 从而得到输出和记忆 Mt

00:00:20.184 --> 00:00:21.539
更具体地讲

00:00:21.539 --> 00:00:26.039
我们要把这两个向量加到一起 让它们乘以一个矩阵 W 再加一个偏差 b

00:00:26.039 --> 00:00:28.920
然后把整个公式套到 tanh 函数里

00:00:28.920 --> 00:00:31.149
从而得到输出 Mt

00:00:31.149 --> 00:00:35.490
这个输出是一个预测 也是我们要输入下一个节点的记忆

00:00:35.490 --> 00:00:37.710
LSTM 的结构很类似

00:00:37.710 --> 00:00:40.679
但里面有更多节点 还有两个输入和输出

00:00:40.679 --> 00:00:43.990
因为它要跟踪长期记忆和短期记忆

00:00:43.990 --> 00:00:45.679
正如我之前说的 短期记忆

00:00:45.679 --> 00:00:47.550
就是输出或预测结果

00:00:47.549 --> 00:00:51.274
别害怕 没看起来那么复杂

00:00:51.274 --> 00:00:53.298
在接下来的几个视频中 我们会把这些拆开来讲

