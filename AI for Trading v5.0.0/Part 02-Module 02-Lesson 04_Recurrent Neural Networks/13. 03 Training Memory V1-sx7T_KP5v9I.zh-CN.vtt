WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.560
上次我们定义了一个模型

00:00:02.560 --> 00:00:06.380
下面我将实例化该模型并使用训练数据训练它

00:00:06.380 --> 00:00:09.120
首先指定模型超参数

00:00:09.120 --> 00:00:11.160
输入和输出大小为 1

00:00:11.160 --> 00:00:14.550
一次处理和生成一个序列

00:00:14.550 --> 00:00:17.310
然后指定隐藏维度

00:00:17.310 --> 00:00:20.250
也就是希望 RNN 层级生成的特征数量

00:00:20.250 --> 00:00:23.065
设为 32 但是对于这样的小型数据集来说

00:00:23.065 --> 00:00:25.105
甚至可以设为更小的值

00:00:25.105 --> 00:00:27.345
暂时将 n_layers 设为 1

00:00:27.345 --> 00:00:29.880
不叠加任何 RNN 层级

00:00:29.880 --> 00:00:31.915
创建并输出此 RNN

00:00:31.915 --> 00:00:33.890
结果应该是我期望的值

00:00:33.890 --> 00:00:36.620
这是 RNN 层级的输入大小和隐藏维度

00:00:36.620 --> 00:00:40.600
这是线性层级的输入特征数量和输出数量

00:00:40.600 --> 00:00:44.090
在开始训练之前 定义损失和优化函数

00:00:44.090 --> 00:00:46.730
我们将训练模型生成数据点

00:00:46.730 --> 00:00:49.830
这些数据点是坐标值

00:00:49.830 --> 00:00:52.640
为了比较预测数据点和真实数据点

00:00:52.640 --> 00:00:54.680
我们将使用回归损失

00:00:54.680 --> 00:00:57.670
因为它只是一个数量

00:00:57.670 --> 00:00:58.970
而不是类别概率

00:00:58.970 --> 00:01:00.980
我将使用均方误差损失

00:01:00.980 --> 00:01:03.650
它会衡量两个点之间的距离

00:01:03.650 --> 00:01:06.990
我将使用 Adam 优化器 它是递归模型的标准优化器

00:01:06.990 --> 00:01:09.655
传入参数和学习速率

00:01:09.655 --> 00:01:12.530
下面是 train 函数

00:01:12.530 --> 00:01:15.320
它将接受一个 rnn 和训练步数

00:01:15.320 --> 00:01:19.315
以及何时输出损失信息的参数

00:01:19.315 --> 00:01:21.560
在此函数开头

00:01:21.560 --> 00:01:23.205
初始化隐藏状态

00:01:23.205 --> 00:01:27.470
一开始设为 None隐藏状态默认全为 0

00:01:27.470 --> 00:01:29.785
然后是批次循环

00:01:29.785 --> 00:01:31.705
这个循环不太符合常规

00:01:31.705 --> 00:01:33.440
根据训练步数

00:01:33.440 --> 00:01:35.880
动态生成数据

00:01:35.880 --> 00:01:40.635
在这几行一次生成 20 个正弦波值序列

00:01:40.635 --> 00:01:42.990
和在开头生成数据一样

00:01:42.990 --> 00:01:45.500
在这里获取输入 x 以及目标 y

00:01:45.500 --> 00:01:48.110
y 只是比 x 晚一个时间步

00:01:48.110 --> 00:01:51.140
在这里将数据转换为张量

00:01:51.140 --> 00:01:55.275
压缩张量 x 的第一个维度 使其批次大小为 1

00:01:55.275 --> 00:01:58.280
然后将输入张量传入 rnn 模型

00:01:58.280 --> 00:02:03.020
rnn 接受的是输入张量 x 和初始隐藏状态

00:02:03.020 --> 00:02:06.820
生成预测参数和新的隐藏状态

00:02:06.820 --> 00:02:08.875
下面是很重要的部分

00:02:08.875 --> 00:02:11.870
当我们在下个时间步再次循环时

00:02:11.870 --> 00:02:14.860
我希望将此新隐藏状态当做输入传入 rnn 中

00:02:14.860 --> 00:02:19.660
将生成的隐藏状态中的值复制到新的变量中

00:02:19.660 --> 00:02:23.330
这样可以将隐藏状态与其历史记录分离开

00:02:23.330 --> 00:02:26.810
不用反向传播一系列累积的隐藏状态

00:02:26.810 --> 00:02:29.540
所以在下个时间步（序列的下个数据点）

00:02:29.540 --> 00:02:33.790
将它传入 rnn 中

00:02:33.790 --> 00:02:36.180
然后是正常的训练命令

00:02:36.180 --> 00:02:38.630
清零累积的梯度

00:02:38.630 --> 00:02:42.720
计算损失 执行反向传播和优化步骤

00:02:42.720 --> 00:02:45.320
在下面输出损失

00:02:45.320 --> 00:02:47.770
并显示输入和预测输出

00:02:47.770 --> 00:02:50.375
最后 此函数返回训练过的 rnn

00:02:50.375 --> 00:02:53.030
你可以保存该模型

00:02:53.030 --> 00:02:55.115
我们来运行下代码

00:02:55.115 --> 00:02:56.840
训练在上面定义的 rnn

00:02:56.840 --> 00:02:59.710
并训练 75 步

00:02:59.710 --> 00:03:02.480
每隔 15 步输出结果

00:03:02.480 --> 00:03:05.390
这里输出了均方误差损失

00:03:05.390 --> 00:03:09.080
并显示了红色输入值和蓝色输出值之间的区别

00:03:09.080 --> 00:03:11.870
我们希望蓝色输出值

00:03:11.870 --> 00:03:14.560
比红色输入值晚一个时间步

00:03:14.560 --> 00:03:16.500
所以一开始完全错了

00:03:16.500 --> 00:03:20.230
在 15 个时间步之后 损失降低了很多

00:03:20.230 --> 00:03:22.760
蓝色虚线更接近红色虚线

00:03:22.760 --> 00:03:27.565
继续训练之后 蓝色预测虚线与已知目标更接近

00:03:27.565 --> 00:03:29.370
在 75 步之后

00:03:29.370 --> 00:03:30.840
损失很低了

00:03:30.840 --> 00:03:34.310
蓝色虚线和已知目标/真实输出很相似

00:03:34.310 --> 00:03:37.565
查看同一时间步的红色输入点

00:03:37.565 --> 00:03:38.890
和蓝色输入点

00:03:38.890 --> 00:03:40.520
应该看到蓝色输入

00:03:40.520 --> 00:03:43.895
比红色输入晚了一个时间步很接近了

00:03:43.895 --> 00:03:47.000
如果训练更多个时间步或添加更多层级

00:03:47.000 --> 00:03:50.680
RNN 的效果应该会更好

00:03:50.680 --> 00:03:51.945
在此视频中

00:03:51.945 --> 00:03:55.850
我希望能演示简单 RNN 的基本结构

00:03:55.850 --> 00:04:00.010
并演示如何在训练过程中记录隐藏状态及表示记忆功能

00:04:00.010 --> 00:04:02.990
你可以对其他数据执行非常相似的操作

00:04:02.990 --> 00:04:07.010
例如全球气温或股价 这些数据更复杂

00:04:07.010 --> 00:04:08.990
如果能通过查看给定的这些数据

00:04:08.990 --> 00:04:11.120
然后预测未来情形 那就很有意思了

00:04:11.120 --> 00:04:13.100
这只是一个例子

00:04:13.100 --> 00:04:15.135
你可以在我们的程序 GitHub 中查看此代码

00:04:15.135 --> 00:04:16.520
下方提供了链接

00:04:16.520 --> 00:04:17.780
建议你尝试不同的模型参数

00:04:17.780 --> 00:04:20.390
直到你充分了解

00:04:20.390 --> 00:04:22.010
RNN 输入和输出的维度

00:04:22.010 --> 00:04:25.570
明白超参数会如何变化以及如何训练模型

00:04:25.570 --> 00:04:29.670
接下来 我和 Matt 将介绍如何生成文本

