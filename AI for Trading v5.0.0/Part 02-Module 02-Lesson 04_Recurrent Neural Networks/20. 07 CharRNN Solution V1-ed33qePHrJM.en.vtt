WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.509
We wanted to define a character RNN with a two layer LSTM.

00:00:04.509 --> 00:00:07.199
Here in my solution, I am running this code on

00:00:07.200 --> 00:00:11.275
GPU and here's my code for defining our character level RNN.

00:00:11.275 --> 00:00:15.255
First, I defined an LSTM layer, self.lstm.

00:00:15.255 --> 00:00:17.019
This takes in an input size,

00:00:17.019 --> 00:00:18.554
which is going to be the length of

00:00:18.554 --> 00:00:20.699
a one-hot encoded input character and

00:00:20.699 --> 00:00:23.364
that's just the length of all of my unique characters.

00:00:23.364 --> 00:00:25.939
Then, it takes a hidden dimension a number of

00:00:25.940 --> 00:00:29.030
layers and a dropout probability that we've specified.

00:00:29.030 --> 00:00:33.685
Remember that this will create a dropout layer in between multiple LSTM layers,

00:00:33.685 --> 00:00:36.335
and all of these are parameters that are going to be passed

00:00:36.335 --> 00:00:39.189
in as input to our RNN when it's constructed.

00:00:39.189 --> 00:00:43.339
Then, I've set batch first to true because when we created our batch data,

00:00:43.340 --> 00:00:45.270
the first dimension is the batch size,

00:00:45.270 --> 00:00:47.215
rather than the sequence length.

00:00:47.215 --> 00:00:49.835
Okay. Next, I've defined a dropout layer to go

00:00:49.835 --> 00:00:52.939
in-between my LSTM and a final linear layer.

00:00:52.939 --> 00:00:56.509
Then, I have FC, my final fully connected linear layer.

00:00:56.509 --> 00:00:59.119
This takes in our LSTM outputs,

00:00:59.119 --> 00:01:01.044
which are going to be dimension and hidden.

00:01:01.045 --> 00:01:05.555
It's going to output our character class scores for the most likely next character.

00:01:05.555 --> 00:01:08.970
So, these are the class scores for each possible next character.

00:01:08.969 --> 00:01:11.480
This output size is the same size as our input,

00:01:11.480 --> 00:01:13.605
the length of our character vocabulary.

00:01:13.605 --> 00:01:15.935
Then, I move to the forward function.

00:01:15.935 --> 00:01:20.275
I'm passing my input X and a hidden state to my LSTM layer here.

00:01:20.275 --> 00:01:23.810
This produces my LSTM output and a new hidden state.

00:01:23.810 --> 00:01:26.269
I'm going to pass the LSTM output through

00:01:26.269 --> 00:01:29.119
the dropout layer that I defined here to get a new output.

00:01:29.120 --> 00:01:31.344
Then, I'm making sure to reshape this output,

00:01:31.344 --> 00:01:33.844
so that the last dimension is our hidden dim.

00:01:33.844 --> 00:01:38.609
This negative one basically means I'm going to be stacking up the outputs of the LSTM.

00:01:38.609 --> 00:01:42.734
Finally, I'm passing this V-shaped output to the final fully connected layer.

00:01:42.734 --> 00:01:45.079
Then, I'm returning this final output and

00:01:45.079 --> 00:01:47.594
the hidden state that was generated by our LSTM.

00:01:47.594 --> 00:01:51.914
These two functions in addition to the init hidden function complete my model.

00:01:51.915 --> 00:01:57.475
Next, it's time to train and let's take a look at the training loop that was provided.

00:01:57.474 --> 00:02:00.739
This function takes in a model to train some data,

00:02:00.739 --> 00:02:02.319
and the number of epics to train for,

00:02:02.319 --> 00:02:03.649
and a batch size,

00:02:03.650 --> 00:02:06.330
and sequence length that define our mini batch size.

00:02:06.329 --> 00:02:09.139
It also takes in a few more training parameters.

00:02:09.139 --> 00:02:13.139
First in here, I've defined my optimizer and my loss function.

00:02:13.139 --> 00:02:16.054
The optimizer is a standard Adam optimizer with

00:02:16.055 --> 00:02:19.069
a learning rate set to the past and learning rate up here.

00:02:19.069 --> 00:02:21.359
The last function is cross entropy loss,

00:02:21.360 --> 00:02:24.775
which is useful for when we're outputting character class scores.

00:02:24.775 --> 00:02:26.960
Here, you'll see some details about creating

00:02:26.960 --> 00:02:31.105
some validation data and moving our model to GPU if it's available.

00:02:31.104 --> 00:02:33.774
Here, you can see the start of our epic loop.

00:02:33.775 --> 00:02:35.460
At the start of each epic,

00:02:35.460 --> 00:02:38.254
I'm initializing the hidden state of our LSTM.

00:02:38.254 --> 00:02:41.750
Recall that this takes in the batch size of our data to define the size of

00:02:41.750 --> 00:02:45.969
the hidden state and it returns a hidden in cell state that are all zeros.

00:02:45.969 --> 00:02:47.694
Then, inside this epic loop,

00:02:47.694 --> 00:02:49.104
I have my batch loop.

00:02:49.104 --> 00:02:53.334
This is getting our X and Y mini batches from our get batches generator.

00:02:53.335 --> 00:02:57.090
Remember that this function basically iterates through are encoded data,

00:02:57.090 --> 00:02:59.810
and returns batches of inputs X and targets

00:02:59.810 --> 00:03:04.390
Y. I'm then converting the input into a one-hot encoded representation,

00:03:04.389 --> 00:03:06.924
and I'm converting both X and Y are

00:03:06.925 --> 00:03:10.335
inputs and targets into Tensors that can be seen by our model.

00:03:10.335 --> 00:03:14.920
If GPU's available, I'm moving those inputs and targets to our GPU device.

00:03:14.919 --> 00:03:17.875
The next thing that you see is making sure that we detach

00:03:17.875 --> 00:03:20.590
any past in hidden state from its history.

00:03:20.590 --> 00:03:23.530
Recall that the hidden state of an LSTM layer is a Tuple,

00:03:23.530 --> 00:03:25.629
and so here, we are getting the data as a tuple.

00:03:25.629 --> 00:03:28.814
Then, we proceed with back propagation as usual.

00:03:28.814 --> 00:03:34.615
We zero out any accumulated gradients and pass in our input Tensors to our model.

00:03:34.615 --> 00:03:37.090
We also pass in the latest hidden state here.

00:03:37.090 --> 00:03:40.819
In this returns of final output and a new hidden state,

00:03:40.819 --> 00:03:44.884
then we calculate the loss by looking at the predicted output and the targets.

00:03:44.884 --> 00:03:47.419
Recall that in the forward function of our model,

00:03:47.419 --> 00:03:52.459
I smashed the batch size and sequence length of our LSTM outputs into one dimension,

00:03:52.460 --> 00:03:55.150
and so I'm doing the same thing for our targets here.

00:03:55.150 --> 00:03:57.575
Then, we're performing back propagation and moving

00:03:57.574 --> 00:04:00.689
one step in the right direction updating the weights of our network.

00:04:00.689 --> 00:04:02.419
Now before the optimization step,

00:04:02.419 --> 00:04:05.044
I've added one line of code that may look unfamiliar.

00:04:05.044 --> 00:04:07.104
I'm calling clip grad norm.

00:04:07.104 --> 00:04:10.969
Now, this kind of LSTM model has one main problem with gradients.

00:04:10.969 --> 00:04:13.219
They can explode and get really, really big.

00:04:13.219 --> 00:04:15.490
So, what we do is we can clip the gradients,

00:04:15.490 --> 00:04:17.569
we just set some clip threshold,

00:04:17.569 --> 00:04:20.305
and then if the gradient is larger than that threshold,

00:04:20.305 --> 00:04:22.530
we set it to that clip threshold,

00:04:22.529 --> 00:04:24.529
and encode we do this by just passing in

00:04:24.529 --> 00:04:27.839
the parameters and the value that we want to clip the gradients at.

00:04:27.839 --> 00:04:29.989
In this case, this value is passed in,

00:04:29.990 --> 00:04:32.485
in our train function as a value five.

00:04:32.485 --> 00:04:34.205
Okay. So, we take a backwards step,

00:04:34.204 --> 00:04:35.509
then we clip our gradients,

00:04:35.509 --> 00:04:37.519
and we perform an optimization step.

00:04:37.519 --> 00:04:40.444
At the end here, I'm doing something very similar for processing

00:04:40.444 --> 00:04:43.860
our validation data except not performing the back propagation step.

00:04:43.860 --> 00:04:46.900
Then, I'm printing out some statistics about our loss.

00:04:46.899 --> 00:04:48.954
Now with this train function defined,

00:04:48.954 --> 00:04:51.740
I can go about instantiating and training a model.

00:04:51.740 --> 00:04:53.275
In the exercise notebook,

00:04:53.274 --> 00:04:55.929
I've left these hyper parameters for you to define.

00:04:55.930 --> 00:05:00.660
I've set our hidden dimension to the value of 512 and a number of layers up two,

00:05:00.660 --> 00:05:02.040
which we talked about before.

00:05:02.040 --> 00:05:04.520
Then, I have instantiated our model, and printed it out,

00:05:04.519 --> 00:05:07.579
and we can see that we have 83 unique characters as input,

00:05:07.579 --> 00:05:09.289
512 as a hidden dimension,

00:05:09.290 --> 00:05:11.420
and two layers in our LSTM.

00:05:11.420 --> 00:05:12.800
For a dropout layer,

00:05:12.800 --> 00:05:17.855
we have the default dropout value of 0.5 and for our last fully connected layer,

00:05:17.855 --> 00:05:19.055
we have our Input features,

00:05:19.055 --> 00:05:22.194
which is the same as this hidden dimension and our output features,

00:05:22.194 --> 00:05:23.610
the number of characters.

00:05:23.610 --> 00:05:26.210
Then, there are more hyper parameters that define

00:05:26.209 --> 00:05:29.564
our batch size sequence length and number of epics to train for.

00:05:29.564 --> 00:05:31.844
Here, I've set the sequence length to 100,

00:05:31.845 --> 00:05:33.365
which is a lot of characters,

00:05:33.365 --> 00:05:36.740
but it gives our model a great deal of context to learn from.

00:05:36.740 --> 00:05:39.019
I also want to note that the hidden dimension is

00:05:39.019 --> 00:05:41.839
basically the number of features that your model can detect.

00:05:41.839 --> 00:05:46.189
Larger values basically allow a network to learn more text features.

00:05:46.189 --> 00:05:50.829
There's some more information below in this notebook about defining hyper parameters.

00:05:50.829 --> 00:05:55.050
In general, I'll try to start out with a pretty big model like this,

00:05:55.050 --> 00:05:58.105
multiple LSTM layers and a large hidden dimension.

00:05:58.105 --> 00:06:01.115
Then, I'll basically take a look at the loss as

00:06:01.115 --> 00:06:04.475
this model trains and if it's decreasing, I'll keep going.

00:06:04.475 --> 00:06:06.285
But if it's not decreasing as I expect,

00:06:06.285 --> 00:06:08.780
then I'll probably change some hyper parameters.

00:06:08.779 --> 00:06:11.329
Our text data is pretty large and here,

00:06:11.329 --> 00:06:14.930
I've trained our entire model for 20 epics on GPU.

00:06:14.930 --> 00:06:19.300
I can see the training and validation loss over time decreasing.

00:06:19.300 --> 00:06:23.430
Around epic 15, I'm seeing the lost slow down a bit.

00:06:23.430 --> 00:06:25.519
But it actually looks like the validation and

00:06:25.519 --> 00:06:28.469
training loss are still decreasing even after epic 20.

00:06:28.470 --> 00:06:31.810
I could have stood to train for an even longer amount of time.

00:06:31.810 --> 00:06:34.670
I encourage you to read this information about setting

00:06:34.670 --> 00:06:38.134
the hyper parameters of a model and really getting the best model.

00:06:38.134 --> 00:06:40.759
Then, after you've trained a model like I've just done,

00:06:40.759 --> 00:06:43.279
you can save it by name and then there's one last step,

00:06:43.279 --> 00:06:47.054
which is using that model to make predictions and generate some new text,

00:06:47.055 --> 00:06:48.600
which I'll go over next.

