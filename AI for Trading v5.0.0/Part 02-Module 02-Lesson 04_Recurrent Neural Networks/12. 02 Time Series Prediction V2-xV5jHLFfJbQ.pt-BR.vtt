WEBVTT
Kind: captions
Language: pt-BR

00:00:00.099 --> 00:00:02.713
Para apresentar as RNNs
no PyTorch,

00:00:02.746 --> 00:00:06.258
criei um notebook que mostra
como fazer previsões série temporal

00:00:06.291 --> 00:00:07.722
com uma RNN.

00:00:07.755 --> 00:00:11.290
Observaremos alguns dados para ver
se conseguimos criar uma RNN

00:00:11.323 --> 00:00:14.947
para prever o próximo ponto de dado
a partir do ponto atual.

00:00:14.980 --> 00:00:17.316
E isso é muito fácil de ver
em um exemplo,

00:00:17.349 --> 00:00:18.779
então vamos começar.

00:00:18.812 --> 00:00:21.236
Importamos os recursos
de costume

00:00:21.269 --> 00:00:25.109
e criamos um input
e os dados de treinamento-alvo.

00:00:25.142 --> 00:00:28.116
Um exemplo é usar uma senoide
como input,

00:00:28.149 --> 00:00:31.095
por ter variância e formato
de uma tarefa interessante,

00:00:31.128 --> 00:00:32.898
além de ser previsível.

00:00:32.931 --> 00:00:36.192
Criaremos o input de amostra e
a sequência-alvo de pontos de dados

00:00:36.225 --> 00:00:39.583
com comprimento igual a 20,
que especificamos em seq_length.

00:00:39.616 --> 00:00:42.311
As RNNs funcionam
com dados sequenciais,

00:00:42.344 --> 00:00:46.808
portanto, esse será o comprimento
da sequência do input.

00:00:46.841 --> 00:00:50.010
Isso indicará a quantidade
de palavras em uma frase

00:00:50.043 --> 00:00:52.121
ou o comprimento
dos dados numéricos,

00:00:52.154 --> 00:00:53.649
como é o caso aqui.

00:00:53.682 --> 00:00:57.090
Nestas duas linhas,
geramos o início de uma senoide

00:00:57.123 --> 00:01:00.106
em um intervalo de 0 a pi
instantes de tempo.

00:01:00.139 --> 00:01:05.114
Criamos a quantidade de pontos,
que é seq_length = 20 + 1,

00:01:05.147 --> 00:01:07.196
depois remodelamos
os dados da senoide

00:01:07.229 --> 00:01:09.809
para ter mais uma dimensão -
a de tamanho de input -

00:01:09.842 --> 00:01:11.227
que será igual a 1.

00:01:11.260 --> 00:01:13.292
Para criar o input
e a sequência-alvo

00:01:13.325 --> 00:01:15.979
do comprimento desejado,
dizemos que o input X

00:01:16.012 --> 00:01:18.763
é igual a tudo menos
ao último ponto dos dados,

00:01:18.796 --> 00:01:21.989
e que o alvo Y é igual a tudo,
menos ao primeiro ponto.

00:01:22.022 --> 00:01:24.172
Então X e Y conterão
20 pontos de dados

00:01:24.205 --> 00:01:26.236
e terão tamanho de input
igual a 1.

00:01:26.269 --> 00:01:29.787
Por fim, exibimos os dados
usando o mesmo eixo X.

00:01:29.820 --> 00:01:32.076
Vemos que o input X
está em vermelho

00:01:32.109 --> 00:01:35.405
e o alvo Y - alterado em 1 -
está em azul.

00:01:35.438 --> 00:01:37.491
Se observarmos este ponto,
por exemplo,

00:01:37.524 --> 00:01:39.035
no mesmo instante de tempo,

00:01:39.068 --> 00:01:43.076
Y é basicamente X alterado
um instante de tempo no futuro.

00:01:43.109 --> 00:01:46.228
E é isso que desejamos.
Agora temos os dados de treinamento

00:01:46.261 --> 00:01:49.883
e o próximo passo será definir a RNN
para aprender com os dados.

00:01:49.916 --> 00:01:51.956
Podemos definir a RNN
como de costume,

00:01:51.989 --> 00:01:55.492
como uma classe nn
da biblioteca PyTorch.

00:01:55.525 --> 00:01:59.227
A sintaxe será parecida
com a definição das CNNs.

00:01:59.260 --> 00:02:01.436
Vamos clicar
na documentação da RNN

00:02:01.469 --> 00:02:05.140
para ler sobre os parâmetros
recorrentes usados como input.

00:02:05.173 --> 00:02:08.184
Aqui está a documentação
para uma camada RNN.

00:02:08.217 --> 00:02:11.282
Vemos que ela é responsável
por calcular um estado oculto

00:02:11.315 --> 00:02:12.608
a partir dos inputs.

00:02:12.641 --> 00:02:15.493
Para definir uma camada assim,
usamos estes parâmetros:

00:02:15.526 --> 00:02:17.305
input_size, hidden_size,

00:02:17.338 --> 00:02:19.978
num_layers,
entre outros argumentos.

00:02:20.011 --> 00:02:23.418
O input_size é a quantidade
de recursos de input.

00:02:23.451 --> 00:02:27.185
No nosso caso, teremos inputs
com 20 valores em sequência

00:02:27.218 --> 00:02:29.850
e 1 nos recursos
do input_size.

00:02:29.883 --> 00:02:34.634
Isso é como a profundidade
de uma imagem nas CNNs.

00:02:34.667 --> 00:02:38.011
A seguir, temos hidden_size,
que define a quantidade de recursos

00:02:38.044 --> 00:02:40.402
que haverá em uma RNN
e o estado oculto.

00:02:40.435 --> 00:02:43.458
Temos a quantidade de camadas,
que, se for maior do que 1,

00:02:43.491 --> 00:02:46.216
significará que as RNNs
serão empilhadas.

00:02:46.249 --> 00:02:49.147
Por fim, preste atenção
neste parâmetro batch_first.

00:02:49.180 --> 00:02:52.647
True significa que os tensores
de input e output que fornecemos

00:02:52.680 --> 00:02:55.121
terão o tamanho do lote
como primeira dimensão.

00:02:55.154 --> 00:02:57.809
Isso, na maioria dos casos,
será verdadeiro.

00:02:57.842 --> 00:03:01.826
É assim que definimos uma camada RNN
e, posteriormente, a função forward.

00:03:01.859 --> 00:03:04.777
Veremos que ela pega um input
e um estado inicial oculto

00:03:04.810 --> 00:03:07.760
e produz um output
com um novo estado oculto.

00:03:07.793 --> 00:03:09.969
Muito bem, vamos retornar
ao notebook.

00:03:10.002 --> 00:03:13.689
Aqui eu defino uma camada RNN
self.rnn.

00:03:13.722 --> 00:03:18.034
Ela usa input_size e hidden_dim,
que definem a quantidade de recursos

00:03:18.067 --> 00:03:20.483
que haverá no output da RNN.

00:03:20.516 --> 00:03:21.989
Depois ela usa n_layers,

00:03:22.022 --> 00:03:24.757
que permite criar
uma RNN empilhada, se desejarmos.

00:03:24.790 --> 00:03:28.022
Este é tipicamente
um valor entre uma ou três camadas.

00:03:28.055 --> 00:03:30.501
Por fim, configuramos batch_first
como True,

00:03:30.534 --> 00:03:32.976
porque moldo o input
para que o tamanho do lote

00:03:33.009 --> 00:03:34.574
use a primeira dimensão.

00:03:34.607 --> 00:03:37.573
Para completar o modelo,
adicionamos mais uma camada,

00:03:37.606 --> 00:03:39.645
a última camada
completamente conectada.

00:03:39.678 --> 00:03:42.658
Essa camada produzirá
a quantidade de outputs -

00:03:42.691 --> 00:03:46.510
output_size - que desejarmos
a partir do output da RNN.

00:03:46.543 --> 00:03:51.141
Estes parâmetros serão passados
para a RNN criada.

00:03:51.174 --> 00:03:53.859
Armazenamos o valor
da dimensão oculta

00:03:53.892 --> 00:03:56.002
para podemos usar
na função forward.

00:03:56.035 --> 00:03:59.858
Nela, especificamos como um lote
de sequências de inputs

00:03:59.891 --> 00:04:01.385
passará pelo modelo.

00:04:01.418 --> 00:04:05.306
Perceba que a forward
pega o input X e o estado oculto.

00:04:05.339 --> 00:04:08.187
O primeiro passo é obter
o tamanho do lote de input

00:04:08.220 --> 00:04:10.098
chamando x.size(0).

00:04:10.131 --> 00:04:12.625
Passo o input inicial
e o estado oculto

00:04:12.658 --> 00:04:14.377
para a camada RNN.

00:04:14.410 --> 00:04:18.106
Isso produzirá o output RNN
e um novo estado oculto.

00:04:18.139 --> 00:04:20.369
Agora chamamos view
no output RNN

00:04:20.402 --> 00:04:22.402
para moldar
do tamanho desejado.

00:04:22.435 --> 00:04:23.847
Ele terá o tamanho do lote

00:04:23.880 --> 00:04:25.788
vezes o comprimento
da sequência

00:04:25.821 --> 00:04:28.152
e a quantidade de colunas
da dimensão oculta.

00:04:28.185 --> 00:04:31.534
Esse é como um passo de achatamento,
no qual preparamos o output

00:04:31.567 --> 00:04:33.815
para uma camada
completamente conectada.

00:04:33.848 --> 00:04:37.023
Passamos o output moldado
para a camada conectada,

00:04:37.056 --> 00:04:41.847
que retornará o output final
e o estado oculto gerado pela RNN.

00:04:41.880 --> 00:04:43.211
Como último passo,

00:04:43.244 --> 00:04:46.265
criamos dados de texto
e uma RNN de texto

00:04:46.298 --> 00:04:48.420
para ver se funciona
conforme o esperado.

00:04:48.453 --> 00:04:50.946
O erro mais comum
ao programar RNNs

00:04:50.979 --> 00:04:53.545
é bagunçar
a dimensão dos dados,

00:04:53.578 --> 00:04:56.443
então vamos conferir
se está tudo como o esperado.

00:04:56.476 --> 00:04:58.674
Aqui criamos uma RNN de texto

00:04:58.707 --> 00:05:00.881
com input e output iguais a 1,

00:05:00.914 --> 00:05:03.818
dimensão oculta igual a 10
e duas camadas.

00:05:03.851 --> 00:05:06.856
Podemos mudar a dimensão oculta
e a quantidade de camadas.

00:05:06.889 --> 00:05:10.570
Só queremos ver se o formato
está conforme o esperado.

00:05:10.603 --> 00:05:12.290
Aqui criamos dados de teste

00:05:12.323 --> 00:05:14.328
do tamanho
do comprimento da sequência,

00:05:14.361 --> 00:05:16.762
convertemos os dados
em um tipo de dado tensor

00:05:16.795 --> 00:05:20.023
e voltamos a primeira dimensão
para tamanho de lote igual a 1,

00:05:20.056 --> 00:05:21.277
como primeira dimensão.

00:05:21.310 --> 00:05:23.393
Agora imprimimos o Input_size,

00:05:23.426 --> 00:05:26.033
passando para a RNN de teste
como input.

00:05:26.066 --> 00:05:28.166
Isso exige
um estado inicial oculto,

00:05:28.199 --> 00:05:30.542
que será, inicialmente,
igual a "None".

00:05:30.575 --> 00:05:33.350
Isso deve retornar um output
e um estado oculto.

00:05:33.383 --> 00:05:35.470
Também imprimiremos
esses tamanhos.

00:05:35.503 --> 00:05:38.069
O tamanho de input
é um tensor 3-D,

00:05:38.102 --> 00:05:39.615
que é o que eu esperava.

00:05:39.648 --> 00:05:42.460
A primeira dimensão é igual a 1,
para tamanho de lote,

00:05:42.493 --> 00:05:44.254
20, para comprimento
de sequência,

00:05:44.287 --> 00:05:47.054
e a quantidade de recursos
de input é igual a 1,

00:05:47.087 --> 00:05:48.558
como especificamos aqui.

00:05:48.591 --> 00:05:50.718
O tamanho do output
é um tensor 2-D.

00:05:50.751 --> 00:05:53.842
Isso porque, na função forward
da definição do modelo,

00:05:53.875 --> 00:05:58.298
o tamanho do lote e o comprimento
da sequência viraram um parâmetro.

00:05:58.331 --> 00:06:01.315
Tamanho do lote vezes comprimento
da sequência é igual a 20,

00:06:01.348 --> 00:06:03.434
então temos
tamanho de output igual a 1.

00:06:03.467 --> 00:06:05.123
Por fim, temos o estado oculto.

00:06:05.156 --> 00:06:07.480
A primeira dimensão
é a quantidade de camadas

00:06:07.513 --> 00:06:10.071
especificada
na definição do modelo, 2.

00:06:10.104 --> 00:06:11.911
A seguir, temos o valor 1,

00:06:11.944 --> 00:06:14.855
que é o tamanho do lote
do input.

00:06:14.888 --> 00:06:19.407
Por fim, a última dimensão é 10,
que é a dimensão oculta.

00:06:19.440 --> 00:06:22.135
Tudo isso parece ótimo
e conforme o esperado.

00:06:22.168 --> 00:06:23.403
Podemos seguir.

00:06:23.436 --> 00:06:26.103
A seguir, veremos como treinar
um modelo como este.

