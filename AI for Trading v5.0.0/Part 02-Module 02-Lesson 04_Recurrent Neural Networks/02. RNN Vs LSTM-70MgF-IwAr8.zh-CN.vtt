WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.669
好了 假设我们有一个普通神经网络

00:00:02.669 --> 00:00:05.429
能够识别图像 我们输入了这张图

00:00:05.429 --> 00:00:08.339
于是神经网络猜测这张图最可能是

00:00:08.339 --> 00:00:13.070
一只狗 小概率是一头狼 更小的概率是一条金鱼

00:00:13.070 --> 00:00:15.033
但如果这张图真是一头狼呢？

00:00:15.032 --> 00:00:17.108
神经网络要怎么知道？

00:00:17.109 --> 00:00:20.445
假设我们正在看一个关于自然的电视节目

00:00:20.445 --> 00:00:24.588
在狼之前的图像是一头熊 再之前是一只狐狸

00:00:24.588 --> 00:00:27.209
这种情况下 我们想用这个信息来提示

00:00:27.210 --> 00:00:30.274
最后一张图是一头狼而不是一条狗

00:00:30.274 --> 00:00:34.755
要做到这点 我们需要用同一个神经网络来分析每张图

00:00:34.755 --> 00:00:39.765
但是 我们要用神经网络的输出作为下一次输入的部分内容

00:00:39.765 --> 00:00:42.798
这能切实提高效果

00:00:42.798 --> 00:00:44.228
从数学的角度看 这很简单

00:00:44.228 --> 00:00:46.664
我们只需要把线性函数的向量结合起来

00:00:46.664 --> 00:00:49.798
代入激活函数里

00:00:49.798 --> 00:00:52.600
这个激活函数可以是 sigmoid 也可以是双曲正切函数

00:00:52.600 --> 00:00:54.929
这么做 我们就可以把之前的信息利用起来

00:00:54.929 --> 00:00:58.079
最后那个神经网络就会知道节目是关于森林里的野生动物的

00:00:58.079 --> 00:01:00.868
并用这个信息来正确预测

00:01:00.868 --> 00:01:04.025
图像是一头狼 并非一条狗

00:01:04.025 --> 00:01:07.140
这基本上就是回归神经网络的工作原理了

00:01:07.140 --> 00:01:08.680
不过还有些问题

00:01:08.680 --> 00:01:10.480
假设熊已经出现有一会了

00:01:10.480 --> 00:01:12.640
而最近出现的两张图分别是一棵树和一只松鼠

00:01:12.640 --> 00:01:14.670
根据这两者

00:01:14.670 --> 00:01:17.165
我们无法知道新图像是一条狗还是一头狼

00:01:17.165 --> 00:01:19.650
因为树和松鼠与森林动物的联系

00:01:19.650 --> 00:01:22.349
和与家禽的联系是一样的

00:01:22.349 --> 00:01:26.409
所以 “在森林里”这个信息会一路追溯到熊的身上

00:01:26.409 --> 00:01:28.125
但我们也知道

00:01:28.125 --> 00:01:30.269
输入信息会不断地

00:01:30.269 --> 00:01:33.120
被 sigmoid 函数吸收掉 更糟的是

00:01:33.120 --> 00:01:36.329
一路用反向传播往回训练网络

00:01:36.328 --> 00:01:40.238
会造成梯度消失等问题

00:01:40.239 --> 00:01:43.730
所以到这个时候 几乎所有关于熊的信息都丢失了

00:01:43.730 --> 00:01:46.147
这就是回归神经网络的问题

00:01:46.147 --> 00:01:49.213
其储存记忆通常都是短期记忆

00:01:49.213 --> 00:01:53.203
RNN 并不擅长储存长期记忆

00:01:53.203 --> 00:01:57.559
于是这时候 LSTM 也就是长短期记忆网络就派上了用场

00:01:57.560 --> 00:02:00.530
简单地总结下 RNN 的工作原理

00:02:00.530 --> 00:02:03.269
记忆输入进来 与当前事件结合在一起

00:02:03.269 --> 00:02:06.299
然后输出出来 输出会预测输入是什么东西

00:02:06.299 --> 00:02:09.870
并作为下一轮神经网络输入的一部分

00:02:09.870 --> 00:02:12.954
LSTM 的工作原理与之类似

00:02:12.954 --> 00:02:16.139
它会一直跟踪记忆 不仅是记忆 而且是长期记忆

00:02:16.139 --> 00:02:18.254
记忆输入进来 又输出出去

00:02:18.253 --> 00:02:19.560
还有短期记忆

00:02:19.560 --> 00:02:21.539
也是输入进来 输出出去

00:02:21.538 --> 00:02:22.954
每一步

00:02:22.955 --> 00:02:25.695
事件的长期和短期记忆会被合并起来

00:02:25.694 --> 00:02:28.500
由此我们得到了一个新的长期记忆、

00:02:28.500 --> 00:02:31.030
短期记忆以及一个预测结果

00:02:31.030 --> 00:02:33.538
更好地保护了旧信息

00:02:33.538 --> 00:02:35.008
必要的话

00:02:35.008 --> 00:02:37.899
这个网络可以记住很久以前的事情

00:02:37.900 --> 00:02:43.000
所以 在接下来的几个视频中 我会为你介绍 LSTM 的结构及工作原理

