WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.844
我们来重温一下 现在有以下问题 我们在看一个电视节目

00:00:04.844 --> 00:00:07.450
我们有一个长期记忆 即记得这个节目是关于自然

00:00:07.450 --> 00:00:10.759
和科学的 而且里面出现了很多动物

00:00:10.759 --> 00:00:12.839
我们还有一个短期记忆

00:00:12.839 --> 00:00:15.403
也就是我们记得最近看到的东西 也即松鼠和树

00:00:15.403 --> 00:00:18.629
还有一个当前事件 即刚看到的东西

00:00:18.629 --> 00:00:21.704
也就是一条狗的图像 但那也有可能是一头狼

00:00:21.704 --> 00:00:26.405
我们想将这三者合并起来 从而预测图像是什么

00:00:26.405 --> 00:00:29.760
在这个案例中 长期记忆 即说明这个节目是关于森林动物的记忆

00:00:29.760 --> 00:00:34.060
暗示我们图像是一头狼 而不是一条狗

00:00:34.060 --> 00:00:37.109
我们还想用这三个信息 长期记忆、

00:00:37.109 --> 00:00:38.579
短期记忆和事件

00:00:38.579 --> 00:00:40.743
来帮我们更新长期记忆

00:00:40.743 --> 00:00:42.679
假设我们只记得节目是关于自然的

00:00:42.679 --> 00:00:44.844
但忘了它是关于科学的

00:00:44.844 --> 00:00:46.505
我们还记得这个节目是关于森林动物和树木的

00:00:46.505 --> 00:00:49.590
因为我们最近看到了一棵树

00:00:49.590 --> 00:00:52.609
所以我们给长期记忆加了点东西 也减了点东西

00:00:52.609 --> 00:00:54.509
最后我们还想用

00:00:54.509 --> 00:00:57.664
这三种信息来帮自己更新短期记忆

00:00:57.664 --> 00:00:59.399
假设对于短期记忆 你想

00:00:59.399 --> 00:01:01.380
忘记节目里有树 记住

00:01:01.380 --> 00:01:06.055
里面有狼 因为树是几张图像前出现的 而我们刚看到一头狼

00:01:06.055 --> 00:01:08.099
基本上我们就得到了一个像这样的结构

00:01:08.099 --> 00:01:10.769
我们来用更多动物代表不同的记忆阶段

00:01:10.769 --> 00:01:15.544
长期记忆用一头大象表示 因为大象有长期记忆

00:01:15.543 --> 00:01:17.619
短期记忆用

00:01:17.620 --> 00:01:22.129
善忘的鱼儿表示 事件则仍用我们刚看到的狼来表示

00:01:22.129 --> 00:01:25.650
所以 LSTM 的工作原理是 这三种信息进入到节点里

00:01:25.650 --> 00:01:27.930
进行了些数学运算

00:01:27.930 --> 00:01:30.760
之后信息得到更新并输出出来

00:01:30.760 --> 00:01:31.974
这是长期记忆、

00:01:31.974 --> 00:01:35.010
短期记忆和事件预测

00:01:35.010 --> 00:01:38.799
更具体地说 LSTM 结构包括几个门

00:01:38.799 --> 00:01:40.189
一个遗忘门、

00:01:40.188 --> 00:01:43.824
一个学习门、一个记忆门和一个使用门

00:01:43.825 --> 00:01:45.954
这些门的基本工作原理如下

00:01:45.953 --> 00:01:47.429
长期记忆进入遗忘门

00:01:47.430 --> 00:01:51.650
忘记它认为没有用处的一切

00:01:51.650 --> 00:01:55.530
短期记忆和事件在学习门里合并到一起

00:01:55.530 --> 00:01:57.599
囊括了我们刚学到的东西

00:01:57.599 --> 00:02:00.930
并移除掉一切不必要的信息

00:02:00.930 --> 00:02:04.019
然后 我们还没遗忘的长期记忆和

00:02:04.019 --> 00:02:07.864
刚学到的新信息会在记忆门里合并到一起

00:02:07.864 --> 00:02:11.454
这个门把这两者放到一起 由于它叫记忆门

00:02:11.454 --> 00:02:14.860
所以它会输出更新后的长期记忆

00:02:14.860 --> 00:02:17.460
这就是我们未来会记住的事情

00:02:17.460 --> 00:02:19.530
最后 使用门会决定

00:02:19.530 --> 00:02:21.938
要从之前知道的信息

00:02:21.938 --> 00:02:23.938
以及刚学到的信息中挑出什么来使用

00:02:23.938 --> 00:02:27.060
从而作出预测 所以它也接受长期记忆

00:02:27.060 --> 00:02:31.093
和新信息的输入 把它们合并到一起并决定要输出什么

00:02:31.093 --> 00:02:35.508
所以输出就包括了预测和新的短期记忆

00:02:35.508 --> 00:02:37.559
我们来站在更高的角度观察这个具体过程

00:02:37.560 --> 00:02:39.658
我们有长期记忆和

00:02:39.658 --> 00:02:44.370
短期记忆输入进来 我们把它们分别叫做 LTM 和 STM

00:02:44.370 --> 00:02:48.245
然后一个事件和一个输出会在 LSTM 里进出

00:02:48.245 --> 00:02:50.324
然后这些会进入下一个节点

00:02:50.324 --> 00:02:53.004
循环往复

00:02:53.003 --> 00:02:55.929
总地来说 到了 t 时 我们会把所有东西

00:02:55.930 --> 00:03:02.000
都下标 t 如图 信息会从 t - 1 时传递到 t 时

