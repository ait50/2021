WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.370
就是这样 正如我们之前看到的

00:00:02.370 --> 00:00:05.878
这就是 LSTM 的结构 里面有四个门

00:00:05.878 --> 00:00:07.003
这是遗忘门

00:00:07.003 --> 00:00:09.993
会接受长期记忆并遗忘掉一部分

00:00:09.993 --> 00:00:12.599
学习门会把短期记忆和

00:00:12.599 --> 00:00:16.035
事件放到一起 作为我们最近学到的信息

00:00:16.035 --> 00:00:20.070
记忆门则把还没有遗忘的长期记忆

00:00:20.070 --> 00:00:24.760
和刚学到的新信息放到一起 以便更新长期记忆并将其输出

00:00:24.760 --> 00:00:27.629
最后 使用门也会把我们刚学到的信息

00:00:27.629 --> 00:00:30.719
和还没遗忘的长期记忆放到一起

00:00:30.719 --> 00:00:34.875
从而作出预测并更新短期记忆

00:00:34.875 --> 00:00:36.734
这就是把所有东西放到一起的样子

00:00:36.734 --> 00:00:38.993
并不复杂 对吗？

00:00:38.993 --> 00:00:40.695
你可能在想 等等

00:00:40.695 --> 00:00:42.350
看着太抽象了

00:00:42.350 --> 00:00:45.520
为什么有时候要用 tanh 有时候却要用 sigmoid？

00:00:45.520 --> 00:00:47.574
为什么有时候要相乘 有时候又要相加？

00:00:47.573 --> 00:00:50.369
有时候又要用更复杂的线性函数？

00:00:50.369 --> 00:00:52.169
你可能会想到别的结构

00:00:52.170 --> 00:00:54.329
一些更合理或更简单的结构

00:00:54.329 --> 00:00:55.908
你是对的

00:00:55.908 --> 00:00:57.613
这是个任意结构

00:00:57.613 --> 00:00:59.339
就像深度学习的很多事情一样

00:00:59.340 --> 00:01:03.170
之所以结构是这样 是因为这样行得通

00:01:03.170 --> 00:01:04.349
在下一节课中

00:01:04.349 --> 00:01:06.689
我们会看到其它结构 可能更简单

00:01:06.688 --> 00:01:09.818
也可能更复杂 但都行得通

00:01:09.819 --> 00:01:12.180
不过欢迎你寻找其它结构并动手实验

00:01:12.180 --> 00:01:14.819
这个领域还处于早期探索阶段 如果你

00:01:14.819 --> 00:01:18.000
想到了其它结构 而且该结构行得通 那就太棒了

