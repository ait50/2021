WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.745
好的创建迷你批次数据后 现在该定义模型了

00:00:04.745 --> 00:00:08.115
此图表显示了模型的结构

00:00:08.115 --> 00:00:12.915
将这些字符传入输入层 然后堆叠 LSTM 单元

00:00:12.915 --> 00:00:17.970
这些 LSTM 单元构成了隐藏递归层 将迷你批次的数据传入这些递归层时

00:00:17.970 --> 00:00:20.130
它们将一次查看一个字符

00:00:20.130 --> 00:00:22.920
并生成输出和隐藏状态

00:00:22.920 --> 00:00:25.500
我们将输入字符传入第一个 LSTM 单元

00:00:25.500 --> 00:00:28.575
该单元生成一个隐藏状态

00:00:28.575 --> 00:00:30.250
在下个时间步

00:00:30.250 --> 00:00:33.450
查看序列中的下个字符并将该字符传入此 LSTM 单元中

00:00:33.450 --> 00:00:37.495
同时将上个隐藏状态当做输入

00:00:37.495 --> 00:00:42.170
你已经在单层 RNN 中见过此行为

00:00:42.170 --> 00:00:46.520
但是在此例中 我们打算使用双层模型 将 LSTM 层级堆叠起来

00:00:46.520 --> 00:00:50.270
所以此 LSTM 层的输出将作为输入进入下个层级

00:00:50.270 --> 00:00:52.790
每个单元都会

00:00:52.790 --> 00:00:56.330
与展开层级中的下个单元分享隐藏状态

00:00:56.330 --> 00:00:59.930
最后一个 LSTM 层级的输出

00:00:59.930 --> 00:01:03.635
将包含一些字符类别分数 长度等于词汇表的长度

00:01:03.635 --> 00:01:07.730
对此输出应用 Softmax 激活函数

00:01:07.730 --> 00:01:11.660
并获得概率分布以预测下个最有可能的字符

00:01:11.660 --> 00:01:14.320
为了帮助你开始此任务

00:01:14.320 --> 00:01:17.015
我们提供了创建模型的框架代码

00:01:17.015 --> 00:01:19.640
首先 我们将检查是否有 GPU 可用于训练

00:01:19.640 --> 00:01:23.370
然后是这个 CharRNN 类

00:01:23.370 --> 00:01:26.270
这个类包含

00:01:26.270 --> 00:01:29.190
常见的 init 和 forward 函数

00:01:29.190 --> 00:01:31.910
我们提供了初始化 LSTM 层级隐藏状态的代码

00:01:31.910 --> 00:01:35.070
稍后我会讲解这些代码的

00:01:35.070 --> 00:01:38.240
你可以自己查看下这些代码

00:01:38.240 --> 00:01:41.770
看看我们是如何创建初始字符字典的 但是不需要更改代码

00:01:41.770 --> 00:01:45.170
在实例化 CharRNN 时我们会传入几个参数

00:01:45.170 --> 00:01:49.555
我将其中几个参数的值保存为类变量

00:01:49.555 --> 00:01:52.130
你需要使用这些输入参数和变量

00:01:52.130 --> 00:01:55.990
创建模型层级并完成 forward 函数

00:01:55.990 --> 00:02:00.420
首先需要创建一个 LSTM 层级 你可以点击此链接并阅读 LSTM 文档

00:02:00.420 --> 00:02:04.510
我们可以使用常见的参数创建 LSTM 层级

00:02:04.510 --> 00:02:06.330
这些参数包括 input_size、hidden_size

00:02:06.330 --> 00:02:08.930
num_layers 和 batch_first

00:02:08.930 --> 00:02:11.140
还需要添加 dropout 值

00:02:11.140 --> 00:02:14.210
如果你决定堆叠多个层级

00:02:14.210 --> 00:02:17.855
此值将在 LSTM 层级的输出之间引入丢弃层

00:02:17.855 --> 00:02:20.210
定义 LSTM 层级之后

00:02:20.210 --> 00:02:22.410
还需要定义两个层级

00:02:22.410 --> 00:02:27.320
一个是丢弃层 一个是最终全连接层 后者用于获取期望的输出大小

00:02:27.320 --> 00:02:28.740
定义好这些层级之后

00:02:28.740 --> 00:02:31.000
需要定义 forward 函数

00:02:31.000 --> 00:02:33.770
参数包括输入 x 和隐藏状态

00:02:33.770 --> 00:02:35.900
你需要使此输入经过模型层级

00:02:35.900 --> 00:02:38.975
并返回最终输出和隐藏状态

00:02:38.975 --> 00:02:41.540
确保改变 LSTM 输出的形状

00:02:41.540 --> 00:02:44.350
使其能够传入最终全连接层

00:02:44.350 --> 00:02:45.940
Ok在底部

00:02:45.940 --> 00:02:49.720
是一个初始化 LSTM 隐藏状态的函数

00:02:49.720 --> 00:02:54.775
LSTM 有一个隐藏状态和单元状态 它们存储为元组 hidden

00:02:54.775 --> 00:02:57.290
隐藏状态和单元状态的形状由以下参数决定：

00:02:57.290 --> 00:02:59.840
模型的层级数量

00:02:59.840 --> 00:03:01.420
输入的批次大小

00:03:01.420 --> 00:03:04.730
以及在创建模型时指定的隐藏维度

00:03:04.730 --> 00:03:07.610
在此函数中 我们将隐藏状态全初始化为 0

00:03:07.610 --> 00:03:10.250
如果有 GPU 则移到 GPU 上

00:03:10.250 --> 00:03:13.290
你不需要更改看到的所有这些代码

00:03:13.290 --> 00:03:16.690
只需定义模型层级和前馈行为

00:03:16.690 --> 00:03:18.555
如果代码实现正确

00:03:18.555 --> 00:03:21.065
你应该能够设置模型超参数

00:03:21.065 --> 00:03:24.230
继续训练模型并生成一些示例文本

00:03:24.230 --> 00:03:26.360
自己尝试一下

00:03:26.360 --> 00:03:27.800
然后看看我的答案

