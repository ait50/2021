WEBVTT
Kind: captions
Language: pt-BR

00:00:00.201 --> 00:00:02.869
Nós queremos definir
uma RNN de caracteres

00:00:02.903 --> 00:00:04.604
com uma LSTM de duas camadas.

00:00:04.638 --> 00:00:08.175
Na minha solução,
eu executei este código na GPU.

00:00:08.208 --> 00:00:11.711
E aqui está o código para definir
a RNN de caracteres.

00:00:11.745 --> 00:00:15.415
Primeiro defino uma camada LSTM:
self.lstm.

00:00:15.448 --> 00:00:18.552
Ela contém um tamanho de input
que será o comprimento

00:00:18.585 --> 00:00:20.787
do caractere de input
codificado em one-hot.

00:00:20.820 --> 00:00:23.657
Este é o comprimento
dos meus caracteres distintos.

00:00:23.690 --> 00:00:26.593
E ela contém uma dimensão oculta,
um número de camadas

00:00:26.626 --> 00:00:29.296
e uma probabilidade de dropout
que especificamos.

00:00:29.329 --> 00:00:31.598
E isto vai criar
uma camada dropout

00:00:31.631 --> 00:00:33.900
entre múltiplas camadas LSTM.

00:00:33.934 --> 00:00:37.204
E tudo isto serão parâmetros
passados como inputs

00:00:37.237 --> 00:00:39.206
para a RNN,
quando for construída.

00:00:39.239 --> 00:00:41.541
E eu configurei
batch_first=True,

00:00:41.575 --> 00:00:43.443
pois quando criamos
os dados do lote,

00:00:43.476 --> 00:00:45.579
a primeira dimensão
é o tamanho do lote

00:00:45.612 --> 00:00:47.414
em vez do comprimento
da sequência.

00:00:47.447 --> 00:00:49.649
Em seguida, eu defino
uma camada dropout

00:00:49.683 --> 00:00:53.019
entre a camada LSTM
e a camada linear.

00:00:53.053 --> 00:00:56.690
E fc, a última camada
completamente conectada.

00:00:56.723 --> 00:01:01.228
Ela contém os outputs LSTM,
que terão dimensão n_hidden,

00:01:01.261 --> 00:01:03.763
e vai fazer o output
da pontuação de classes

00:01:03.797 --> 00:01:06.099
para o próximo caractere
mais provável.

00:01:06.132 --> 00:01:09.102
Estas são as pontuações
dos caracteres mais prováveis,

00:01:09.135 --> 00:01:11.638
então o tamanho de output
e o de input são iguais:

00:01:11.671 --> 00:01:13.874
o comprimento do vocabulário
de caracteres.

00:01:13.907 --> 00:01:16.243
Agora vou
para a função forward.

00:01:16.276 --> 00:01:18.645
Estou passando meu input X
e o estado oculto

00:01:18.678 --> 00:01:20.447
para a camada LSTM.

00:01:20.480 --> 00:01:24.217
Isto produz o output da LSTM
e um novo estado oculto.

00:01:24.251 --> 00:01:27.153
Vou passar o output LSTM
pela camada dropout

00:01:27.187 --> 00:01:29.456
que defini aqui
para conseguir um novo output.

00:01:29.489 --> 00:01:31.491
Vou remodelar o output,

00:01:31.525 --> 00:01:34.194
para que a última dimensão
seja a dimensão oculta.

00:01:34.227 --> 00:01:38.565
Este -1 significa que vou empilhar
os outputs da LSTM.

00:01:38.598 --> 00:01:41.134
E, por último, eu passo
este output remodelado

00:01:41.168 --> 00:01:43.035
para a camada
completamente conectada.

00:01:43.069 --> 00:01:44.905
E vou retornar o output final

00:01:44.938 --> 00:01:47.707
e o estado oculto
que foi gerado pela LSTM.

00:01:47.741 --> 00:01:50.744
Estas duas funções mais
a função init_hidden

00:01:50.777 --> 00:01:52.078
completam meu modelo.

00:01:52.112 --> 00:01:53.980
Agora é hora de treinar.

00:01:54.014 --> 00:01:57.551
Vamos dar uma olhada no loop
de treino que foi fornecido.

00:01:57.584 --> 00:02:00.787
Esta função contém o modelo
que será treinado, os dados,

00:02:00.820 --> 00:02:02.589
os números de epochs
que treinaremos,

00:02:02.622 --> 00:02:04.890
o tamanho do lote
e o comprimento da sequência

00:02:04.923 --> 00:02:06.693
que definem o tamanho
do minilote.

00:02:06.726 --> 00:02:08.995
Ela também contém outros
parâmetros de treino.

00:02:09.028 --> 00:02:13.333
Aqui, eu defino o otimizador
e a função de perda.

00:02:13.366 --> 00:02:15.902
O otimizador é um otimizador
Adam padrão

00:02:15.936 --> 00:02:19.306
com a taxa de aprendizado
configurada aqui.

00:02:19.339 --> 00:02:21.641
E usamos a função de perda
de entropia cruzada,

00:02:21.675 --> 00:02:24.778
útil para fazer output de pontuações
de classe de caracteres.

00:02:24.811 --> 00:02:28.248
Aqui veremos alguns detalhes
sobre criar dados de validação

00:02:28.281 --> 00:02:31.218
e mover os dados para a GPU,
se estiver disponível.

00:02:31.251 --> 00:02:33.787
E aqui você pode ver o começo
do loop epoch.

00:02:33.820 --> 00:02:35.622
No começo de cada epoch,

00:02:35.655 --> 00:02:38.191
eu inicializo
o estado oculto da LSTM.

00:02:38.225 --> 00:02:40.927
Ele contém o tamanho
do lote dos dados

00:02:40.961 --> 00:02:42.896
para definir o tamanho
da camada oculta

00:02:42.929 --> 00:02:45.999
e retorna um estado oculto
e um estado de célula só com zeros.

00:02:46.032 --> 00:02:49.169
Dentro deste loop epoch,
tem o loop de lote.

00:02:49.202 --> 00:02:53.507
Ele pega os minilotes de X e Y
do gerador get_batches.

00:02:53.540 --> 00:02:57.244
Lembre-se de que esta função
itera pelos dados codificados

00:02:57.277 --> 00:03:00.614
e retorna lotes de inputs X
e alvos Y.

00:03:00.647 --> 00:03:02.015
Converto o input

00:03:02.048 --> 00:03:04.451
numa representação codificada
one-hot.

00:03:04.484 --> 00:03:08.188
E converto X e Y,
os inputs e os alvos,

00:03:08.221 --> 00:03:10.557
em tensores que podem ser vistos
pelo modelo.

00:03:10.590 --> 00:03:11.992
Se a GPU estiver disponível,

00:03:12.025 --> 00:03:14.928
vou mover os inputs e os alvos
para o dispositivo GPU.

00:03:14.961 --> 00:03:17.097
A próxima linha
está garantindo

00:03:17.130 --> 00:03:20.367
que separamos estados ocultos
passados de seus históricos.

00:03:20.400 --> 00:03:23.703
Lembre-se de que a camada oculta
de uma LSTM é uma tupla.

00:03:23.737 --> 00:03:26.239
Então estamos recebendo os dados
como uma tupla.

00:03:26.273 --> 00:03:28.909
Seguimos com a retropropagação,
como sempre.

00:03:28.942 --> 00:03:31.645
Nós zeramos
qualquer gradiente acumulado

00:03:31.678 --> 00:03:34.814
e passamos os tensores
de input para o modelo.

00:03:34.848 --> 00:03:37.284
Também passamos
o último estado oculto aqui.

00:03:37.317 --> 00:03:40.754
E isto retorna um output final
e um novo estado oculto.

00:03:40.787 --> 00:03:45.158
E calculamos a perda olhando
o output previsto e os alvos.

00:03:45.192 --> 00:03:47.527
Na função forward do modelo,

00:03:47.561 --> 00:03:51.398
eu espremi o batch_size
e o seq_lenght dos outputs da LSTM

00:03:51.431 --> 00:03:52.732
em uma dimensão.

00:03:52.766 --> 00:03:54.835
Então faço o mesmo
com os alvos aqui.

00:03:54.868 --> 00:03:56.903
Depois fazemos retropropagação

00:03:56.937 --> 00:03:59.172
e nos movemos
para a direção certa,

00:03:59.206 --> 00:04:00.874
atualizando os pesos da rede.

00:04:00.907 --> 00:04:02.742
Antes da otimização,

00:04:02.776 --> 00:04:05.312
inseri uma linha de código
que parece estranha.

00:04:05.345 --> 00:04:07.380
Eu usei clip_grad_norm.

00:04:07.414 --> 00:04:11.151
Este tipo de modelo LSTM tem
um problema com gradientes,

00:04:11.184 --> 00:04:13.286
eles podem explodir
e ficar bem grandes.

00:04:13.320 --> 00:04:15.655
Então podemos recortar
os gradientes.

00:04:15.689 --> 00:04:17.824
Nós configuramos
um limite de recorte

00:04:17.858 --> 00:04:20.560
e, se o gradiente
for maior que o limite,

00:04:20.594 --> 00:04:22.596
nós o mandamos
para o recorte de limite.

00:04:22.629 --> 00:04:25.465
E faremos isso
passando os parâmetros

00:04:25.498 --> 00:04:28.168
e o valor do qual
queremos recortar os gradientes.

00:04:28.201 --> 00:04:32.572
Neste caso, os valores são passados
na função de treino como valor 5.

00:04:32.606 --> 00:04:34.508
Damos um passo atrás,

00:04:34.541 --> 00:04:37.377
fazemos o recorte dos gradientes
e fazemos a otimização.

00:04:37.410 --> 00:04:39.913
Aqui, estou fazendo
algo muito similar

00:04:39.946 --> 00:04:44.217
para processar dados de validação,
menos a retropropagação.

00:04:44.251 --> 00:04:47.287
E estou imprimindo estatísticas
sobre as perdas.

00:04:47.320 --> 00:04:49.089
Com a função
de treino definida,

00:04:49.122 --> 00:04:51.791
posso instanciar e treinar
um modelo.

00:04:51.825 --> 00:04:53.426
No notebook de exercício,

00:04:53.460 --> 00:04:55.996
eu deixei hiperparâmetros
para você definir.

00:04:56.029 --> 00:05:00.901
Defini a dimensão oculta como 512,
e o número de camadas como 2,

00:05:00.934 --> 00:05:02.435
o que já falamos antes.

00:05:02.469 --> 00:05:04.804
Eu instanciei o modelo
e imprimi.

00:05:04.838 --> 00:05:07.741
Podemos ver que temos
83 caracteres distintos como input,

00:05:07.774 --> 00:05:11.611
512 como dimensão oculta
e duas camadas na LSTM.

00:05:11.645 --> 00:05:15.782
A camada dropout tem
o valor de dropout padrão de 0,5,

00:05:15.815 --> 00:05:17.984
e a última camada
completamente conectada

00:05:18.018 --> 00:05:21.021
tem os recursos de input,
os mesmos que a dimensão oculta,

00:05:21.054 --> 00:05:23.790
e os recursos de output,
o número de caracteres.

00:05:23.823 --> 00:05:27.093
E há mais hiperparâmetros
que definem o tamanho do lote,

00:05:27.127 --> 00:05:29.829
o comprimento da sequência
e o número de epochs.

00:05:29.863 --> 00:05:32.065
Defini o comprimento
da sequência para 100,

00:05:32.098 --> 00:05:33.600
que são muitos caracteres,

00:05:33.633 --> 00:05:36.770
mas isso dá ao modelo muito contexto
de onde aprender.

00:05:36.803 --> 00:05:39.005
Também quero dizer
que a dimensão oculta

00:05:39.039 --> 00:05:42.108
é o número de recursos
que seu modelo pode detectar.

00:05:42.142 --> 00:05:44.477
E valores maiores permitem
que a rede

00:05:44.511 --> 00:05:46.213
aprenda mais
recursos de texto.

00:05:46.246 --> 00:05:48.882
Há mais informações abaixo
neste notebook

00:05:48.915 --> 00:05:50.884
sobre definir hiperparâmetros.

00:05:50.917 --> 00:05:54.888
Em geral, eu tento começar
com um grande modelo como este.

00:05:54.921 --> 00:05:58.191
Várias camadas LSTM
e uma dimensão oculta grande.

00:05:58.225 --> 00:06:02.429
E observo a perda
quando este modelo treina.

00:06:02.462 --> 00:06:04.664
Se estiver diminuindo,
eu continuo,

00:06:04.698 --> 00:06:06.433
mas se não diminuir
como quero,

00:06:06.466 --> 00:06:08.635
eu mudo
alguns hiperparâmetros.

00:06:08.668 --> 00:06:10.904
Nossos dados de texto
são bem grandes.

00:06:10.937 --> 00:06:14.975
Aqui eu treino o modelo
por 20 epochs na GPU.

00:06:15.008 --> 00:06:19.212
Eu posso ver que a perda de treino
e de validação estão diminuindo.

00:06:19.246 --> 00:06:21.081
E na epoch 15,

00:06:21.114 --> 00:06:23.517
vejo que a perda está diminuindo
um pouco,

00:06:23.550 --> 00:06:27.521
mas as perdas de validação
e de treino ainda estão diminuindo

00:06:27.554 --> 00:06:28.989
mesmo após a epoch 20,

00:06:29.022 --> 00:06:32.325
então eu podia treinar
por um tempo maior.

00:06:32.359 --> 00:06:34.794
Leia as informações
sobre configuração

00:06:34.828 --> 00:06:36.563
de hiperparâmetros
de um modelo

00:06:36.596 --> 00:06:38.465
e consiga o melhor modelo.

00:06:38.498 --> 00:06:40.934
Depois de treinar o modelo,
como eu fiz,

00:06:40.967 --> 00:06:43.670
salve-o pelo nome
e faça mais um passo:

00:06:43.703 --> 00:06:47.174
usar o modelo para fazer previsões
e gerar novos textos,

00:06:47.207 --> 00:06:48.608
o que farei em seguida.

