WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.189
所以这是我们的基准情形

00:00:02.189 --> 00:00:03.990
我们有个长期记忆

00:00:03.990 --> 00:00:06.708
即记得我们看的节目是关于自然和科学的

00:00:06.708 --> 00:00:09.554
还有个短期记忆 即我们最近看到的内容

00:00:09.554 --> 00:00:11.323
也就是一只松鼠和一棵树

00:00:11.323 --> 00:00:14.219
最后 我们还有当前事件 也就是一张我们刚看到的图片

00:00:14.220 --> 00:00:17.769
有点像只狗 但也有可能是头狼

00:00:17.768 --> 00:00:19.018
我们来研究学习门

00:00:19.018 --> 00:00:21.029
学习门要做的是

00:00:21.030 --> 00:00:26.429
取短期记忆和事件 将两者合并 实际上 它做的不止这些

00:00:26.428 --> 00:00:29.729
它会接受短期记忆和事件 将两者合并起来

00:00:29.730 --> 00:00:33.783
然后忽略其中的一部分 只保留重要的部分

00:00:33.783 --> 00:00:37.199
所以这里它忘了有棵树

00:00:37.200 --> 00:00:41.685
只记得我们最近看到了一只松鼠和一条狗/一只狼

00:00:41.685 --> 00:00:43.185
这背后的数学原理是什么呢？

00:00:43.185 --> 00:00:46.939
是这样的 我们有短期记忆 STMt-1

00:00:46.939 --> 00:00:51.810
和事件 Et 学习门会把它们合并起来 也就是把它们代入一个

00:00:51.810 --> 00:00:56.130
线性函数里 也就是把两个向量放到一起

00:00:56.130 --> 00:01:01.890
然后乘以一个矩阵 再加一个偏差 最后将所得结果代入 tanh 激活函数里

00:01:01.890 --> 00:01:05.474
然后新信息 Nt 就产生了 它的结构就是这样

00:01:05.474 --> 00:01:07.118
那我们要怎么忽略其中一部分呢？

00:01:07.117 --> 00:01:11.593
我们可以将其乘以一个遗忘因子 it

00:01:11.593 --> 00:01:14.003
遗忘因子 it

00:01:14.004 --> 00:01:17.989
实际上是一个向量 但它可以像元素一样进行乘法运算

00:01:17.989 --> 00:01:19.795
要怎么计算 it 呢？

00:01:19.795 --> 00:01:24.239
这就要用到短期记忆和事件以前的信息了

00:01:24.239 --> 00:01:26.680
所以 我们要再次创建一个小型神经网络

00:01:26.680 --> 00:01:29.969
其输入为短期记忆和事件

00:01:29.968 --> 00:01:33.789
把它们代入一个小型线性函数里 在函数里乘以一个新矩阵

00:01:33.790 --> 00:01:39.045
再加一个新偏差 把所得结果代入 sigmoid 函数 使其值保持在 0 和 1 之间

00:01:39.045 --> 00:01:41.700
就是这样了 这就是学习门的工作原理

