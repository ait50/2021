WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.629
This is a notebook where you'll be building a characterwise RNN.

00:00:03.629 --> 00:00:06.419
You're going to train this on the text of Anna Karenina,

00:00:06.419 --> 00:00:09.344
which is a really great but also quite sad a book.

00:00:09.345 --> 00:00:11.220
The general idea behind this,

00:00:11.220 --> 00:00:13.380
is that we're going to be passing one character at

00:00:13.380 --> 00:00:15.870
a time into a recurrent neural network.

00:00:15.869 --> 00:00:18.089
We're going to do this for a whole bunch of text,

00:00:18.089 --> 00:00:19.594
and at the end what's going to happen,

00:00:19.594 --> 00:00:22.169
is that our network is going to be able to generate new text,

00:00:22.170 --> 00:00:23.750
one character at a time.

00:00:23.750 --> 00:00:25.475
This is the general structure.

00:00:25.475 --> 00:00:28.980
We have our input characters and we want to one-hot encode them.

00:00:28.980 --> 00:00:32.819
This one-hot vector, will be fed into a hidden recurrent layer,

00:00:32.819 --> 00:00:35.215
and then the hidden layer has two outputs.

00:00:35.215 --> 00:00:37.680
First, it produces some RNN output,

00:00:37.679 --> 00:00:39.015
and it produces a hidden state,

00:00:39.015 --> 00:00:41.000
which will continue to change and be fed to

00:00:41.000 --> 00:00:43.850
this hidden layer at the next time step in the sequence.

00:00:43.850 --> 00:00:46.785
We saw something similar in the last code example.

00:00:46.784 --> 00:00:49.879
So, our recurrent layer keeps track of our hidden state,

00:00:49.880 --> 00:00:53.625
and its output goes to a final fully connected output layer.

00:00:53.625 --> 00:00:55.070
Our linear output layer,

00:00:55.070 --> 00:00:57.320
will produce a series of character class scores.

00:00:57.320 --> 00:01:00.034
So, this output will be as long as our input vector,

00:01:00.034 --> 00:01:02.149
and we can apply a Softmax function to get

00:01:02.149 --> 00:01:05.855
a probability distribution for the most likely next character.

00:01:05.855 --> 00:01:11.189
So, this network is based off of Andrej Karpathy's post on RNNs, which you can find here.

00:01:11.189 --> 00:01:14.920
It's a really good post and so you can check out these links to read more about RNNs.

00:01:14.920 --> 00:01:19.945
[inaudible] notebook is broken into a small series of exercises that you can implement yourself.

00:01:19.944 --> 00:01:23.614
For each exercise, I'm also going to provide a solution to consult.

00:01:23.614 --> 00:01:25.909
I recommend that you open the exercise notebook in

00:01:25.909 --> 00:01:28.280
one window and watch videos in another.

00:01:28.280 --> 00:01:30.180
That way you can work alongside me.

00:01:30.180 --> 00:01:31.845
Okay, so first things first,

00:01:31.844 --> 00:01:34.420
I'm loading in and taking a look at our text data.

00:01:34.420 --> 00:01:35.920
Here, I'm loading in

00:01:35.920 --> 00:01:40.284
the Anna Karenina text file and I'm printing out the first 100 characters.

00:01:40.284 --> 00:01:43.229
The characters are everything from letters, to spaces,

00:01:43.230 --> 00:01:46.814
to newline characters, and we can see the classic first-line,

00:01:46.814 --> 00:01:48.465
"Happy families are all alike.

00:01:48.465 --> 00:01:51.344
Every unhappy family is unhappy in its own way."

00:01:51.344 --> 00:01:55.219
Then, I'll actually want to turn our text into numerical tokens.

00:01:55.219 --> 00:01:58.280
This is because our network can only learn from numerical data,

00:01:58.280 --> 00:02:02.060
and so we want to map every character in the text to a unique index.

00:02:02.060 --> 00:02:04.394
So, first off, with the text,

00:02:04.394 --> 00:02:07.409
we can just create a unique vocabulary as a set.

00:02:07.409 --> 00:02:09.965
Sets, are a built in python data structure,

00:02:09.965 --> 00:02:13.384
and what this will do, is look at every character in the past in the text.

00:02:13.384 --> 00:02:16.739
Separate it out as a string and get rid of any duplicates.

00:02:16.739 --> 00:02:20.325
So, chars, is going to be a set of all our unique characters.

00:02:20.324 --> 00:02:23.655
This is also sometimes referred to as a vocabulary.

00:02:23.655 --> 00:02:27.979
Then, I'm creating a dictionary from a vocabulary of all our characters,

00:02:27.979 --> 00:02:31.429
that maps the actual character to a unique integer.

00:02:31.430 --> 00:02:35.659
So, it's just giving a numerical value to each of our unique characters,

00:02:35.659 --> 00:02:38.210
and putting it in a dictionary int2char.

00:02:38.210 --> 00:02:39.920
Then I'm doing this the other way,

00:02:39.919 --> 00:02:43.428
where we have a dictionary that goes from integers to characters.

00:02:43.429 --> 00:02:47.270
Recall that any dictionary is made of a set of key and value pairs.

00:02:47.270 --> 00:02:48.950
In the int2char case,

00:02:48.949 --> 00:02:53.069
the keys are going to be integers and the values are going to be string characters.

00:02:53.069 --> 00:02:54.739
In the char2int case,

00:02:54.740 --> 00:02:56.780
our keys are going to be the characters and

00:02:56.780 --> 00:02:59.430
our values are going to be their unique integers.

00:02:59.430 --> 00:03:03.265
So, these basically give us a way to encode text as numbers.

00:03:03.264 --> 00:03:05.154
Here, I am doing just that.

00:03:05.155 --> 00:03:08.854
I'm encoding each character in the text as an integer.

00:03:08.854 --> 00:03:11.044
This creates an encoded text,

00:03:11.044 --> 00:03:14.000
and just like I printed the first 100 characters before,

00:03:14.000 --> 00:03:16.669
I can print the first 100 encoded values.

00:03:16.669 --> 00:03:19.098
If you look at the length of our unique characters,

00:03:19.098 --> 00:03:21.849
you'll see that we have 83 unique characters in the text.

00:03:21.849 --> 00:03:24.500
So, our encoded values will fall in this range.

00:03:24.500 --> 00:03:27.764
You can also see some repeating values here like 82,

00:03:27.764 --> 00:03:30.799
82, 82 and 19,19.

00:03:30.800 --> 00:03:32.675
If we scroll back up to our actual text,

00:03:32.675 --> 00:03:36.590
we can surmise that the repeated 82s are probably this new line character,

00:03:36.590 --> 00:03:39.110
and 19 is maybe a p. Okay,

00:03:39.110 --> 00:03:40.405
so are encodings are working,

00:03:40.405 --> 00:03:41.710
and now what we want to do,

00:03:41.710 --> 00:03:44.105
is turn these encodings into one-hot vectors,

00:03:44.104 --> 00:03:46.060
that our RNN can take in as input,

00:03:46.060 --> 00:03:47.949
just like in our initial diagram.

00:03:47.949 --> 00:03:51.769
Here, I've actually written a function that takes in an encoded array,

00:03:51.770 --> 00:03:54.710
and turns it into a one-hot vector of some specified length.

00:03:54.710 --> 00:03:58.094
I can show you what this does with an example below.

00:03:58.094 --> 00:04:00.930
I've made a short test sequence three, five,

00:04:00.930 --> 00:04:03.974
one and a vector length that I specify, eight.

00:04:03.974 --> 00:04:06.530
So, I'm passing this test sequence and the number of

00:04:06.530 --> 00:04:09.765
labels that I expect into our one-hot function.

00:04:09.764 --> 00:04:14.084
I can see that the result is an array of three one-hot vectors.

00:04:14.085 --> 00:04:17.439
All of these vectors are of length eight and the index three,

00:04:17.439 --> 00:04:20.759
five, and one are on for their respective encodings.

00:04:20.759 --> 00:04:23.480
Now, for our vocabulary of 83 characters,

00:04:23.480 --> 00:04:25.745
these are just going to be much longer vectors.

00:04:25.745 --> 00:04:30.259
Cool. So, we have our preprocessing functions and data in place,

00:04:30.259 --> 00:04:33.379
and now your first task will be to take our encoded characters,

00:04:33.379 --> 00:04:36.975
and actually turn them into mini batches that we can feed into our network.

00:04:36.975 --> 00:04:38.910
So, as Matt mentioned before,

00:04:38.910 --> 00:04:40.880
the idea is that we actually want to run

00:04:40.879 --> 00:04:43.430
multiple sequences through our network at a time.

00:04:43.430 --> 00:04:47.110
Where one mini batch of data contains multiple sequences.

00:04:47.110 --> 00:04:49.694
So, here's an example starting sequence.

00:04:49.694 --> 00:04:51.620
If we say we want a batch size of two,

00:04:51.620 --> 00:04:54.405
we're going to split this data into two batches.

00:04:54.404 --> 00:04:57.109
Then, we'll have these sequence length windows that

00:04:57.110 --> 00:04:59.555
specify how big we want our sequences to be.

00:04:59.555 --> 00:05:01.879
In this case, we have a sequence length of three,

00:05:01.879 --> 00:05:03.899
and so our window will be three in width.

00:05:03.899 --> 00:05:06.319
For a batch size of two and sequence length of

00:05:06.319 --> 00:05:09.370
three these values will make up our first mini-batch.

00:05:09.370 --> 00:05:12.949
We'll just slide this window over by three to get the next mini-batch.

00:05:12.949 --> 00:05:17.370
So, each mini-batch is going to have the dimensions batch size by sequence length.

00:05:17.370 --> 00:05:20.120
In this case, we have a two by three window

00:05:20.120 --> 00:05:22.840
on are encoded array that we pass into our network.

00:05:22.839 --> 00:05:26.344
If you scroll down, I have more specific instructions.

00:05:26.345 --> 00:05:30.355
The first thing you're going to be doing is taking in an encoded array,

00:05:30.355 --> 00:05:35.319
and you'll want to discard any values that don't fit into completely full mini-batches.

00:05:35.319 --> 00:05:39.605
Then, you want to reshape this array into batch size number of rows.

00:05:39.605 --> 00:05:41.629
Finally, once you have that batch data,

00:05:41.629 --> 00:05:43.654
you're going to want to create a window that iterates

00:05:43.654 --> 00:05:45.919
over the batches a sequence length at a time,

00:05:45.920 --> 00:05:47.410
to get your mini batches.

00:05:47.410 --> 00:05:49.080
So, here's the skeleton code.

00:05:49.079 --> 00:05:51.589
Your array is going to be some encoded data,

00:05:51.589 --> 00:05:54.384
then you have a batch size and sequence length.

00:05:54.384 --> 00:05:57.589
Basically, you want to create an input x that should be

00:05:57.589 --> 00:06:01.804
a sequence length or number of timesteps wide and a batch size tall.

00:06:01.805 --> 00:06:05.870
This will make up our input data and you'll also want to provide targets.

00:06:05.870 --> 00:06:10.694
The targets y, for this network are going to be just like the input characters x,

00:06:10.694 --> 00:06:12.550
only shifted over by one.

00:06:12.550 --> 00:06:14.750
That's because we want our network to predict

00:06:14.750 --> 00:06:17.964
the most likely next character more some input sequence.

00:06:17.964 --> 00:06:22.549
So, you'll have your input sequence x and our targets y shifted over by one.

00:06:22.550 --> 00:06:24.954
Then finally, when we [inaudible] batches,

00:06:24.954 --> 00:06:27.109
we're going to create a generator that iterates through

00:06:27.110 --> 00:06:30.634
our array and returns x and y with this yield command.

00:06:30.634 --> 00:06:33.824
Okay, I'll leave implementing this batching function up to you.

00:06:33.824 --> 00:06:37.050
You can find more information about how you could do this in the notebook.

00:06:37.050 --> 00:06:39.925
There's some code for testing out your implementation below.

00:06:39.925 --> 00:06:43.569
In fact, this is what your batches should look like when you run this code.

00:06:43.569 --> 00:06:46.459
If you need any help or you just want to see my solution,

00:06:46.459 --> 00:06:49.469
go ahead and check out the solution video next.

