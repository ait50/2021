WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.500
我们的目标是训练该模型

00:00:03.500 --> 00:00:07.835
使其能够接受一个字符并预测出下个字符

00:00:07.835 --> 00:00:09.590
下一步“做出预测”将讲解这方面的知识

00:00:09.590 --> 00:00:12.675
我们需要创建函数

00:00:12.675 --> 00:00:15.955
该函数能够接受一个字符并使网络预测下个字符

00:00:15.955 --> 00:00:17.850
然后再将该字符放入网络中

00:00:17.850 --> 00:00:21.675
并预测更多的下个字符

00:00:21.675 --> 00:00:24.435
直到生成大量文本

00:00:24.435 --> 00:00:27.770
我们提供了这个 predict 函数

00:00:27.770 --> 00:00:29.895
此函数接受一个模型和当前字符

00:00:29.895 --> 00:00:33.210
它的职责是

00:00:33.210 --> 00:00:34.770
返回下个预测字符的编码值

00:00:34.770 --> 00:00:39.330
以及模型生成的隐藏状态

00:00:39.330 --> 00:00:42.100
我们逐步了解下该函数

00:00:42.100 --> 00:00:47.040
它会接受输入字符并将其转换为编码整数值

00:00:47.040 --> 00:00:49.080
在预处理过程中

00:00:49.080 --> 00:00:50.210
我们将该值变成

00:00:50.210 --> 00:00:54.905
独热编码表示法 并将这些输入转换为张量

00:00:54.905 --> 00:00:57.410
然后就可以将这些输入传入模型中

00:00:57.410 --> 00:00:59.060
下面的几步

00:00:59.060 --> 00:01:01.285
与训练循环中的步骤很相似

00:01:01.285 --> 00:01:03.470
如果有 GPU 则将输入移到 GPU 上

00:01:03.470 --> 00:01:07.180
在这里使隐藏状态与其历史记录分离

00:01:07.180 --> 00:01:10.160
将输入和隐藏状态传入模型中

00:01:10.160 --> 00:01:13.820
返回一个输出和新的隐藏状态

00:01:13.820 --> 00:01:16.600
接下来 继续处理该输出

00:01:16.600 --> 00:01:21.790
应用 softmax 函数以获得下个潜在字符的概率 p

00:01:21.790 --> 00:01:24.800
p 是给定输入字符 x 后

00:01:24.800 --> 00:01:28.930
所有潜在下个字符的概率分布

00:01:28.930 --> 00:01:31.595
我们可以通过仅考虑前 k 个潜在字符

00:01:31.595 --> 00:01:34.785
生成更合理的字符

00:01:34.785 --> 00:01:38.900
这几行代码是 top_k 抽样代码

00:01:38.900 --> 00:01:41.880
可以帮助我们查找前 k 个最有可能的下个字符

00:01:41.880 --> 00:01:44.660
在这里引入随机性

00:01:44.660 --> 00:01:48.280
从这些前 k 个下个字符中选择字符

00:01:48.280 --> 00:01:51.950
选择最有可能的下个字符

00:01:51.950 --> 00:01:56.170
并返回该字符的编码值和模型生成的隐藏状态

00:01:56.170 --> 00:01:59.510
我们需要多次调用 predict 函数

00:01:59.510 --> 00:02:01.320
生成一个字符的输出

00:02:01.320 --> 00:02:05.145
将该输出当做输入传入模型中并预测下个字符、下下个字符等

00:02:05.145 --> 00:02:07.725
这就要提到下个函数 sample 了

00:02:07.725 --> 00:02:12.385
sample 的参数是训练过的模型、要生成的文本大小

00:02:12.385 --> 00:02:14.235
还有 prime

00:02:14.235 --> 00:02:18.440
prime 是一组帮助模型开始构建文本的字符

00:02:18.440 --> 00:02:21.380
最后一个参数是 top_k

00:02:21.380 --> 00:02:24.950
返回 predict 函数中的前 k 个字符

00:02:24.950 --> 00:02:29.430
首先在这里将模型移到 GPU 上（如果有 GPU 的话）

00:02:29.430 --> 00:02:33.950
并用批次大小 1 初始化隐藏状态

00:02:33.950 --> 00:02:36.080
因为我们一次输入一个字符

00:02:36.080 --> 00:02:37.520
因此批次大小将为 1

00:02:37.520 --> 00:02:40.830
这么看来 预测和训练模型截然不同

00:02:40.830 --> 00:02:44.560
然后获取 prime 单词中的每个字符

00:02:44.560 --> 00:02:46.855
prime 单词可以告诉我们

00:02:46.855 --> 00:02:49.135
如何开始生成文本

00:02:49.135 --> 00:02:50.960
我们不能随机地开始

00:02:50.960 --> 00:02:54.975
因此通常会提供一个 prime 单词或一组字符

00:02:54.975 --> 00:02:57.320
我们传入的 prime 单词是 The

00:02:57.320 --> 00:03:02.315
但是你可以传入任何一组字符

00:03:02.315 --> 00:03:04.505
sample 函数首先按顺序处理这些字符

00:03:04.505 --> 00:03:08.035
将它们添加到字符列表中

00:03:08.035 --> 00:03:11.179
然后对这些字符调用 predict

00:03:11.179 --> 00:03:14.044
传入模型、每个字符以及隐藏状态

00:03:14.044 --> 00:03:17.485
返回 prime 序列后的下个字符和隐藏状态

00:03:17.485 --> 00:03:20.870
这些是 prime 字符

00:03:20.870 --> 00:03:22.175
即 T、h、e

00:03:22.175 --> 00:03:25.970
然后附加下个最有可能的字符

00:03:25.970 --> 00:03:29.805
在这里形成一个字符列表

00:03:29.805 --> 00:03:32.660
然后生成越来越多的字符

00:03:32.660 --> 00:03:37.765
在此循环里 传入模型和字符列表里的最后一个字符

00:03:37.765 --> 00:03:40.450
返回下个字符和隐藏状态

00:03:40.450 --> 00:03:44.380
将此字符附加到列表上并重新开始此循环

00:03:44.380 --> 00:03:47.930
predict 生成下个潜在字符

00:03:47.930 --> 00:03:52.005
将此字符附加到列表上并作为输入传入 predict 函数中

00:03:52.005 --> 00:03:54.500
这样我们就能获取下个字符、下下个字符等

00:03:54.500 --> 00:03:57.740
并将它们添加到字符列表中

00:03:57.740 --> 00:04:00.265
直到达到期望的文本长度

00:04:00.265 --> 00:04:04.455
最后 将这些字符连接到一起并返回示例文本

00:04:04.455 --> 00:04:06.770
我生成了几个示例文本

00:04:06.770 --> 00:04:10.740
传入训练 20 个周期的模型

00:04:10.740 --> 00:04:15.145
生成长 1000 个字符的文本并以 prime 单词 Anna 开头

00:04:15.145 --> 00:04:18.340
top_k 的值设为 5

00:04:18.340 --> 00:04:21.220
可以看到 示例文本以 prime 单词开头

00:04:21.220 --> 00:04:24.195
生成了一个小说段落

00:04:24.195 --> 00:04:26.515
即使只有几个 prime 字符

00:04:26.515 --> 00:04:30.395
模型也能够生成完整的真实单词并且比较合理

00:04:30.395 --> 00:04:32.875
结构和拼写看起来很棒

00:04:32.875 --> 00:04:35.765
虽然内容本身很奇怪

00:04:35.765 --> 00:04:38.800
这是另一个示例 我根据名称传入了一个模型

00:04:38.800 --> 00:04:42.820
并用此模型生成了更长的文本

00:04:42.820 --> 00:04:46.150
以 prime 单词“And Levin said”开头

00:04:46.150 --> 00:04:47.740
很酷

00:04:47.740 --> 00:04:51.235
训练良好的模型能够生成比较合理的文本

00:04:51.235 --> 00:04:53.750
模型通过查看很长的字符序列

00:04:53.750 --> 00:04:56.655
预测下个字符可能是什么

00:04:56.655 --> 00:04:58.880
在我们的抽样和预测代码中

00:04:58.880 --> 00:05:04.115
我们利用 top-k 抽样和随机性选择下个最有可能的字符

00:05:04.115 --> 00:05:07.630
你可以使用任何其他文本数据训练模型

00:05:07.630 --> 00:05:10.010
例如 你可以用它尝试生成

00:05:10.010 --> 00:05:12.755
莎士比亚十四行诗或其他文本

00:05:12.755 --> 00:05:14.350
很棒 你已经学习了很多知识

00:05:14.350 --> 00:05:18.330
知道如何在 PyTorch 中实现 RNN 了

