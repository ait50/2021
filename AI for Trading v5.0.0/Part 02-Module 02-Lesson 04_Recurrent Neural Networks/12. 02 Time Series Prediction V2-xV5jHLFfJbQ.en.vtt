WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.955
To introduce you to RNNs in PyTorch,

00:00:02.955 --> 00:00:04.919
I've created a notebook that will show you how to do

00:00:04.919 --> 00:00:07.714
simple time series prediction with an RNN.

00:00:07.714 --> 00:00:10.769
Specifically, we'll look at some data and see if we can create

00:00:10.769 --> 00:00:14.849
an RNN to accurately predict the next data point given a current data point,

00:00:14.849 --> 00:00:17.484
and this is really easiest to see in an example.

00:00:17.484 --> 00:00:18.765
So, let's get started.

00:00:18.765 --> 00:00:21.205
I'm importing our usual resources,

00:00:21.204 --> 00:00:24.959
and then I'm actually going to create some simple input and target training data.

00:00:24.960 --> 00:00:28.200
A classic example is to use a sine wave as input

00:00:28.199 --> 00:00:31.469
because it has enough variance and shape to be an interesting task,

00:00:31.469 --> 00:00:32.729
but it's also very predictable.

00:00:32.729 --> 00:00:37.039
So, I want to create a sample input and target sequence of data points of length 20,

00:00:37.039 --> 00:00:39.475
which I specify here as sequence length.

00:00:39.475 --> 00:00:42.500
Recall that RNNs are meant to work with sequential data,

00:00:42.500 --> 00:00:44.030
and so the sequence length is

00:00:44.030 --> 00:00:46.550
just the length of a sequence that it will look at as input.

00:00:46.549 --> 00:00:49.519
Often, the sequence length will indicate the number of words in

00:00:49.520 --> 00:00:53.734
a sentence or just some length of numerical data as is the case here.

00:00:53.734 --> 00:00:55.234
So, in these two lines,

00:00:55.234 --> 00:00:56.659
I'm just going to generate the start of

00:00:56.659 --> 00:01:00.034
a sine wave in a range from zero to Pi time steps.

00:01:00.034 --> 00:01:04.844
At first, I'm going to create a number of points that sequence length 20 plus 1,

00:01:04.844 --> 00:01:08.959
then I'm going to reshape my sine wave data to give it one extra dimension,

00:01:08.959 --> 00:01:11.134
the input size, which is just going to be one.

00:01:11.135 --> 00:01:14.545
Then, to create an input and target sequence of the length I want,

00:01:14.545 --> 00:01:19.099
I'm going to say an input X is equal to all but the last point in data,

00:01:19.099 --> 00:01:21.890
and the target Y is equal to all but the first point.

00:01:21.890 --> 00:01:26.094
So, X and Y should contain 20 data points and have an input size of one.

00:01:26.094 --> 00:01:29.864
Finally, I'm going to display this data using the same x-axis.

00:01:29.864 --> 00:01:35.379
You can see the input X is in red and the target Y is shifted over by one in blue.

00:01:35.379 --> 00:01:39.125
So, if we look at this point as an example at the same time step,

00:01:39.125 --> 00:01:43.045
Y is basically X shifted one time step in the future,

00:01:43.045 --> 00:01:44.704
and that's exactly what we want.

00:01:44.704 --> 00:01:46.489
So, now we have our training data and

00:01:46.489 --> 00:01:49.799
the next step is defining an RNN to learn from this data.

00:01:49.799 --> 00:01:52.099
We can define an RNN as usual,

00:01:52.099 --> 00:01:55.469
which is to say as a class using PyTorche's NN library.

00:01:55.469 --> 00:01:59.219
The syntax will look similar to how we've defined CNNs in the past.

00:01:59.219 --> 00:02:01.939
Let's actually click on the RNN documentation to read

00:02:01.939 --> 00:02:05.170
about the parameters that our recurrent layer takes in as input.

00:02:05.170 --> 00:02:08.180
So, here's the documentation for an RNN layer.

00:02:08.180 --> 00:02:10.129
We can see that this layer is responsible for

00:02:10.129 --> 00:02:12.549
calculating a hidden state based on its inputs.

00:02:12.550 --> 00:02:14.260
Now, to define a layer like this,

00:02:14.259 --> 00:02:16.634
we have these parameters: an input size,

00:02:16.634 --> 00:02:20.039
a hidden size, a number of layers and a few other arguments.

00:02:20.039 --> 00:02:23.370
The input size is just the number of input features,

00:02:23.370 --> 00:02:26.599
and in our specific case we're going to have inputs that are 20 values

00:02:26.599 --> 00:02:29.754
in sequence and one in input size features.

00:02:29.754 --> 00:02:34.604
This is like when we thought about the depth of an input image when we made CNN's.

00:02:34.604 --> 00:02:37.219
Next, we have a hidden size that defines how many

00:02:37.219 --> 00:02:40.439
features the output of an RNN will have and its hidden state.

00:02:40.439 --> 00:02:41.990
We also have a number of layers,

00:02:41.990 --> 00:02:43.265
which if it's greater than one,

00:02:43.264 --> 00:02:46.129
just means we're going to stack two RNNs on top of each other.

00:02:46.129 --> 00:02:49.430
Lastly, I want you to pay attention to this batch first parameter.

00:02:49.430 --> 00:02:52.010
If it is true, that means the input and output tensors that we

00:02:52.009 --> 00:02:55.219
provide are going to have the batch size as the first dimension,

00:02:55.219 --> 00:02:57.580
which in most cases that we go through will be true.

00:02:57.580 --> 00:03:00.500
So, this is how you define an RNN layer,

00:03:00.500 --> 00:03:02.780
and later in the forward function we'll see that it takes

00:03:02.780 --> 00:03:05.090
in an input and an initial hidden state,

00:03:05.090 --> 00:03:07.965
and it produces an output and a new hidden state.

00:03:07.965 --> 00:03:13.495
Back to our notebook. Here, I'm defining an RNN layer, self- doubt RNN.

00:03:13.495 --> 00:03:16.925
This RNN is taking in an input size and a hidden dimension

00:03:16.925 --> 00:03:20.410
that defines how many features the output of this RNN will have.

00:03:20.409 --> 00:03:24.229
Then it takes in a number of layers which allows you to create a stacked RNN if

00:03:24.229 --> 00:03:28.114
you want and this is typically a value kept between one and three layers.

00:03:28.115 --> 00:03:30.950
Finally, I'm setting batch first to true because I'm

00:03:30.949 --> 00:03:34.644
shaping the input such that the batch size will be the first dimension.

00:03:34.645 --> 00:03:36.830
Okay. Then to complete this model I have to add

00:03:36.830 --> 00:03:39.675
one more layer which is a final fully-connected layer.

00:03:39.675 --> 00:03:42.859
This layer is responsible for producing the number of outputs,

00:03:42.859 --> 00:03:46.344
output size that I want given the output of the RNN.

00:03:46.344 --> 00:03:51.115
So, all of these parameters are just going to be passed into our RNN when we create it.

00:03:51.115 --> 00:03:53.120
You'll also note that I'm storing the value of

00:03:53.120 --> 00:03:56.115
our hidden dimension so I can use it later in our forward function.

00:03:56.115 --> 00:03:58.985
In the forward function, I'm going to specify how a batch

00:03:58.985 --> 00:04:01.700
of input sequences will pass through this model.

00:04:01.699 --> 00:04:05.234
Note that this forward takes in an input X and the hidden state.

00:04:05.235 --> 00:04:07.040
The first thing I'm doing is grabbing

00:04:07.039 --> 00:04:10.204
the batch size of our input calling X dot size of 0.

00:04:10.205 --> 00:04:14.710
Then I'm passing my initial input and hidden state into the RNN layer.

00:04:14.710 --> 00:04:17.939
This produces the RNN output and a new hidden state.

00:04:17.939 --> 00:04:22.394
Then I'm going to call view on the RNN output to shape it into the size I want.

00:04:22.394 --> 00:04:25.229
In this case that's going to be batch size, times sequence length

00:04:25.230 --> 00:04:28.185
rows and the hidden dimension number of columns.

00:04:28.185 --> 00:04:30.670
This is a flattening step where I'm

00:04:30.670 --> 00:04:33.350
preparing the output to be fed into a fully-connected layer.

00:04:33.350 --> 00:04:37.310
So, I'll pass this shaped output to the final fully-connected layer,

00:04:37.310 --> 00:04:41.259
and return my final output here and my hidden state generated from the RNN.

00:04:41.259 --> 00:04:43.365
Now, as a last step here,

00:04:43.365 --> 00:04:45.560
I'm going to actually create some text data and to

00:04:45.560 --> 00:04:48.290
test RNN and see if it's working as I expect.

00:04:48.290 --> 00:04:50.705
The most common error I get when programming

00:04:50.704 --> 00:04:53.539
RNNs is that I've messed up the data dimension somewhere.

00:04:53.540 --> 00:04:56.345
So, I'm just going to check that there as I expect.

00:04:56.345 --> 00:05:00.925
So, here I'm just creating a test RNN with an input and output size of one,

00:05:00.925 --> 00:05:03.840
a hidden dimension of 10, and the number of layers equal to two,

00:05:03.839 --> 00:05:06.764
and you can change the hidden dimension and the number of layers.

00:05:06.764 --> 00:05:10.539
I basically just want to see that this is making the shape of outputs I expect.

00:05:10.540 --> 00:05:14.240
So, here I'm creating some test data that are sequence length along.

00:05:14.240 --> 00:05:17.180
I'm converting that data into a tensor datatype,

00:05:17.180 --> 00:05:19.100
and I'm squeezing the first dimension to give it

00:05:19.100 --> 00:05:21.620
a batch size of one as a first dimension.

00:05:21.620 --> 00:05:26.139
Then I'm going to print out this input size and I'll pass it into our test RNN as input.

00:05:26.139 --> 00:05:28.279
Recall that this takes an initial hidden state,

00:05:28.279 --> 00:05:30.664
and an initial one here is just going to be none.

00:05:30.665 --> 00:05:33.530
Then this should return an output and a hidden state,

00:05:33.529 --> 00:05:35.544
and I'm going to print out those sizes as well.

00:05:35.545 --> 00:05:39.860
Okay. So, our input size is a 3D tensor which is exactly what I expect.

00:05:39.860 --> 00:05:42.504
If first dimension is one our batch size,

00:05:42.504 --> 00:05:44.305
then 20 our sequence length,

00:05:44.305 --> 00:05:46.370
and finally our input number of features.

00:05:46.370 --> 00:05:48.574
Which is just one as we specified here.

00:05:48.574 --> 00:05:50.979
The output size is a 2D tensor.

00:05:50.980 --> 00:05:54.710
This is because in the forward function of our model definition actually

00:05:54.709 --> 00:05:58.279
smooshed the batch size and sequence length into one parameter.

00:05:58.279 --> 00:06:03.529
So, batch size times sequence length is 20 and then we have an output size of one.

00:06:03.529 --> 00:06:05.019
Finally we have our hidden state.

00:06:05.019 --> 00:06:07.129
Now, the first dimension here is our number of

00:06:07.129 --> 00:06:10.250
layers that I specified in the model definition two.

00:06:10.250 --> 00:06:15.004
Next we have the value one which is just the batch size of our input here.

00:06:15.004 --> 00:06:19.430
Finally, the last dimension here is 10 which is just our hidden dimension.

00:06:19.430 --> 00:06:23.389
So, all of these look pretty good and as I expect and I can proceed.

00:06:23.389 --> 00:06:26.329
Next I'll show you how to train a model like this.

