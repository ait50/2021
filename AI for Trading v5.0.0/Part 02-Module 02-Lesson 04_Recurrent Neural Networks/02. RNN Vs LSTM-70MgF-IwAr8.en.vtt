WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.669
Okay so, let's say we have a regular neural network which

00:00:02.669 --> 00:00:05.429
recognizes images and we fitted this image.

00:00:05.429 --> 00:00:08.339
And the neural neural network guesses that the image is most likely

00:00:08.339 --> 00:00:13.070
a dog with a small chance of being a wolf and an even smaller chance of being a goldfish.

00:00:13.070 --> 00:00:15.033
But, what if this image is actually a wolf?

00:00:15.032 --> 00:00:17.108
How would the neural network know?

00:00:17.109 --> 00:00:20.445
So, let's say we're watching a TV show about nature and the previous image

00:00:20.445 --> 00:00:24.588
before the wolf was a bear and the previous one was a fox.

00:00:24.588 --> 00:00:27.209
So, in this case, we want to use this information to hint

00:00:27.210 --> 00:00:30.274
to us that the last image is a wolf and not a dog.

00:00:30.274 --> 00:00:34.755
So, what we do is analyze each image with the same copy of a neural network.

00:00:34.755 --> 00:00:39.765
But, we use the output of the neural network as a part of the input of the next one.

00:00:39.765 --> 00:00:42.798
And, that will actually improve our results.

00:00:42.798 --> 00:00:44.228
Mathematically, this is simple.

00:00:44.228 --> 00:00:46.664
We just combine the vectors in a linear function,

00:00:46.664 --> 00:00:49.798
which will then be squished with an activation function,

00:00:49.798 --> 00:00:52.600
which could be sigmoid or hyperbolic tan.

00:00:52.600 --> 00:00:54.929
This way, we can use previous information and

00:00:54.929 --> 00:00:58.079
the final neural network will know that the show is about wild animals in

00:00:58.079 --> 00:01:00.868
the forest and actually use this information to correctly

00:01:00.868 --> 00:01:04.025
predict that the image is of a wolf and not a dog.

00:01:04.025 --> 00:01:07.140
And, this is basically how recurrent neural networks work.

00:01:07.140 --> 00:01:08.680
However, this has some drawbacks.

00:01:08.680 --> 00:01:10.480
Let's say the bear appeared a while ago and

00:01:10.480 --> 00:01:12.640
the two recent images are a tree and a squirrel.

00:01:12.640 --> 00:01:14.670
Based on those two,

00:01:14.670 --> 00:01:17.165
we don't really know if the new image is a dog or a wolf.

00:01:17.165 --> 00:01:19.650
Since trees and squirrels are just as associated to

00:01:19.650 --> 00:01:22.349
domestic animals as they are with forest animals.

00:01:22.349 --> 00:01:26.409
So, the information about being in the forest comes all the way back from the bear.

00:01:26.409 --> 00:01:28.125
But, as we've already experienced,

00:01:28.125 --> 00:01:30.269
information coming in gets repeatedly

00:01:30.269 --> 00:01:33.120
squished by sigmoid functions and even worse than that,

00:01:33.120 --> 00:01:36.329
training a network using back propagation all the way back,

00:01:36.328 --> 00:01:40.238
will lead to problems such as the vanishing gradient problem etc.

00:01:40.239 --> 00:01:43.730
So, by this point pretty much all the bear information has been lost.

00:01:43.730 --> 00:01:46.147
That's a problem with recurring neural networks;

00:01:46.147 --> 00:01:49.213
that the memory that is stored is normally short term memory.

00:01:49.213 --> 00:01:53.203
RNNs, have a hard time storing long term memory and this is where

00:01:53.203 --> 00:01:57.559
LSTMs or long short term memory networks will come to the rescue.

00:01:57.560 --> 00:02:00.530
So, as a small summary, an RNN works as follows;

00:02:00.530 --> 00:02:03.269
memory comes in and merges with a current event

00:02:03.269 --> 00:02:06.299
and the output comes out as a prediction of what the input is.

00:02:06.299 --> 00:02:09.870
And also, as part of the input for the next iteration of the neural network.

00:02:09.870 --> 00:02:12.954
And in a similar way, an LSTM works as follows;

00:02:12.954 --> 00:02:16.139
it keeps track not just of memory but of long term memory,

00:02:16.139 --> 00:02:18.254
which comes in and comes out.

00:02:18.253 --> 00:02:19.560
And also, short term memory,

00:02:19.560 --> 00:02:21.539
which also comes in and comes out.

00:02:21.538 --> 00:02:22.954
And in every stage,

00:02:22.955 --> 00:02:25.695
the long and short term memory in the event get merged.

00:02:25.694 --> 00:02:28.500
And from there, we get a new long term memory,

00:02:28.500 --> 00:02:31.030
short term memory and a prediction.

00:02:31.030 --> 00:02:33.538
In here, we protect old information more.

00:02:33.538 --> 00:02:35.008
If we deem it necessary,

00:02:35.008 --> 00:02:37.899
the network can remember things from long time ago.

00:02:37.900 --> 00:02:43.000
So, in the next few videos, I will show you the architecture of LSTMs and how they work.

