WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.390
在本课中 你将实现一个字符型 RNN

00:00:03.390 --> 00:00:06.817
也就是说 此网络将通过每一次学习

00:00:06.817 --> 00:00:11.794
一个字符来学习一些文本 然后再一次一个字符生成新的文本

00:00:11.794 --> 00:00:14.564
假设 我们想生成新的莎士比亚台词

00:00:14.564 --> 00:00:17.515
例如“To be or not to be”

00:00:17.515 --> 00:00:21.875
我们将一次一个字符将此序列传入我们的 RNN

00:00:21.875 --> 00:00:24.719
训练后 此网络将根据它看到的字符

00:00:24.719 --> 00:00:28.230
预测下一个字符 以此来生成新的文本

00:00:28.230 --> 00:00:30.750
那么要训练此网络 我们要

00:00:30.750 --> 00:00:33.750
预测输入序列中的下一个字符

00:00:33.750 --> 00:00:36.270
这样 网络将能够生成与原文本

00:00:36.270 --> 00:00:39.630
大致一样的一系列字符

00:00:39.630 --> 00:00:42.899
我们来看看这个网络的架构是什么样的

00:00:42.899 --> 00:00:47.960
首先我们展开这个 RNN 看看它的序列的工作原理

00:00:47.960 --> 00:00:50.399
这里是输入层 我们

00:00:50.399 --> 00:00:53.189
在此将字符作为独热编码向量传入

00:00:53.189 --> 00:00:55.603
这些向量进入隐藏层

00:00:55.603 --> 00:00:58.769
隐藏层由 LSTM 单元构建 其中

00:00:58.770 --> 00:01:03.625
隐藏状态和单元状态从序列中的一个单元传递到下一个单元

00:01:03.625 --> 00:01:07.405
在实践中 我们实际上会使用多层 LSTM 单元

00:01:07.405 --> 00:01:09.510
只需像这样将它们堆叠起来即可

00:01:09.510 --> 00:01:12.920
这些单元的输出将进入输出层

00:01:12.920 --> 00:01:16.099
输出层用于预测下一个字符

00:01:16.099 --> 00:01:18.905
我们希望用你在卷积网络中进行

00:01:18.905 --> 00:01:22.409
图像分类时使用的相同方式 来获得每个字符的概率

00:01:22.409 --> 00:01:26.810
这意味着我们要对输出使用 Softmax 激活

00:01:26.810 --> 00:01:30.340
我们在此的目标是输出输入序列 但要后移一个

00:01:30.340 --> 00:01:34.299
以便每个字符能预测序列中的下一个字符

00:01:34.299 --> 00:01:38.495
再次 我们将使用交叉熵损失和梯度下降法进行训练

00:01:38.495 --> 00:01:41.965
此网络训练好后 我们便可以传入一个字符

00:01:41.965 --> 00:01:46.015
并获得可能的下一个字符的概率分布

00:01:46.015 --> 00:01:49.704
然后从该分布中取样以得到下一个字符

00:01:49.703 --> 00:01:52.429
然后我们传入

00:01:52.430 --> 00:01:54.915
这一个字符并获得另一个

00:01:54.915 --> 00:01:59.114
重复这样做 最终我们就会获得一些全新的文本

00:01:59.114 --> 00:02:02.250
我们将使用我最爱的一本书

00:02:02.250 --> 00:02:04.250
《安娜·卡列尼娜》中的文本来训练网络

00:02:04.250 --> 00:02:07.640
这本书属于公共领域 所以你可以随意使用

00:02:07.640 --> 00:02:09.229
而且 这也是一本了不起的小说

