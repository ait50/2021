WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.495
神经网络具有某些特殊的层级结构

00:00:04.495 --> 00:00:07.320
第一层称为输入层 (input layer)

00:00:07.320 --> 00:00:08.934
它包含了输入数据

00:00:08.932 --> 00:00:11.931
这个例子中的输入就是 x1 和 x2

00:00:11.932 --> 00:00:14.460
第二层称为隐藏层 (hidden layer)

00:00:14.460 --> 00:00:18.855
也就是针对输入层的一系列线性模型

00:00:18.855 --> 00:00:21.940
最后一层是输出层

00:00:21.940 --> 00:00:26.614
多个线性模型组合成一个非线性模型

00:00:26.614 --> 00:00:28.644
神经网络可以具有不同的结构

00:00:28.643 --> 00:00:31.763
例如 这个神经网络的隐藏层节点较多

00:00:31.765 --> 00:00:33.689
现在我们对三个线性模型进行组合

00:00:33.689 --> 00:00:36.600
以得到输出层中的三角形边界

00:00:36.600 --> 00:00:39.649
如果输入层具有更多节点呢？

00:00:39.649 --> 00:00:43.460
例如 这个神经网络的输入层有三个节点

00:00:43.460 --> 00:00:46.435
这表明我们不再处于二维空间

00:00:46.435 --> 00:00:48.755
而是三维空间

00:00:48.755 --> 00:00:50.045
现在我们的隐藏层

00:00:50.045 --> 00:00:51.689
也就是这些线性模型

00:00:51.689 --> 00:00:54.795
给出了三维空间里的各种平面

00:00:54.795 --> 00:00:59.820
输出层在三维空间里划出了一个非线性区域

00:00:59.820 --> 00:01:03.030
通常 如果输入层里有 n 个节点

00:01:03.030 --> 00:01:06.780
那么处理的就是 n 维空间的数据

00:01:06.780 --> 00:01:08.983
如果输出层有更多节点呢？

00:01:08.983 --> 00:01:10.890
这只是表明我们有更多的输出

00:01:10.890 --> 00:01:14.209
这就是具有多种类别的分类模型

00:01:14.209 --> 00:01:18.329
如果模型需要告诉我们图片中是猫 狗 还是鸟

00:01:18.328 --> 00:01:20.308
那么我们在输出层的每个节点中

00:01:20.310 --> 00:01:25.140
输出每个类别的得分 (score)

00:01:25.140 --> 00:01:27.930
猫 狗 鸟 各有一个得分

00:01:27.930 --> 00:01:31.189
最后有个比较酷的概念

00:01:31.188 --> 00:01:33.274
如果有更多的隐藏层呢？

00:01:33.275 --> 00:01:36.090
那么就形成了深度神经网络

00:01:36.090 --> 00:01:39.435
现在 这些线性模型相结合 形成非线性模型

00:01:39.435 --> 00:01:45.364
这些非线性模型进一步再结合 形成更多的非线性模型

00:01:45.364 --> 00:01:48.150
通常 我们可以这么操作很多次

00:01:48.150 --> 00:01:51.329
形成具有大量隐藏层的复杂模型

00:01:51.328 --> 00:01:54.433
这就是神经网络的神奇所在

00:01:54.435 --> 00:01:56.406
现实生活中的很多模型

00:01:56.406 --> 00:01:59.054
例如无人驾驶或游戏中的电脑玩家

00:01:59.055 --> 00:02:01.049
都具有非常非常多的隐藏层

00:02:01.049 --> 00:02:02.879
神经网络使用高度非线性化的边界

00:02:02.879 --> 00:02:07.091
拆分整个 n 维空间

00:02:07.090 --> 00:02:08.370
右侧的模型就是其中的一个例子

