WEBVTT
Kind: captions
Language: ja-JP

00:00:00.000 --> 00:00:02.520
私たちの目的はニューラルネットワークをトレーニングすることです

00:00:02.520 --> 00:00:03.715
そのためには

00:00:03.715 --> 00:00:05.950
エラー関数を定義する必要があります

00:00:05.950 --> 00:00:10.375
それではパーセプトロンにとってエラー関数とは何かをもう一度見てみましょう

00:00:10.375 --> 00:00:12.135
これがパーセプトロンです

00:00:12.135 --> 00:00:15.000
左には入力ベクターがあり

00:00:15.000 --> 00:00:18.900
エントリーx1からxnまでとバイアスユニットとしての1があります

00:00:18.900 --> 00:00:23.945
エッジには重みW1からWnまでと

00:00:23.945 --> 00:00:26.360
バイアスユニットとしてbがあります

00:00:26.360 --> 00:00:30.275
最後に このパーセプトロンはシグモイド関数を使用していることがわかります

00:00:30.275 --> 00:00:37.008
予測はyハット＝シグモイド(Wx+b)として定義されます

00:00:37.008 --> 00:00:39.750
見てのとおり この関数は

00:00:39.750 --> 00:00:44.175
各ポイントがどのように誤って分類されているかというエラーの尺度を与えてくれます

00:00:44.175 --> 00:00:48.565
大まかに言って ポイントが正しく分類されている場合 これは非常に小さい数であり

00:00:48.565 --> 00:00:50.640
直線からのポイントの距離は

00:00:50.640 --> 00:00:53.415
ポイントが誤って分類されているときの測定になります

00:00:53.415 --> 00:00:57.840
では 多層パーセプトロンでエラー関数を定義するにはどうしたらよいでしょうか

00:00:57.840 --> 00:01:00.000
見てのとおり 予測は

00:01:00.000 --> 00:01:03.740
行列乗算とシグモイド関数の組み合わせにすぎません

00:01:03.740 --> 00:01:07.370
しかし エラー関数は正確に同じものでしょうか

00:01:07.370 --> 00:01:08.817
まったく同じ式になりえますが

00:01:08.817 --> 00:01:12.000
yハットは少しだけ複雑です

00:01:12.000 --> 00:01:17.490
やはり この関数はポイントがどれくらい誤って分類されているかを示します

00:01:17.490 --> 00:01:20.000
ただし 境界がより複雑です

