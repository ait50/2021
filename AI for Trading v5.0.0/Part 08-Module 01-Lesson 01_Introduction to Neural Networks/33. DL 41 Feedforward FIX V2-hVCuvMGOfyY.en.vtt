WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.000
So now that we have defined what neural networks are,

00:00:03.000 --> 00:00:04.810
we need to learn how to train them.

00:00:04.810 --> 00:00:07.290
Training them really means what parameters should they

00:00:07.290 --> 00:00:10.495
have on the edges in order to model our data well.

00:00:10.495 --> 00:00:12.180
So in order to learn how to train them,

00:00:12.180 --> 00:00:16.800
we need to look carefully at how they process the input to obtain an output.

00:00:16.800 --> 00:00:19.820
So let's look at our simplest neural network, a perceptron.

00:00:19.820 --> 00:00:23.400
This perceptron receives a data point of the form x1,

00:00:23.400 --> 00:00:27.195
x2 where the label is Y=1.

00:00:27.195 --> 00:00:29.385
This means that the point is blue.

00:00:29.385 --> 00:00:34.680
Now the perceptron is defined by a linear equation say w1, x1 plus w2,

00:00:34.680 --> 00:00:41.595
x2 plus B, where w1 and w2 are the weights in the edges and B is the bias in the note.

00:00:41.595 --> 00:00:43.555
Here, w1 is bigger than w2,

00:00:43.555 --> 00:00:46.200
so we'll denote that by drawing the edge labelled w1

00:00:46.200 --> 00:00:49.820
much thicker than the edge labelled w2.

00:00:49.820 --> 00:00:53.280
Now, what the perceptron does is it plots the point x1,

00:00:53.280 --> 00:00:57.240
x2 and it outputs the probability that the point is blue.

00:00:57.240 --> 00:01:01.173
Here is the point is in the red area and then the output is a small number,

00:01:01.173 --> 00:01:03.795
since the point is not very likely to be blue.

00:01:03.795 --> 00:01:07.045
This process is known as feedforward.

00:01:07.045 --> 00:01:11.070
We can see that this is a bad model because the point is actually blue.

00:01:11.070 --> 00:01:12.570
Given that the third coordinate,

00:01:12.570 --> 00:01:14.820
the Y is one.

00:01:14.820 --> 00:01:17.010
Now if we have a more complicated neural network,

00:01:17.010 --> 00:01:18.570
then the process is the same.

00:01:18.570 --> 00:01:22.050
Here, we have thick edges corresponding to large weights and

00:01:22.050 --> 00:01:26.280
thin edges corresponding to small weights and the neural network plots

00:01:26.280 --> 00:01:29.070
the point in the top graph and also in

00:01:29.070 --> 00:01:35.025
the bottom graph and the outputs coming out will be a small number from the top model.

00:01:35.025 --> 00:01:38.580
The point lies in the red area which means it has a small probability of being

00:01:38.580 --> 00:01:43.140
blue and a large number from the second model,

00:01:43.140 --> 00:01:44.895
since the point lies in the blue area which means

00:01:44.895 --> 00:01:47.280
it has a large probability of being blue.

00:01:47.280 --> 00:01:51.650
Now, as the two models get combined into this nonlinear model and

00:01:51.650 --> 00:01:53.180
the output layer just plots

00:01:53.180 --> 00:01:57.485
the point and it tells the probability that the point is blue.

00:01:57.485 --> 00:02:00.620
As you can see, this is a bad model because it

00:02:00.620 --> 00:02:03.750
puts the point in the red area and the point is blue.

00:02:03.750 --> 00:02:08.280
Again, this process called feedforward and we'll look at it more carefully.

00:02:08.280 --> 00:02:13.070
Here, we have our neural network and the other notations so the bias is in the outside.

00:02:13.070 --> 00:02:15.260
Now we have a matrix of weights.

00:02:15.260 --> 00:02:21.285
The matrix w superscript one denoting the first layer and the entries are the weights w1,

00:02:21.285 --> 00:02:23.310
1 up to w3, 2.

00:02:23.310 --> 00:02:26.175
Notice that the biases have now been written as w3,

00:02:26.175 --> 00:02:30.110
1 and w3, 2 this is just for convenience.

00:02:30.110 --> 00:02:31.520
Now in the next layer,

00:02:31.520 --> 00:02:36.115
we also have a matrix this one is w superscript two for the second layer.

00:02:36.115 --> 00:02:38.840
This layer contains the weights that tell us how to combine

00:02:38.840 --> 00:02:43.700
the linear models in the first layer to obtain the nonlinear model in the second layer.

00:02:43.700 --> 00:02:45.060
Now what happens is some math.

00:02:45.060 --> 00:02:47.135
We have the input in the form x1, x2,

00:02:47.135 --> 00:02:51.000
1 where the one comes from the bias unit.

00:02:51.000 --> 00:02:55.660
Now we multiply it by the matrix w1 to get these outputs.

00:02:55.660 --> 00:03:01.250
Then, we apply the sigmoid function to turn the outputs into values between zero and one.

00:03:01.250 --> 00:03:04.130
Then the vector format these values gets a one attatched for

00:03:04.130 --> 00:03:08.280
the bias unit and multiplied by the second matrix.

00:03:08.280 --> 00:03:12.110
This returns an output that now gets thrown into a sigmoid function to

00:03:12.110 --> 00:03:16.290
obtain the final output which is y-hat.

00:03:16.290 --> 00:03:21.155
Y-hat is the prediction or the probability that the point is labeled blue.

00:03:21.155 --> 00:03:23.275
So this is what neural networks do.

00:03:23.275 --> 00:03:25.760
They take the input vector and then apply

00:03:25.760 --> 00:03:29.170
a sequence of linear models and sigmoid functions.

00:03:29.170 --> 00:03:32.825
These maps when combined become a highly non-linear map.

00:03:32.825 --> 00:03:37.310
And the final formula is simply y-hat equals sigmoid of

00:03:37.310 --> 00:03:42.995
w2 combined with sigmoid of w1 applied to x.

00:03:42.995 --> 00:03:48.025
Just for redundance, we do this again on a multi-layer perceptron or neural network.

00:03:48.025 --> 00:03:51.105
To calculate our prediction y-hat,

00:03:51.105 --> 00:03:53.380
we start with the unit vector x,

00:03:53.380 --> 00:03:55.560
then we apply the first matrix and

00:03:55.560 --> 00:04:00.405
a sigmoid function to get the values in the second layer.

00:04:00.405 --> 00:04:05.360
Then, we apply the second matrix and another sigmoid function to get the values on

00:04:05.360 --> 00:04:13.315
the third layer and so on and so forth until we get our final prediction, y-hat.

00:04:13.315 --> 00:04:16.430
And this is the feedforward process that the neural networks

00:04:16.430 --> 00:04:20.000
use to obtain the prediction from the input vector.

