WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.714
So let's study gradient descent in more mathematical detail.

00:00:03.714 --> 00:00:07.799
Our function is a function of the weights and it can be graph like this.

00:00:07.799 --> 00:00:10.199
It's got a mathematical structure so it's not Mt.

00:00:10.199 --> 00:00:13.639
Everest anymore, it's more of a mount Math-Er-Horn.

00:00:13.640 --> 00:00:17.830
So we're standing somewhere in Mount Math-Er-Horn and we need to go down.

00:00:17.829 --> 00:00:25.544
So now the inputs of the functions are W1 and W2 and the error function is given by

00:00:25.545 --> 00:00:29.470
E. Then the gradient of E is given by

00:00:29.469 --> 00:00:35.924
the vector sum of the partial derivatives of E with respect to W1 and W2.

00:00:35.924 --> 00:00:39.289
This gradient actually tells us the direction we want to

00:00:39.289 --> 00:00:42.879
move if we want to increase the error function the most.

00:00:42.880 --> 00:00:45.679
Thus, if we take the negative of the gradient,

00:00:45.679 --> 00:00:48.884
this will tell us how to decrease the error function the most.

00:00:48.884 --> 00:00:51.536
And this is precisely what we'll do.

00:00:51.536 --> 00:00:52.882
At the point we're standing,

00:00:52.881 --> 00:00:58.634
we'll take the negative of the gradient of the error function at that point.

00:00:58.634 --> 00:01:01.254
Then we take a step in that direction.

00:01:01.255 --> 00:01:02.650
Once we take a step,

00:01:02.649 --> 00:01:04.575
we'll be in a lower position.

00:01:04.575 --> 00:01:07.525
So we do it again, and again,

00:01:07.525 --> 00:01:12.902
and again, until we are able to get to the bottom of the mountain.

00:01:12.902 --> 00:01:15.219
So this is how we calculate the gradient.

00:01:15.219 --> 00:01:19.724
We start with our initial prediction Y had equals sigmoid of W Expo's B.

00:01:19.724 --> 00:01:22.089
And let's say this prediction is bad because

00:01:22.090 --> 00:01:25.250
the error is large since we're high up in the mountain.

00:01:25.250 --> 00:01:26.765
The prediction looks like this,

00:01:26.765 --> 00:01:32.930
Y had equal sigmoid of W 1 x 1 plus all the way to WnXn plus b.

00:01:32.930 --> 00:01:37.025
Now the error function is given by the formula we saw before.

00:01:37.025 --> 00:01:40.505
But what matters here is the gradient of the error function.

00:01:40.504 --> 00:01:44.899
The gradient of the error function is precisely the vector formed by

00:01:44.900 --> 00:01:50.170
the partial derivative of the error function with respect to the weights and the bias.

00:01:50.170 --> 00:01:54.189
Now, we take a step in the direction of the negative of the gradient.

00:01:54.189 --> 00:01:57.530
As before, we don't want to make any dramatic changes,

00:01:57.530 --> 00:02:00.230
so we'll introduce a smaller learning rate alpha.

00:02:00.230 --> 00:02:02.630
For example, 0.1.

00:02:02.629 --> 00:02:05.935
And we'll multiply the gradient by that number.

00:02:05.935 --> 00:02:09.140
Now taking the step is exactly the same thing as

00:02:09.139 --> 00:02:12.204
updating the weights and the bias as follows.

00:02:12.205 --> 00:02:15.625
The weight Wi will now become Wi prime.

00:02:15.625 --> 00:02:21.562
Given by Wi minus alpha times the partial derivative of the error,

00:02:21.562 --> 00:02:23.880
with respect to Wi.

00:02:23.879 --> 00:02:28.495
And the bias will now become b prime given by b minus

00:02:28.495 --> 00:02:33.545
alpha times partial derivative of the error with respect to b.

00:02:33.544 --> 00:02:37.889
Now this will take us to a prediction with a lower error function.

00:02:37.889 --> 00:02:43.214
So, we can conclude that the prediction we have now with weights W prime b prime,

00:02:43.215 --> 00:02:48.055
is better than the one we had before with weights W and b.

00:02:48.055 --> 00:02:51.000
This is precisely the gradient descent step.

