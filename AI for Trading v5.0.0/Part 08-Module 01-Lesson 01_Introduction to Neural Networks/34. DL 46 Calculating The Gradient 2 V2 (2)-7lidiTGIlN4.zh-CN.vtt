WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.305
我们再次回到含有权重和输入的神经网络

00:00:04.305 --> 00:00:08.730
回想一下 含有上标 1 的权重属于第一层

00:00:08.730 --> 00:00:12.775
含有上标 2 的权重属于第二层

00:00:12.775 --> 00:00:15.390
偏差不再叫做 b

00:00:15.390 --> 00:00:16.960
现在为了方便称为 w31

00:00:16.960 --> 00:00:19.665
w32 等等

00:00:19.665 --> 00:00:22.830
所以我们把一切都用矩阵符号表示

00:00:22.830 --> 00:00:25.280
现在怎么处理输入呢？

00:00:25.280 --> 00:00:28.218
我们来进行前馈过程

00:00:28.218 --> 00:00:29.910
在第一层中

00:00:29.910 --> 00:00:34.620
我们利用输入 把它乘以权重 得到 h1

00:00:34.620 --> 00:00:37.860
这是输入和权重的线性函数

00:00:37.860 --> 00:00:39.600
同样 h2

00:00:39.600 --> 00:00:42.085
通过这个公式可以得到

00:00:42.085 --> 00:00:43.350
现在 在第二层中

00:00:43.350 --> 00:00:46.485
我们利用 h1 和 h2 以及新的偏差

00:00:46.485 --> 00:00:48.645
应用 sigmoid 函数

00:00:48.645 --> 00:00:52.980
然后对它们应用线性函数 用它们乘以

00:00:52.980 --> 00:00:57.345
权重 添加值 h

00:00:57.345 --> 00:00:58.530
最后在第三层中

00:00:58.530 --> 00:01:02.100
我们只是利用 h 的 sigmoid 函数

00:01:02.100 --> 00:01:07.540
得到 0 到 1 之间的预测或概率 也就是 y-hat

00:01:07.540 --> 00:01:11.070
我们以更加浓缩的符号法这样写

00:01:11.070 --> 00:01:15.244
认为第一层对应的矩阵是 W1 (1 为上标)

00:01:15.244 --> 00:01:19.930
第二层对应的矩阵是 W2 (2 为上标)

00:01:19.930 --> 00:01:22.570
那么我们得到的预测等于

00:01:22.570 --> 00:01:26.260
σ W2 (2 为上标)

00:01:26.260 --> 00:01:33.540
和 σ W1 (1 为上标) (x) 这是前馈

00:01:33.540 --> 00:01:35.890
现在我们要进行反向传播

00:01:35.890 --> 00:01:39.010
也就是前馈的反向过程

00:01:39.010 --> 00:01:40.930
那么我们要使用链式法则

00:01:40.930 --> 00:01:44.890
计算标签中每个权重

00:01:44.890 --> 00:01:52.611
误差函数的导数

00:01:52.611 --> 00:01:59.110
我们想到误差函数是这里的公式

00:01:59.110 --> 00:02:02.760
也就是预测 y-hat

00:02:02.760 --> 00:02:07.015
不过 因为预测是所有权重 wij 的函数

00:02:07.015 --> 00:02:13.540
那么误差函数可以看做所有 wij 的函数

00:02:13.540 --> 00:02:16.960
所以梯度是向量

00:02:16.960 --> 00:02:23.500
通过每个权重对应的误差函数 E 的偏导数得到结果

00:02:23.500 --> 00:02:25.196
我们来计算其中一个导数

00:02:25.196 --> 00:02:31.210
计算 W11 (上标 1) E 的导数

00:02:31.210 --> 00:02:35.140
因为预测是函数的组合 通过链式法则

00:02:35.140 --> 00:02:37.750
我们知道了它的导数

00:02:37.750 --> 00:02:41.650
是所有偏导数的乘积

00:02:41.650 --> 00:02:44.710
在这个例子中 W11 的导数 E

00:02:44.710 --> 00:02:48.617
是 y-hat 对应的导数乘以

00:02:48.617 --> 00:02:52.480
h 对应的导数 y-hat

00:02:52.480 --> 00:02:57.650
乘以 h1 对应的导数 h 乘以 W11 对应的导数 h1

00:02:57.650 --> 00:03:01.345
这可能看起来很复杂

00:03:01.345 --> 00:03:03.790
不过我们可以通过四个偏导数的乘法

00:03:03.790 --> 00:03:06.370
计算这个复杂的复合函数导数

00:03:06.370 --> 00:03:10.235
这是值得注意的

00:03:10.235 --> 00:03:12.360
现在我们已经计算了第一个导数

00:03:12.360 --> 00:03:14.767
y-hat 对应 E 的导数

00:03:14.767 --> 00:03:16.430
如果你还记得 我们求出了 y-hat 减去 y

00:03:16.430 --> 00:03:20.095
我们来计算其他几个

00:03:20.095 --> 00:03:25.193
我们放大一下 观察多层感知器其中的一部分

00:03:25.193 --> 00:03:28.665
输入是一些值 h1 和 h2

00:03:28.665 --> 00:03:30.955
这是之前得到的值

00:03:30.955 --> 00:03:34.905
一旦对 h1 和 h2 以及偏置单元对应的 1

00:03:34.905 --> 00:03:39.045
应用 sigmoid 和线性函数

00:03:39.045 --> 00:03:41.550
我们会得到结果 h

00:03:41.550 --> 00:03:44.670
那么 h1 对应的导数 h 是什么呢？

00:03:44.670 --> 00:03:51.130
h 是三项的总和 其中只有一个包含 h1

00:03:51.130 --> 00:03:55.940
那么第二个和第三个求和可以得到 0 的导数

00:03:55.940 --> 00:04:03.205
第一个求和可以得到 W11 (上标 2) 因为这是个常量

00:04:03.205 --> 00:04:08.715
然后乘以 h1 对应的 sigmoid 函数的导数

00:04:08.715 --> 00:04:12.615
这是我们在讲师注释下面的内容

00:04:12.615 --> 00:04:15.960
其中 sigmoid 函数具有很漂亮的导数

00:04:15.960 --> 00:04:19.200
也就是说 σ h 的导数

00:04:19.200 --> 00:04:24.660
是 σ h 乘以 1 - σ h

00:04:24.660 --> 00:04:27.600
你可以在讲师注释下面看到这个过程

00:04:27.600 --> 00:04:31.275
你也有机会在练习中编写代码 因为最后

00:04:31.275 --> 00:04:35.635
我们会编码这些公式 然后长期使用 就是这样

00:04:35.635 --> 00:04:37.020
这就是你训练神经网络的方法

