WEBVTT
Kind: captions
Language: ja-JP

00:00:00.000 --> 00:00:04.305
それでは 重みと入力を適用したニューラルネットワークに戻りましょう

00:00:04.305 --> 00:00:08.730
重み上付き1は第1層に 重み上付き2は第2層に属することを

00:00:08.730 --> 00:00:12.775
思い出してください

00:00:12.775 --> 00:00:15.390
また バイアスをbと呼ばなくなったことも思い出してください

00:00:15.390 --> 00:00:16.960
ここでは W31 W32 などと呼びます

00:00:16.960 --> 00:00:19.665
便宜上です

00:00:19.665 --> 00:00:22.830
これで 行列表記にすることができます

00:00:22.830 --> 00:00:25.280
入力はどうなっているでしょうか?

00:00:25.280 --> 00:00:28.218
フィードフォワードプロセスを実行してみましょう

00:00:28.218 --> 00:00:29.910
第1層では 入力を受け取り

00:00:29.910 --> 00:00:34.620
それに重みを乗算すると h1を求められます

00:00:34.620 --> 00:00:37.860
これは入力と重みの線形関数です

00:00:37.860 --> 00:00:39.600
h2も同様で

00:00:39.600 --> 00:00:42.085
この式で求められます

00:00:42.085 --> 00:00:43.350
第2層では h1とh2

00:00:43.350 --> 00:00:46.485
および新しいバイアスにシグモイド関数を適用します

00:00:46.485 --> 00:00:48.645
さらに線形関数を適用するため

00:00:48.645 --> 00:00:52.980
 各値に重みを乗算して

00:00:52.980 --> 00:00:57.345
すべてを加算し hの値を求めます

00:00:57.345 --> 00:00:58.530
最後に第3層で

00:00:58.530 --> 00:01:02.100
hのシグモイド関数を使って予測

00:01:02.100 --> 00:01:07.540
つまり0～1の確率を求めます  これがŷです

00:01:07.540 --> 00:01:11.070
これをより簡潔に表記するために

00:01:11.070 --> 00:01:15.244
第1層に対応する行列をW1

00:01:15.244 --> 00:01:19.930
第2層に対応する行列をW2とします

00:01:19.930 --> 00:01:22.570
W2のシグモイドをW1のシグモイドと組み合わせて

00:01:22.570 --> 00:01:26.260
入力xに適用したものが予測となります

00:01:26.260 --> 00:01:33.540
これがフィードフォワードです

00:01:33.540 --> 00:01:35.890
これから逆伝搬を開発します

00:01:35.890 --> 00:01:39.010
これはまさにフィードフォワードの逆です

00:01:39.010 --> 00:01:40.930
連鎖法則を使って

00:01:40.930 --> 00:01:44.890
ラベル内の各重みに対する

00:01:44.890 --> 00:01:52.611
この誤差関数の微分係数を計算します

00:01:52.611 --> 00:01:59.110
誤差関数がこの式であったことを思い出しましょう

00:01:59.110 --> 00:02:02.760
これは予測ŷの関数です

00:02:02.760 --> 00:02:07.015
ただし予測はすべての重みwijの関数であるため

00:02:07.015 --> 00:02:13.540
誤差関数はwijすべてに対する関数と見なすことができます

00:02:13.540 --> 00:02:16.960
したがって勾配は単に

00:02:16.960 --> 00:02:23.500
各重みに対する誤差関数Eのすべての偏微分係数で形成されるベクターです

00:02:23.500 --> 00:02:25.196
これらの微分係数の1つを計算しましょう

00:02:25.196 --> 00:02:31.210
W11上付き1に対するEの微分係数を計算しましょう

00:02:31.210 --> 00:02:35.140
予測は単に関数の合成であり 連鎖法則によるものであるため

00:02:35.140 --> 00:02:37.750
これに対する微分係数は

00:02:37.750 --> 00:02:41.650
すべての偏微分係数の積であることがわかります

00:02:41.650 --> 00:02:44.710
この例では W11に対する微分係数Eは

00:02:44.710 --> 00:02:48.617
ŷに対する微分係数×hに対する微分係数ŷ

00:02:48.617 --> 00:02:52.480
×h1に対する微分係数h

00:02:52.480 --> 00:02:57.650
×W11に対する微分係数h1で求めることができます

00:02:57.650 --> 00:03:01.345
複雑なように見えますが

00:03:01.345 --> 00:03:03.790
実は4つの微分係数を乗算するだけで

00:03:03.790 --> 00:03:06.370
このような複雑な合成関数の微分係数を

00:03:06.370 --> 00:03:10.235
計算できます

00:03:10.235 --> 00:03:12.360
最初の微分係数

00:03:12.360 --> 00:03:14.767
つまりŷに対する微分係数の計算が終わりました

00:03:14.767 --> 00:03:16.430
ŷマイナスyは以前求めましたね

00:03:16.430 --> 00:03:20.095
ほかの微分係数を計算しましょう

00:03:20.095 --> 00:03:25.193
少しズームインして 多層パーセプトロンの一部を見てみましょう

00:03:25.193 --> 00:03:28.665
h1やh2などの入力は

00:03:28.665 --> 00:03:30.955
それより前の計算から取得します

00:03:30.955 --> 00:03:34.905
バイアスユニットに対応するh1、h2、1に

00:03:34.905 --> 00:03:39.045
シグモイドと線形関数を適用すると

00:03:39.045 --> 00:03:41.550
結果はhになります

00:03:41.550 --> 00:03:44.670
それでは h1に対するhの微分係数は何でしょうか?

00:03:44.670 --> 00:03:51.130
hは3項の合計であり それらの1つだけにh1が含まれています

00:03:51.130 --> 00:03:55.940
第2項と第3項は微分係数がゼロになります

00:03:55.940 --> 00:04:03.205
第1項では 定数であるW11上付き2と

00:04:03.205 --> 00:04:08.715
h1に対するシグモイド関数の微分係数を掛け合わせます

00:04:08.715 --> 00:04:12.615
これは計算結果であり その内容は下記の「講師注記」欄にあります

00:04:12.615 --> 00:04:15.960
つまり シグモイド関数は美しい微分係数をとるということです

00:04:15.960 --> 00:04:19.200
hのシグモイドの導関数は hのシグモイド×1-hのシグモイドで求められます

00:04:19.200 --> 00:04:24.660
繰り返しますが

00:04:24.660 --> 00:04:27.600
計算内容は下記の「講師注記」欄で確認できます

00:04:27.600 --> 00:04:31.275
1日の締めくくりにクイズでこれをコーディングしてもらいます

00:04:31.275 --> 00:04:35.635
これらの式をコーディングして 先々で使用します

00:04:35.635 --> 00:04:37.020
ニューラルネットワークのトレーニング方法は以上です

