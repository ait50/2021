WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.305
So, let us go back to our neural network with our weights and our input.

00:00:04.305 --> 00:00:08.730
And recall that the weights with superscript 1 belong to the first layer,

00:00:08.730 --> 00:00:12.775
and the weights with superscript 2 belong to the second layer.

00:00:12.775 --> 00:00:15.390
Also, recall that the bias is not called b anymore.

00:00:15.390 --> 00:00:16.960
Now, it is called W31,

00:00:16.960 --> 00:00:19.665
W32 etc. for convenience,

00:00:19.665 --> 00:00:22.830
so that we can have everything in matrix notation.

00:00:22.830 --> 00:00:25.280
And now what happens with the input?

00:00:25.280 --> 00:00:28.218
So, let us do the feedforward process.

00:00:28.218 --> 00:00:29.910
In the first layer,

00:00:29.910 --> 00:00:34.620
we take the input and multiply it by the weights and that gives us h1,

00:00:34.620 --> 00:00:37.860
which is a linear function of the input and the weights.

00:00:37.860 --> 00:00:39.600
Same thing with h2,

00:00:39.600 --> 00:00:42.085
given by this formula over here.

00:00:42.085 --> 00:00:43.350
Now, in the second layer,

00:00:43.350 --> 00:00:46.485
we would take this h1 and h2 and the new bias,

00:00:46.485 --> 00:00:48.645
apply the sigmoid function,

00:00:48.645 --> 00:00:52.980
and then apply a linear function to them by multiplying them by

00:00:52.980 --> 00:00:57.345
the weights and adding them to get a value of h. And finally,

00:00:57.345 --> 00:00:58.530
in the third layer,

00:00:58.530 --> 00:01:02.100
we just take a sigmoid function of h to get

00:01:02.100 --> 00:01:07.540
our prediction or probability between 0 and 1, which is ŷ.

00:01:07.540 --> 00:01:11.070
And we can read this in more condensed notation by saying that

00:01:11.070 --> 00:01:15.244
the matrix corresponding to the first layer is W superscript 1,

00:01:15.244 --> 00:01:19.930
the matrix corresponding to the second layer is W superscript 2,

00:01:19.930 --> 00:01:22.570
and then the prediction we had is just going to be

00:01:22.570 --> 00:01:26.260
the sigmoid of W superscript 2 combined with

00:01:26.260 --> 00:01:33.540
the sigmoid of W superscript 1 applied to the input x and that is feedforward.

00:01:33.540 --> 00:01:35.890
Now, we are going to develop backpropagation,

00:01:35.890 --> 00:01:39.010
which is precisely the reverse of feedforward.

00:01:39.010 --> 00:01:40.930
So, we are going to calculate the derivative of

00:01:40.930 --> 00:01:44.890
this error function with respect to each of the weights in

00:01:44.890 --> 00:01:52.611
the labels by using the chain rule.

00:01:52.611 --> 00:01:59.110
So, let us recall that our error function is this formula over here,

00:01:59.110 --> 00:02:02.760
which is a function of the prediction ŷ.

00:02:02.760 --> 00:02:07.015
But, since the prediction is a function of all the weights wij,

00:02:07.015 --> 00:02:13.540
then the error function can be seen as the function on all the wij.

00:02:13.540 --> 00:02:16.960
Therefore, the gradient is simply the vector formed by

00:02:16.960 --> 00:02:23.500
all the partial derivatives of the error function E with respect to each of the weights.

00:02:23.500 --> 00:02:25.196
So, let us calculate one of these derivatives.

00:02:25.196 --> 00:02:31.210
Let us calculate derivative of E with respect to W11 superscript 1.

00:02:31.210 --> 00:02:35.140
So, since the prediction is simply a composition of functions and by the chain rule,

00:02:35.140 --> 00:02:37.750
we know that the derivative with respect to this

00:02:37.750 --> 00:02:41.650
is the product of all the partial derivatives.

00:02:41.650 --> 00:02:44.710
In this case, the derivative E with respect

00:02:44.710 --> 00:02:48.617
to W11 is the derivative of either respect to ŷ times

00:02:48.617 --> 00:02:52.480
the derivative ŷ with respect to h

00:02:52.480 --> 00:02:57.650
times the derivative h with respect to h1 times the derivative h1 with respect to W11.

00:02:57.650 --> 00:03:01.345
This may seem complicated,

00:03:01.345 --> 00:03:03.790
but the fact that we can calculate a derivative of

00:03:03.790 --> 00:03:06.370
such a complicated composition function by just

00:03:06.370 --> 00:03:10.235
multiplying 4 partial derivatives is remarkable.

00:03:10.235 --> 00:03:12.360
Now, we have already calculated the first one,

00:03:12.360 --> 00:03:14.767
the derivative of E with respect to ŷ.

00:03:14.767 --> 00:03:16.430
And if you remember, we got ŷ minus y.

00:03:16.430 --> 00:03:20.095
So, let us calculate the other ones.

00:03:20.095 --> 00:03:25.193
Let us zoom in a bit and look at just one piece of our multi-layer perceptron.

00:03:25.193 --> 00:03:28.665
The inputs are some values h1 and h2,

00:03:28.665 --> 00:03:30.955
which are values coming in from before.

00:03:30.955 --> 00:03:34.905
And once we apply the sigmoid and a linear function

00:03:34.905 --> 00:03:39.045
on h1 and h2 and 1 corresponding to the biased unit,

00:03:39.045 --> 00:03:41.550
we get a result h. So,

00:03:41.550 --> 00:03:44.670
now what is the derivative of h with respect to h1?

00:03:44.670 --> 00:03:51.130
Well, h is a sum of three things and only one of them contains h1.

00:03:51.130 --> 00:03:55.940
So, the second and the third summon just give us a derivative of 0.

00:03:55.940 --> 00:04:03.205
The first summon gives us W11 superscript 2 because that is a constant,

00:04:03.205 --> 00:04:08.715
and that times the derivative of the sigmoid function with respect to h1.

00:04:08.715 --> 00:04:12.615
This is something that we calculated below in the instructor comments,

00:04:12.615 --> 00:04:15.960
which is that the sigmoid function has a beautiful derivative,

00:04:15.960 --> 00:04:19.200
namely the derivative of sigmoid of h is

00:04:19.200 --> 00:04:24.660
precisely sigmoid of h times 1 minus sigmoid of h. Again,

00:04:24.660 --> 00:04:27.600
you can see this development underneath in the instructor comments.

00:04:27.600 --> 00:04:31.275
You also have the chance to code this in the quiz because at the end of the day,

00:04:31.275 --> 00:04:35.635
we just code these formulas and then use them forever, and that is it.

00:04:35.635 --> 00:04:37.020
That is how you train a neural network.

