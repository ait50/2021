WEBVTT
Kind: captions
Language: pt-BR

00:00:00.424 --> 00:00:01.835
Na última sessão,

00:00:01.868 --> 00:00:03.880
aprendemos que podemos
somar modelos lineares

00:00:03.913 --> 00:00:05.633
para obter um terceiro modelo.

00:00:05.666 --> 00:00:08.055
Na verdade,
fizemos ainda mais.

00:00:08.088 --> 00:00:10.460
Podemos fazer uma combinação linear
de dois modelos,

00:00:10.493 --> 00:00:12.276
assim, o primeiro modelo
vezes uma constante,

00:00:12.309 --> 00:00:15.550
mais o segundo modelo,
vezes uma constante, mais um viés

00:00:15.583 --> 00:00:17.993
nos dá um modelo não linear.

00:00:18.026 --> 00:00:19.793
Parece muito com perceptrons,

00:00:19.826 --> 00:00:21.880
em que podemos pegar
um valor vezes uma constante,

00:00:21.913 --> 00:00:23.749
mais outro valor,
vezes uma constante,

00:00:23.782 --> 00:00:26.774
mais um viés
e obter um novo valor.

00:00:26.807 --> 00:00:27.993
E isso não é coincidência.

00:00:28.026 --> 00:00:31.458
É de fato a essência
das redes neurais.

00:00:31.491 --> 00:00:33.215
Vamos ver um exemplo.

00:00:33.248 --> 00:00:37.044
Digamos que temos um modelo linear
em que a equação linear é

00:00:37.077 --> 00:00:40.451
5x1 - 2x2 + 8.

00:00:40.484 --> 00:00:42.741
Está representada
por este perceptron.

00:00:42.774 --> 00:00:48.625
E temos outro modelo linear
com a equação 7x1 - 3x2 - 1,

00:00:48.658 --> 00:00:51.914
representada
por este perceptron aqui.

00:00:51.947 --> 00:00:54.145
Vamos representá-los aqui.

00:00:54.178 --> 00:00:58.111
Vamos usar outro perceptron
para combinar estes dois modelos

00:00:58.144 --> 00:01:02.379
usando a equação linear
7 vezes o primeiro modelo,

00:01:02.412 --> 00:01:06.444
+ 5 vezes o segundo modelo - 6.

00:01:06.477 --> 00:01:07.913
É aí que a mágica acontece,

00:01:07.946 --> 00:01:11.433
quando juntamos as duas
e obtemos uma rede neural.

00:01:11.466 --> 00:01:15.066
Limpamos um pouco
e obtemos isto.

00:01:15.099 --> 00:01:16.550
Todos os pesos estão aí.

00:01:16.583 --> 00:01:18.595
Os pesos da esquerda

00:01:18.628 --> 00:01:22.729
nos dizem quantas equações
os modelos lineares têm.

00:01:22.762 --> 00:01:24.945
E os pesos da direita

00:01:24.978 --> 00:01:27.947
nos dizem qual combinação linear
dos dois modelos

00:01:27.980 --> 00:01:31.469
gera a curva
do modelo não linear à direita.

00:01:31.502 --> 00:01:35.547
Então, sempre que você vir
uma rede neural como a da esquerda,

00:01:35.580 --> 00:01:37.873
pense em qual seria
a fronteira não linear

00:01:37.906 --> 00:01:40.865
definida pela rede neural.

00:01:40.898 --> 00:01:43.134
Veja que isto foi desenhado
usando a notação

00:01:43.167 --> 00:01:45.472
que coloca o viés
dentro do nó.

00:01:45.505 --> 00:01:47.376
Isto pode ser representado
usando a notação

00:01:47.409 --> 00:01:50.258
que mantém o viés
como um nó separado.

00:01:50.291 --> 00:01:51.584
Aqui, o que fazemos é:

00:01:51.617 --> 00:01:54.273
em cada camada temos
uma unidade de viés

00:01:54.306 --> 00:01:56.706
vindo de um nó com 1 nele.

00:01:56.739 --> 00:01:57.956
Então, por exemplo,

00:01:57.989 --> 00:02:02.414
o -8 no nó de cima
se torna um limite com rótulo -8

00:02:02.447 --> 00:02:04.356
vindo de um nó de viés.

00:02:04.389 --> 00:02:07.593
Podemos ver que esta rede neural
usa uma função de ativação sigmoide

00:02:07.626 --> 00:02:08.929
e perceptrons.

